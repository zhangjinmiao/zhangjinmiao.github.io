

[
  
  
    {
      "title"    : "页面没有找到",
      "url"      : "http://zhangjinmiao.github.io/404.html",
      "keywords" : "404"
    } ,
  
  
  
    {
      "title"    : "JAVA 系列进阶指南",
      "url"      : "http://zhangjinmiao.github.io/todo",
      "keywords" : "进阶,提高"
    } ,
  
  
  
    {
      "title"    : "About",
      "url"      : "http://zhangjinmiao.github.io/about",
      "keywords" : "Zhang jinmiao, jimzhang, 山川尽美, 火炎焱"
    } ,
  
  
  
    {
      "title"    : "架构师的自我修养",
      "url"      : "http://zhangjinmiao.github.io/arch",
      "keywords" : ""
    } ,
  
  
  
    {
      "title"    : "归档",
      "url"      : "http://zhangjinmiao.github.io/archives/",
      "keywords" : ""
    } ,
  
  
  
    {
      "title"    : "Categories",
      "url"      : "http://zhangjinmiao.github.io/categories/",
      "keywords" : "分类"
    } ,
  
  
  
    {
      "title"    : "Docker 系列文章",
      "url"      : "http://zhangjinmiao.github.io/docker",
      "keywords" : "Docker,DockerFile,Swarm,docker-machine,MCompose,Docker 学习,服务编排"
    } ,
  
  
  
    {
      "title"    : "捐助 / Donate",
      "url"      : "http://zhangjinmiao.github.io/donate/",
      "keywords" : "Donate"
    } ,
  
  
  
    {
      "title"    : "福利",
      "url"      : "http://zhangjinmiao.github.io/fuli",
      "keywords" : "福利"
    } ,
  
  
  
  
  
    {
      "title"    : "我的生活故事",
      "url"      : "http://zhangjinmiao.github.io/life",
      "keywords" : "生活,故事"
    } ,
  
  
  
    {
      "title"    : "Links",
      "url"      : "http://zhangjinmiao.github.io/links",
      "keywords" : "友情链接"
    } ,
  
  
  
    {
      "title"    : "mindmap",
      "url"      : "http://zhangjinmiao.github.io/mindmap-viewer/",
      "keywords" : "mindmap"
    } ,
  
  
  
    {
      "title"    : "挣钱",
      "url"      : "http://zhangjinmiao.github.io/money",
      "keywords" : "挣钱"
    } ,
  
  
  
    {
      "title"    : "Open Source Projects",
      "url"      : "http://zhangjinmiao.github.io/open-source",
      "keywords" : "开源,open-source,GitHub,开源项目"
    } ,
  
  
  
    {
      "title"    : "支付系列文章",
      "url"      : "http://zhangjinmiao.github.io/payment",
      "keywords" : "支付,对账,交易,结算,清算,账户"
    } ,
  
  
  
  
  
    {
      "title"    : "Spring Boot 系列文章",
      "url"      : "http://zhangjinmiao.github.io/spring-boot",
      "keywords" : "Spring Boot 教程,Spring Boot 示例,Spring Boot 学习,Spring Boot 资源,Spring Boot 2.0"
    } ,
  
  
  
    {
      "title"    : "Spring Cloud 系列文章",
      "url"      : "http://zhangjinmiao.github.io/spring-cloud",
      "keywords" : "Spring Cloud 教程,Spring Cloud 示例,Spring Cloud 学习,Spring Cloud 资源,Spring Cloud"
    } ,
  
  
  
    {
      "title"    : "优秀网站",
      "url"      : "http://zhangjinmiao.github.io/used-urls",
      "keywords" : "github,手册,汇总,项目,工具,网址"
    } ,
  
  
  
    {
      "title"    : "Wiki",
      "url"      : "http://zhangjinmiao.github.io/wiki/",
      "keywords" : "维基, Wiki"
    } ,
  
  
  
    {
      "title"    : "易学知识",
      "url"      : "http://zhangjinmiao.github.io/yixue",
      "keywords" : "易学,相术"
    } ,
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  
    {
      "title"    : "牛人博客",
      "category" : "Blog",
      "content": "摆好姿势，向各位大牛看齐！ 团队  阿里中间件团队   美团点评技术团队   网易乐得技术团队 个人  廖雪峰   阮一峰的网络日志   江南白衣   酷壳-陈皓   张戈   凤凰牌老熊——支付大牛   黄建宏——Redis 大牛   宋净超-云原生   扶墙老师说   鸟窝——Scala   你假笨@JVM   R 大   公众号瞬息之间作者   芋道源码的博客   周立   程序猿 DD   占小狼   唐亚峰   柒 blog   KL 博客   stormzhang   HollisChuang’s Blog-Java 工程师成神之路   Java 技术驿站   人生设计师   代码家   陈浩翔   运维大佬   mkyong   baeldung   Java 全栈知识体系 算法  LeetCode   wang   labuladong 独家算法教程   五分钟学算法  ",
      "url"      : "http://zhangjinmiao.github.io/blog/2018/03/13/dalao-blogs.html",
      "keywords" : "汇总,团队,个人,博客"
    } ,
  
    {
      "title"    : "优秀网站",
      "category" : "Blog",
      "content": "把平时看到的好的网址收集到一块，使用时便于查找，以下内容持续更新，欢迎推荐。 文档类 技术文档中文版 StuQ 程序员技能图谱 后端架构师技术图谱 free-programming-books Interview-Notebook-技术面试需要掌握的基础知识整理 Nginx 中文文档 tutorials-国外牛人维护的很全 书籍  《微服务：从设计到部署》中文版 支付版块  凤凰牌老熊 面试  CS-Notes BAT_interviews 源码阅读  Spring 工具类 流程图手绘网站 线程快照信息分析平台 OOMdump 文件分析平台 压力测试工具 siege  使用参考   在线手机号生成器 在线身份证生成器 手册类 Linux linux 命令行手册 Linux 命令大全 浏览器模拟 linux 快乐的 Linux 命令行 LINUX 大棚 我使用过的 Linux 命令系列总目录 Java 程序员眼中的 Linux Redis  官方网站 Redis 中国用户组 Redis 中文网 Redis 命令参考 MySQL  MySQL 入门教程 Git Git 的奇技淫巧 项目类 SpringBoot 教程 demo springboot 脚手架  spring-boot 项目实践总结-各种例子 SpringBootUnity 纯洁的微笑 DD 嘟嘟独立博客 梁桂钊 后台管理项目 BootDo 基于 SpringBoot,更简洁的后台管理系统 Guns iBase4J-SpringBoot hsweb 企业后台管理系统基础框架 人人开源 SpringBootAdmin 微服务快速开发脚手架 springboot-hadmin 前后端分离的基础权限管理后台 轻量级销售团队管理 博客项目 响应式开源个人博客 foblog cms 系统，mybatis-guice 集成通用 Mapper 通用 Mapper 和分页插件脚手架 Guns jcalaBlog 支付项目 小柒支付整合项目 xxpay 龙果支付 best-pay-sdk Jigsaw Payment 开源支付项目 xpay 规则引擎 springBoot-drools SpringCloud 纯洁的微笑 汇总 方志朋 DD wangkang80 尹吉欢 easy-cloud Spring-Cloud-AG-Admin 通用工具 hutool 接口管理工具 JApiDocs 阿里妈妈出品：RAP Mock.js 作者博客 网易出品：NEI 接口管理平台 swagger 的改版 运维工具 useful-scripts：脚本集淘宝 oldratlee 阿里巴巴 druid-spring-boot-starter 其他 SpringBatch 读取 txt 文件 美女爬虫 各种 Java 示例项目 Java Examples ",
      "url"      : "http://zhangjinmiao.github.io/blog/2018/03/13/resource-site.html",
      "keywords" : "github,手册, 汇总,项目,工具,资源"
    } ,
  
    {
      "title"    : "开启我的博客生活",
      "category" : "Blog",
      "content": "首记 Hello，大家好，很高兴与大家见面，从今天开始我将在这里记录我的工作与生活的点滴，希望大家支持！ 现在开始吧CPU 一直转。 ​ ​ t t t t t t t t t t t t t t t t t t t—— 2018.03.13 记  时间过的真快，一晃一年时间已经过去，回想 2018 年博客记录没有坚持，2019 年得改掉拖延的坏习惯，坚持把博客写下去！ ​ t t t t t t t t t t t t t t t t t t t—— 2019.03.22 记 ",
      "url"      : "http://zhangjinmiao.github.io/blog/2018/03/13/start-my-blog-life.html",
      "keywords" : "blog,life"
    } ,
  
    {
      "title"    : "工具神器",
      "category" : "Blog",
      "content": "图片类  Iconfont-阿里巴巴矢量图标库 pixabay高清图片 图床 iPic 图床 uPic 屏幕截图 Greenshot 画图工具 draw.io processon 编辑器 sublimetext 公众号排版 录屏软件 OBS Studio 中文版 视频转换 HandBrake 可将任意视频文件转换为任意其他视频格式，支持批量。 命令行  iTerm2 + Oh My Zsh 打造舒适终端体验 开发辅助  Server 酱 pushbear 参考：  Awesome Mac ",
      "url"      : "http://zhangjinmiao.github.io/blog/2018/03/13/tools.html",
      "keywords" : "画图,工具"
    } ,
  
    {
      "title"    : "Map源码解析之HashMap源码分析",
      "category" : "SourceCode",
      "content": "注：文中源码为 JDK 1.8 。 实现原理 HashMap 是数组 + 链表 + 红黑树（JDK1.8 增加了红黑树部分）实现的。 HashMap 的工作原理 HashMap 基于 hashing 原理，当我们往 HashMap 中 put 元素时，先根据 key 的 hash 值得到这个 Entry 元素在数组中的位置（即下标），然后把这个 Entry 元素放到对应的位置中，如果这个 Entry 元素所在的位子上已经存放有其他元素就在同一个位子上的 Entry 元素以链表的形式存放，新加入的放在链头( JDK 1.8 以前碰撞节点会在链表头部插入，而 JDK 1.8 开始碰撞节点会在链表尾部插入，对于扩容操作后的节点转移 JDK 1.8 以前转移前后链表顺序会倒置，而 JDK 1.8 中依然保持原序。)，从 HashMap 中 get  Entry 元素时先计算 key 的 hashcode，找到数组中对应位置的某一 Entry 元素，然后通过 key 的 equals 方法在对应位置的链表中找到需要的 Entry 元素，所以 HashMap 的数据结构是数组和链表的结合，此外 HashMap 中 key 和 value 都允许为 null，key 为 null 的键值对永远都放在以 table[0] 为头结点的链表中。 HashMap 在每个链表节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么 它们会储存在同一个bucket位置的链表中。键对象的equals()方法用来找到键值对。 Node[] table 的初始化长度 length (默认值是16)，Load factor 为负载因子(默认值是 0.75 )，threshold 是HashMap 所能容纳的最大数据量的 Node (键值对)个数。 threshold 就是在此 Load factor 和 length (数组长度)对应下允许的最大元素数目，超过这个数目就重新 resize (扩容)，扩容后的 HashMap 容量是之前容量的两倍。 在 HashMap 中，哈希桶数组 table 的长度 length 大小必须为 2 的 n 次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数。 HashMap 采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap 定位哈希桶索引位置时，也加入了高位参与运算的过程。 当链表长度太长（默认超过 8 ）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高 HashMap 的性能。 属性和构造函数  初始容量：表示哈希表在其容量自动增加之前可以达到多满的一种尺度  加载因子：当哈希表中的条目超过了容量和加载因子的乘积的时候，就会进行重哈希操作。 //======常量========== // 默认初始化容量16 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // 最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认负载因子 加载因子过高，容易产生哈希冲突，加载因子过小，容易浪费空间，0.75是一种折中。 static final float DEFAULT_LOAD_FACTOR = 0.75f; // ======属性======= transient Node&lt;K,V&gt;[] table; // 哈希桶数组 // transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; // 大小 transient int size; // 用来记录HashMap内部结构发生变化的次数 transient int modCount; // 所能容纳的key-value对极限 (int)(capacity * load factor) int threshold; //哈希表的装载因子 final float loadFactor; Node 是 HashMap 的一个内部类，实现了 Map.Entry 接口，本质是就是一个映射(键值对)。 static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; {  final int hash; //用来定位数组索引位置  final K key;  V value;  Node&lt;K,V&gt; next; //链表的下一个node  Node(int hash, K key, V value, Node&lt;K,V&gt; next) {   this.hash = hash;   this.key = key;   this.value = value;   this.next = next;  }  public final K getKey()  { return key; }  public final V getValue() { return value; }  public final String toString() { return key + = + value; }  public final int hashCode() {   return Objects.hashCode(key) ^ Objects.hashCode(value);  }  public final V setValue(V newValue) {   V oldValue = value;   value = newValue;   return oldValue;  }  public final boolean equals(Object o) {   if (o == this)    return true;   if (o instanceof Map.Entry) {    Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o;    if (Objects.equals(key, e.getKey()) &amp;&amp;    Objects.equals(value, e.getValue()))    return true;   }   return false;  }  } hash(Object key) 确定哈希桶数组索引位置，本质上就三部：取 key 的 hashCode 值、高位运算、取模运算。 方法一： static final int hash(Object key) { //jdk1.8 &amp; jdk1.7  int h;  // h = key.hashCode() 为第一步 取hashCode值  // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算  return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); } 方法二： static int indexFor(int h, int length) { //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的  return h &amp; (length-1); //第三步 取模运算 } 只要它的 hashCode() 返回值相同，那么程序调用方法一所计算得到的 Hash 码值总是相同的。 put(K key, V value)  当我们往 HashMap 中 put 元素的时候，先根据 key 的 hashCode 重新计算 hash 值，根据 hash 值得到这个元素在数组中的位置（即下标），如果数组该位置上已经存放有其他元素了，那么在这个位置上的元素将以链表的形式存放，新加入的放在链头（1.8 以前），最先加入的放在链尾。如果数组该位置上没有元素，就直接将该元素放到此数组中的该位置上。 // 添加一个元素 public V put(K key, V value) { // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); } // 内部实现 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) {  Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i;  t t// 步骤①：tab为空则创建  if ((tab = table) == null || (n = tab.length) == 0)   n = (tab = resize()).length;  t t// 步骤②：计算index，并对null做处理  if ((p = tab[i = (n - 1) &amp; hash]) == null)   tab[i] = newNode(hash, key, value, null);  else {   Node&lt;K,V&gt; e; K k;   // 步骤③：节点key存在，直接覆盖value   if (p.hash == hash &amp;&amp;    ((k = p.key) == key || (key != null &amp;&amp; key.equals(k))))    e = p;   // 步骤④：判断该链为红黑树   else if (p instanceof TreeNode)    e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value);  // 步骤⑤：该链为链表   else {    for (int binCount = 0; ; ++binCount) {    if ((e = p.next) == null) {     p.next = newNode(hash, key, value, null);     // 链表长度大于8转换为红黑树进行处理     if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st      treeifyBin(tab, hash);     break;    }    // key已经存在直接覆盖value    if (e.hash == hash &amp;&amp;     ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))     break;    p = e;    }   }   if (e != null) { // existing mapping for key    V oldValue = e.value;    if (!onlyIfAbsent || oldValue == null)    e.value = value;    afterNodeAccess(e);    return oldValue;   }  }  ++modCount; // 步骤⑥：超过最大容量 就扩容  if (++size &gt; threshold)   resize();  afterNodeInsertion(evict);  return null;  }  读取 get(Object key) // 根据key获取value public V get(Object key) { Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; } // 内部方法 /** * Implements Map.get and related methods * * @param hash hash for key * @param key the key * @return the node, or null if none */ final Node&lt;K,V&gt; getNode(int hash, Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // table有元素 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp;  (first = tab[(n - 1) &amp; hash]) != null) {  // 从第1个node开始  if (first.hash == hash &amp;&amp; // always check first node  ((k = first.key) == key || (key != null &amp;&amp; key.equals(k))))  return first;  // first的下一个node  if ((e = first.next) != null) {  // 若是链表  if (first instanceof TreeNode)  return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key);  // 否则循环查找  do {  if (e.hash == hash &amp;&amp;   ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))   return e;  } while ((e = e.next) != null);  }  // 就一个node，first，取的还不是它，直接退出 } // table没元素直接返回null return null; } 移除元素remove(Object key) // 根据key移除元素，并返回移除的元素 public V remove(Object key) { Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ?  null : e.value; } 扩容机制resize 当元素个数 &gt; 数组大小 （默认 16 ）* 负载因子（默认 0.75f ）时开始扩容，即默认情况下元素个数大于 12 个时，数组大小扩为 2 * 16 = 32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常消耗性能的操作，所以如果我们已经预知 HashMap 中元素的个数，那么预设元素的个数能够有效的提高 HashMap 的性能。 使用一个新的数组代替已有的容量小的数组 HashMap 并发问题 多线程put后可能导致get死循环 由于 HashMap 是线程不安全的，高并发情况下会出现死循环，即会出现 环形链表，进而导致 CPU 被占满的情况。 具体分析： 单线程情况下，HashMap 重复插入某个值的时候，会覆盖之前的值，这个没错。但在多线程访问的时候，由于其内部实现机制(在多线程环境且未作同步的情况下，对同一个 HashMap 做 put 操作可能导致两个或以上线程同时做 rehash 动作，就可能导致循环键表出现，一旦出现线程将无法终止，持续占用 CPU ，导致 CPU 使用率居高不下)，就可能出现安全问题了。 办法：使用jstack工具dump出问题的那台服务器的栈信息，查看具体日志信息。 注意：不合理使用HashMap导致出现的是死循环而不是死锁。 多线程put的时候可能导致元素丢失 主要问题出在addEntry方法的new Entry (hash, key, value, e)，如果两个线程都同时取得了e,则他们下一个元素都是e，然后赋值给table元素的时候有一个成功有一个丢失。 三种解决方案  Hashtable替换HashMap Collections.synchronizedMap将HashMap包装起来 ConcurrentHashMap替换HashMap （推荐 ） 总结  扩容是一个特别耗性能的操作，所以当程序员在使用 HashMap 的时候，估算 map 的大小，初始化的时候给一个大致的数值，避免 map 进行频繁的扩容。 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 HashMap 是线程不安全的，不要在并发的环境中同时操作 HashMap，建议使用 ConcurrentHashMap。 JDK1.8 引入红黑树大程度优化了 HashMap 的性能。 使用可变对象做 Key 会造成 value 找不到的情况，所以使用String，Integer等不可变类型用作 Key。 延伸阅读 哈希 Hash，一般翻译做“散列”，也有直接音译为“哈希”的，就是把任意长度的输入，通过散列算法，变换成固定长度的输出，该输出就是散列值。这种转换是一种压缩映射，也就是，散列值的空间通常远小于输入的空间，不同的输入可能会散列成相同的输出，所以不可能从散列值来唯一的确定输入值。简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。 所有散列函数都有如下一个基本特性：根据同一散列函数计算出的散列值如果不同，那么输入值肯定也不同。但是，根据同一散列函数计算出的散列值如果相同，输入值不一定相同。 两个不同的输入值，根据同一散列函数计算出的散列值相同的现象叫做碰撞 常见的Hash函数有以下几个：  直接定址法：直接以关键字k或者k加上某个常数（k+c）作为哈希地址。 数字分析法：提取关键字中取值比较均匀的数字作为哈希地址。 除留余数法：用关键字k除以某个不大于哈希表长度m的数p，将所得余数作为哈希表地址。 分段叠加法：按照哈希表地址位数将关键字分成位数相等的几部分，其中最后一部分可以比较短。然后将这几部分相加，舍弃最高进位后的结果就是该关键字的哈希地址。 平方取中法：如果关键字各个部分分布都不均匀的话，可以先求出它的平方值，然后按照需求取中间的几位作为哈希地址。 伪随机数法：采用一个伪随机数当作哈希函数。 解决碰撞的方法：  开放定址法：   开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。   链地址法：   将哈希表的每个单元作为链表的头结点，所有哈希地址为i的元素构成一个同义词链表。即发生冲突时就把该关键字链在以该单元为头结点的链表的尾部。   再哈希法：   当哈希地址发生冲突用其他的函数计算另一个哈希函数地址，直到冲突不再产生为止。   建立公共溢出区    将哈希表分为基本表和溢出表两部分，发生冲突的元素都放入溢出表中。  ​    参考：  美团点评技术团队 Java 8系列之重新认识HashMap    疫苗：Java HashMap的死循环   潜龙勿用   Hollis ​  ",
      "url"      : "http://zhangjinmiao.github.io/sourcecode/2018/03/20/HashMap.html",
      "keywords" : "Map,HashMap,源码"
    } ,
  
    {
      "title"    : "Redis 命令",
      "category" : "Redis",
      "content": "1. Key 2. String 3. Hash Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象，能够存储 key 对多个属性的数据。 4. List Redis Lists 基于 Linked List 实现。这意味着即使在一个 list 中有数百万个元素，在头部或尾部添加一个元素的操作，其时间复杂度也是常数级别的。用 LPUSH 命令在十个元素的list头部添加新元素，和在千万元素 list 头部添加新元素的速度相同。 Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 5. Set Redis 的 Set 是 String 类型的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。 6. Sorted set Redis 有序集合和集合一样也是 string 类型元素的集合，且不允许重复的成员。不同的是每个元素都会关联一个double 类型的分数，redis 正是通过分数来为集合中的成员进行从小到大的排序。 7. 其他 redis 127.0.0.1:6380&gt; time # 显示服务器时间 , 时间戳(秒), 微秒数 1) 1375270361 2) 504511 redis 127.0.0.1:6380&gt; dbsize # 当前数据库的key的数量 (integer) 2 Flushall  清空所有库所有键 Flushdb  清空当前库所有键 Showdown [save/nosave] 注: 如果不小心运行了flushall, 立即 shutdown nosave ,关闭服务器 然后 手工编辑 aof 文件, 去掉文件中的 “flushall ”相关行, 然后开启服务器,就可以导入回原来数据. 如果,flushall之后,系统恰好bgrewriteaof了,那么aof就清空了,数据丢失. ",
      "url"      : "http://zhangjinmiao.github.io/redis/2018/03/29/redis-command.html",
      "keywords" : "redis, key, string,"
    } ,
  
    {
      "title"    : "Redis 配置",
      "category" : "Redis",
      "content": "1. 组件介绍 redis-benchmark、redis-check-dump、redis-sentinel、redis-check-aof、redis-cli、redis-server 构成了redis软件包，下面我们来了解一下它们分别是用来干什么的。    组件  用途     redis-server  Redis服务器的启动程序。    redis-cli  Redis命令行操作工具。当然，你也可以用telnet根据其纯文本协议来操作。    redis-benchmark  Redis性能测试工具，测试Redis在你的系统及你的配置下的读写性能    redis-stat  Redis状态检测工具，可以检测Redis当前状态参数及延迟状况。   2. 配置文件 redis 服务相关参数都需要在 redis.conf 文件中进行配置，所以我们有必要花点时间来简单了解一下 conf 文件中的基本参数。    参数  作用     daemonize  是否以后台daemon方式运行redis服务    port redis  服务端口，默认6379    Timeout  请求超时时间    requirepass  连接数据库密码   redis.conf 中 daemonize 参数默认为no，为了让 redis 服务在后台运行，我们需要将 daemonize 参数设置为yes。 文件参数详细说明： 生产环境不建议一个Redis实例数据太多{（20-30）G数据内存对应（96-128）G实际内存）}，这种20%-23%的比例比较合适，因为磁盘读到内存的恢复时间也很慢，可以使用ssd磁盘来提高磁盘读取速度。 一个64G的内存服务器，建议不要超过23%的比例的数据，即64*0.23G=15G。 内存管理开销大（不要超过物理内存的3/5）。buffer io 可能会造成系统内存溢出（OOM-Out of Memory）。 3. 持久化配置 Redis 的持久化有 2 种方式：   1快照  2是日志   Rdb 快照的配置选项 save 900 1      // 900内,有1条写入,则产生快照 save 300 1000   // 如果300秒内有1000次写入,则产生快照 save 60 10000  // 如果60秒内有10000次写入,则产生快照 (这 3 个选项都屏蔽，则 rdb 禁用) stop-writes-on-bgsave-error yes  // 后台备份进程出错时,主进程停不停止写入? rdbcompression yes    // 导出的rdb文件是否压缩 Rdbchecksum   yes //  导入rbd恢复时数据时,要不要检验rdb的完整性 dbfilename dump.rdb  //导出来的rdb文件名 dir ./  // rdb 的放置路径 Aof 的配置 appendonly no # 是否打开 aof日志功能   appendfsync always   # 每1个命令,都立即同步到aof. 安全,速度慢 appendfsync everysec # 折衷方案,每秒写1次 appendfsync no      # 写入工作交给操作系统,由操作系统判断缓冲区大小,统一写入到aof. 同步频率低,速度快, no-appendfsync-on-rewrite  yes: # 正在导出rdb快照的过程中,要不要停止同步aof auto-aof-rewrite-percentage 100 #aof文件大小比起上次重写时的大小,增长率100%时,重写 auto-aof-rewrite-min-size 64mb #aof文件,至少超过64M时,重写 解答几个问题：  注: 在 dump rdb 过程中，aof 如果停止同步，会不会丢失?   答: 不会，所有的操作缓存在内存的队列里，dump 完成后，统一操作。   注: aof 重写是指什么?   答: aof 重写是指把内存中的数据逆化成命令，写入到.aof日志里。  以解决 aof 日志过大的问题。   问: 如果 rdb 文件和 aof 文件都存在，优先用谁来恢复数据?   答: aof   问: 2 种是否可以同时用? 答: 可以，而且推荐这么做   问: 恢复时 rdb 和 aof 哪个恢复的快 答: rdb 快,，因为其是数据的内存映射,直接载入到内存，而 aof 是命令，需要逐条执行 ​ 4. 慢查询 Slowlog 显示慢查询 注:多慢才叫慢?  答: 由slowlog-log-slower-than 10000 ,来指定,(单位是微秒)   服务器储存多少条慢查询的记录?  答: 由 slowlog-max-len 128 ,来做限制 查看redis服务器的信息  Info [Replication/CPU/Memory..] 5. Redis运维时需要注意的参数 1: 内存 Memory used_memory:859192 数据结构的空间 used_memory_rss:7634944 实占空间 mem_fragmentation_ratio:8.89 前2者的比例,1.N为佳,如果此值过大,说明redis的内存的碎片化严重,可以导出再导入一次. 2: 主从复制 Replication role:slave master_host:192.168.1.128 master_port:6379 master_link_status:up 3:持久化 Persistence rdb_changes_since_last_save:0 rdb_last_save_time:1375224063 4: fork耗时 Status latest_fork_usec:936 上次导出rdb快照,持久化花费微秒 注意: 如果某实例有10G内容,导出需要2分钟, 每分钟写入10000次,导致不断的rdb导出,磁盘始处于高IO状态. 5: 慢日志 config get/set slowlog-log-slower-than CONFIG get/SET slowlog-max-len slowlog get N 获取慢日志 注:   以解决 aof 日志过大的问题.   问: 如果rdb文件，和aof 文件都存在,优先用谁来恢复数据? 答: aof   问: 2 种是否可以同时用? 答: 可以，而且推荐这么做   问: 恢复时 rdb 和 aof 哪个恢复的快 答: rdb快，因为其是数据的内存映射,直接载入到内存，而 aof 是命令，需要逐条执行 ",
      "url"      : "http://zhangjinmiao.github.io/redis/2018/03/29/redis-config.html",
      "keywords" : "redis"
    } ,
  
    {
      "title"    : "Redis 简介",
      "category" : "Redis",
      "content": "1. 什么是redis Redis—— Remote Dictionary Server，它是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API，我们可使用它构建高性能，可扩展的Web应用程序。 Redis是目前最流行的键值对存储数据库，从2010年3月15日起，Redis的开发工作由VMware主持。从2013年5月开始，Redis的开发由Pivotal赞助。 如果你想了解Redis最新的资讯，可以访问  官方网站  Redis作者博客   中文社区   huangz/blog   gitchat 购买的课程 ​ 2. 特点 相对于其他的同类型数据库而言，Redis 支持更多的数据类型，除了和 string 外，还支持 lists（列表）、sets（集合）和 zsets（有序集合），hash 几种数据类型。 这些数据类型都支持 push/pop、add/remove 及取交集、并集和差集及更丰富的操作，而且这些操作都是原子性的。Redis 具备以下特点：  异常快速： Redis 数据库完全在内存中，因此处理速度非常快，单机qps（每秒的并发）可以达到的速度是110000次/s，写的速度是81000次/s，适合小数据量高速读写访问。 ​   数据持久化： redis 支持数据持久化，可以将内存中的数据存储到磁盘上，方便在宕机等突发情况下快速恢复。   方式一（RDB）：根据指定的规则，定时周期性的把内存中更新的数据写入到磁盘里。RDB的方式是通过快照（snapshot）完成，当符合规则时redis会把内存的数据生成一个副本并存储在硬盘中，这个过程称之为“快照”。  方式二（AOF）：把修改的操作记录追加到文件里，默认情况redis没有开启AOF方式，可以通过appendonly命令来启用，如：appendonly yes。  两种方式的区别：RDB方式性能较高，但是可能会引起一定程度的数据丢失，AOF方式正好相反。   支持丰富的数据类型： Redis不仅仅支持简单的key-value类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。 ​   单进程单线程高性能服务器：启动一个实例只能用一个CPU，所以用Redis可以用多个实例，一个实例用一个CPU以便提高效率。 ​   crash safe 和 recovery slow redis 崩溃后，数据相对安全，但是恢复起来比较缓慢，所以生产环境不建议一个Redis实例数据太多{（20-30）G数据内存对应（96-128）G实际内存）}，这种20%-23%的比例比较合适，因为磁盘读到内存的恢复时间也很慢，可以使用ssd磁盘来提高磁盘读取速度。 ​   数据一致性： 所有 Redis 操作是 原子 的，这保证了如果两个客户端同时访问的 Redis 服务器将获得更新后的值。单个操作是原子性的。多个操作也支持事务，即原子性，通过 MULTI 和 EXEC 指令包起来。 ​ ​   多功能实用工具： Redis 是一个多实用的工具，可以在多个用例如缓存，消息，队列使用(Redis 原生支持发布/订阅)，任何短暂的数据，应用程序，如 Web 应用程序会话，网页命中计数等。 ​   Redis支持数据的备份： 即 master-slave 模式的数据备份。 ​ ​ 3. Redis缺陷与陷阱 内存管理开销大（不要超过物理内存的3/5）。buffer io 可能会造成系统内存溢出（OOM-Out of Memory）。 4. 应用场景 在实际生产环境中，很多公司都曾经使用过这样的架构，使用MySQL进行海量数据存储的，通过Memcached将热点数据加载到cache，加速访问，但随着业务数据量的不断增加，和访问量的持续增长，我们遇到了很多问题：  MySQL需要不断进行拆库拆表，Memcached也需不断跟着扩容，扩容和维护工作占据大量开发时间。 Memcached与MySQL数据库数据一致性问题。 Memcached数据命中率低或down机，大量访问直接穿透到DB，MySQL无法支撑。 跨机房cache同步问题。 以上问题都是非常的棘手，不过现在不用担心了，因为我们可以使用redis来完美解决。  会话缓存（Session Cache） Redis 缓存会话有非常好的优势，因为 Redis 提供持久化，在需要长时间保持会话的应用场景中，如购物车场景这样的场景中能提供很好的长会话支持，能给用户提供很好的购物体验。   全页缓存 在 WordPress 中，Pantheon 提供了一个不错的插件wp-redis，这个插件能以最快的速度加载你曾经浏览过的页面。   队列 Reids 提供 list 和 set 操作，这使得 Redis 能作为一个很好的消息队列平台来使用。 我们常通过 Reids 的队列功能做购买限制。比如到节假日或者推广期间，进行一些活动，对用户购买行为进行限制，限制今天只能购买几次商品或者一段时间内只能购买一次，也比较适合适用。   排名 Redis 在内存中对数字进行递增或递减的操作实现得非常好。所以我们在很多排名的场景中会应用 Redis 来进行，比如小说网站对小说进行排名，根据排名，将排名靠前的小说推荐给用户。   发布/订阅 Redis 提供发布和订阅功能，发布和订阅的场景很多，比如我们可以基于发布和订阅的脚本触发器，实现用Redis 的发布和订阅功能建立起来的聊天系统。 5. Redis与其他key-value存储有什么不同？ Redis 有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。Redis 的数据类型都是基于基本数据结构的，同时对程序员透明，无需进行额外的抽象。 Redis 运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，因为数据量不能大于硬件内存。在内存数据库方面的另一个优点是， 相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样 Redis 可以做很多内部复杂性很强的事情。 同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 6. Redis集群简单介绍 Redis有3种集群策略  主从 1台机器可写作为主，另外2台可读，作为从，类似于MySQL的主从复制，不过Redis没有BINLOG机制。   哨兵 增加一台机器作为哨兵，监控3台主从机器，当主节点挂机的时候，机器内部进行选举，从集群中从节点里指定一台机器升级为主节点，从而实现高可用。当主节点恢复的时候，加入到从节点中继续提供服务。   集群 Redis3.0以后增加了集群的概念，可以实现多主多从结构，实现正真的高可用。 7. 事务 Redis 支持简单的事务。   Redis与 mysql事务的对比        Mysql  Redis     开启  start transaction  muitl    语句  普通sql  普通命令    失败  rollback 回滚  discard 取消    成功  commit  exec     注: rollback与discard 的区别 如果已经成功执行了2 条语句， 第 3 条语句出错  Rollback 后,前 2 条的语句影响消失 Discard 只是结束本次事务，前 2 条语句造成的影响仍然还在   注: 在 mutil 后面的语句中, 语句出错可能有 2 种情况 1: 语法就有问题, 这种,exec 时,报错, 所有语句得不到执行   2: 语法本身没错,但适用对象有问题. 比如 zadd 操作 list 对象 Exec 之后,会执行正确的语句,并跳过有不适当的语句。 例子： 我正在买票 Ticket -1 , money -100 而票只有1张, 如果在我 multi 之后,和 exec 之前, 票被别人买了—即 ticket 变成 0 了 我该如何观察这种情景,并不再提交？   悲观的想法: 世界充满危险,肯定有人和我抢, 给 ticket上锁, 只有我能操作。[悲观锁]   乐观的想法: 没有那么人和我抢,因此,我只需要注意， 有没有人更改ticket的值就可以了 [乐观锁] Redis的事务中,启用的是乐观锁,只负责监测key没有被改动 具体的命令—-  watch命令 例: redis 127.0.0.1:6379&gt; watch ticket OK redis 127.0.0.1:6379&gt; multi OK redis 127.0.0.1:6379&gt; decr ticket QUEUED redis 127.0.0.1:6379&gt; decrby money 100 QUEUED redis 127.0.0.1:6379&gt; exec (nil) // 返回nil,说明监视的ticket已经改变了,事务就取消了. redis 127.0.0.1:6379&gt; get ticket 0 redis 127.0.0.1:6379&gt; get money 200 watch key1 key2  … keyN 作用：监听key1 key2..keyN有没有变化,如果有变, 则事务取消  unwatch 作用: 取消所有watch监听 ",
      "url"      : "http://zhangjinmiao.github.io/redis/2018/03/29/redis-introduction.html",
      "keywords" : "redis"
    } ,
  
    {
      "title"    : "Redis 使用规约",
      "category" : "Redis",
      "content": "1. 活跃度查询 setbit 的使用 场景: 1亿个用户, 每个用户 登陆/做任意操作  ,记为 今天活跃,否则记为不活跃   每周评出: 有奖活跃用户: 连续7天活动 每月评,等等… 思路:    Userid  dt  active     1  2017-07-27  1    1  2017-07-26  1     如果是放在表中, 1:表急剧增大,2:要用group ,sum运算,计算较慢   用: 位图法 bit-map Log0721:  ‘011001……………0’ …… log0726 :   ‘011001……………0’ Log0727 :  ‘0110000………….1’   1: 记录用户登陆: 每天按日期生成一个位图, 用户登陆后,把user_id位上的bit值置为1   2: 把1周的位图  and 计算, 位上为1的,即是连续登陆的用户    redis 127.0.0.1:6379&gt; setbit mon 100000000 0 (integer) 0 redis 127.0.0.1:6379&gt; setbit mon 3 1 (integer) 0 redis 127.0.0.1:6379&gt; setbit mon 5 1 (integer) 0 redis 127.0.0.1:6379&gt; setbit mon 7 1 (integer) 0 redis 127.0.0.1:6379&gt; setbit thur 100000000 0 (integer) 0 redis 127.0.0.1:6379&gt; setbit thur 3 1 (integer) 0 redis 127.0.0.1:6379&gt; setbit thur 5 1 (integer) 0 redis 127.0.0.1:6379&gt; setbit thur 8 1 (integer) 0 redis 127.0.0.1:6379&gt; setbit wen 100000000 0 (integer) 0 redis 127.0.0.1:6379&gt; setbit wen 3 1 (integer) 0 redis 127.0.0.1:6379&gt; setbit wen 4 1 (integer) 0 redis 127.0.0.1:6379&gt; setbit wen 6 1 (integer) 0 redis 127.0.0.1:6379&gt; bitop and  res mon feb wen (integer) 12500001 如上例,优点: 1: 节约空间, 1亿人每天的登陆情况,用1亿bit,约1200WByte,约10M 的字符就能表示 2: 计算方便 2. 消息订阅 使用办法: 订阅端: Subscribe 频道名称 发布端: publish 频道名称 发布内容   客户端例子: redis 127.0.0.1:6379&gt; subscribe news Reading messages... (press Ctrl-C to quit) 1) subscribe 2) news 3) (integer) 1 1) message 2) news 3) good good study 1) message 2) news 3) day day up 服务端例子: redis 127.0.0.1:6379&gt; publish news 'good good study' (integer) 1 redis 127.0.0.1:6379&gt; publish news 'day day up' (integer) 1 3. 标签云 使用 MySQL： create table book ( bookid int, title char(20) )engine myisam charset utf8; insert into book values (5 , 'PHP圣经'), (6 , 'ruby实战'), (7 , 'mysql运维') (8, 'ruby服务端编程'); create table tags ( tid int, bookid int, content char(20) )engine myisam charset utf8; insert into tags values (10 , 5 , 'PHP'), (11 , 5 , 'WEB'), (12 , 6 , 'WEB'), (13 , 6 , 'ruby'), (14 , 7 , 'database'), (15 , 8 , 'ruby'), (16 , 8 , 'server');  既有web标签,又有PHP,同时还标签的书,要用连接查询 select * from tags inner join tags as t on tags.bookid=t.bookid where tags.content='PHP' and t.content='WEB'; 换成key-value存储 set book:5:title 'PHP圣经' set book:6:title 'ruby实战' set book:7:title 'mysql运难' set book:8:title ‘ruby server’ sadd tag:PHP 5 sadd tag:WEB 5 6 sadd tag:database 7 sadd tag:ruby 6 8 sadd tag:SERVER 8 查: 既有PHP,又有WEB的书 Sinter tag:PHP tag:WEB #查集合的交集 查: 有PHP或有WEB标签的书 Sunin tag:PHP tag:WEB  u001e查:含有ruby,不含WEB标签的书 Sdiff tag:ruby tag:WEB #求差集 4.Redis key 设计技巧 1: 把表名转换为key前缀 如, tag: 2: 第2段放置用于区分区key的字段–对应mysql中的主键的列名,如userid 3: 第3段放置主键值,如2,3,4…., a , b ,c 4: 第4段,写要存储的列名 用户表 user  , 转换为key-value存储    userid  username  password  email     9  Lisi  11111111  lisi@163.com   set user:userid:9:username lisi set user:userid:9:password 111111 set user:userid:9:email lisi@163.com keys user:userid:9* 注意： 在关系型数据中，除主键外，还有可能其他列也步骤查询，如上表中， username 也是极频繁查询的，往往这种列也是加了索引的。   转换到 k-v 数据中，则也要相应的生成一条按照该列为主的key-value  Set  user:username:lisi:uid  9     这样，我们可以根据 username:lisi:uid ，查出 userid=9, 再查user:9:password/email …   完成了根据用户名来查询用户信息。 ",
      "url"      : "http://zhangjinmiao.github.io/redis/2018/03/29/redis-use-manual.html",
      "keywords" : "redis"
    } ,
  
    {
      "title"    : "MySQL 小记",
      "category" : "MySQL",
      "content": "1. InnoDB 表如何设计主键索引 建表语句： a 表： CREATE TABLE `a` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `message_id` int(11) NOT NULL, `user_id` int(11) NOT NULL, `msg` varchar(1024) DEFAULT NULL, `gmt_create` datetime NOT NULL, PRIMARY KEY (`id`), KEY `user_id` (`user_id`,`message_id`), KEY `idx_gmt_create` (`gmt_create`) ) ENGINE=InnoDB DEFAULT CHARSET=gbk; b 表： CREATE TABLE `b` ( `user_id` int(11) NOT NULL, `message_id` int(11) NOT NULL, `msg` varchar(1024) DEFAULT NULL, `gmt_create` datetime NOT NULL, PRIMARY KEY (`user_id`,`message_id`), KEY `idx_gmt_create` (`gmt_create`) ) ENGINE=InnoDB DEFAULT CHARSET=gbk; a 表和 b 表区别：       记录  空间  优点  缺点  适用场景     A 表  500 万（顺序）  509 M  主键 ID 自增,在写入数据的候，Btree 分裂成本低，写性能高  物理空间相对较多，如果根据 user_id 来查记录，需要走两次 IO  写操作较多    B 表  500 万（随机）  361 M  1.物理空间相对减少          2.根据 user_id 查数据,直接走主键拿到数据，无需回表  (user_id,message_id)为随机写入,Btree 分裂成本高,写性能低  写少读多的场景，例如从hadoop回流到MySQL的统计结果表，这种统计结果一般数据较多，但主要是读   2. 字符串索引隐式转换 字段类型为字符串，查询时使用 数值型，该字段所在的索引不会用到。  数字类型的 0001 等价于 1   字符串的 0001 和 1 不等值   当字符串的列有对应的索引，而在 where 条件里面不指定为字符串， index 无法确认最终的记录   被隐式转换后，会进行全表遍历   建表需要注意对应好字段类型 ​ ",
      "url"      : "http://zhangjinmiao.github.io/mysql/2018/04/03/Mysql.html",
      "keywords" : "mysql, 表设计"
    } ,
  
    {
      "title"    : "MySQL 查询优化",
      "category" : "MySQL",
      "content": "1. 数据库性能参数 查询 mysql 数据库的一些运行状态 show status; 查看操作次数 show status like 'Com_(CRUD)'; 查看 mysql 数据库启动多长时间，myisam 存储引擎长时间启动需要进行碎片整理 show status like 'uptime'; 查看慢查询 show status like 'slow_queries'; 查询慢查询时间 show variables like 'long_query_time'; 设置慢查询时间 set long_query_time = 0.5; 2. 查询优化 2.1 查看 SQL 执行计划 使用命令： EXPLAIN/DESCRIBE/DESC SELECT * FROM...;  2.2 每个字段说明 id：SELECT 标识符。这是 SELECT 的查询序列号。 select_type：表示 SELECT 语句的类型。它可以是以下几种取值：  SIMPLE：表示简单查询，其中不包括连接查询和子查询；   PRIMARY：表示主查询，或者最外层的查询语句；    UNION：表示连接查询的第 2 个或后面的查询语句；    DEPENDENT UNION：连接查询中的第 2 个或后面的 SELECT 语句，取决于外面的查询；   UNION RESULT：连接查询的结果；   SUBQUERY：子查询中的第一个 SELECT 语句；    DEPENDENT SUBQUERY：子查询中的第一个 SELECT，取决于外面的查询；   DERIVED：导出表的 SELECT (FROM 语句的子查询）。 table：表示查询的表。 type：（★重要★）表示表的连接类型。下面按照从最佳类型到最差类型的顺序给出各种连接类型：  该表仅有一行的系统表。这是 const 连接类型的一个特例，平时不会出现，这个也可以忽略不计   const： 数据表最多只有一个匹配行，它将在查询开始时被读取，并在余下的査询优化中作为常量对待。const 表查询速度很快，因为它们只读取一次。const 用于使用常数值比较 PRIMARY KEY 或 UNIQUE 索引的所有部分的场合。 在下面查询中，tb1_name 可用 const 表： SELECT * from tb1_name WHERE primary_key=1; SELECT * from tb1_name WHERE primary_key_part1=1 AND primary_key_part2=2    ​   eq_ref mysql 手册是这样说的:”对于每个来自于前面的表的行组合，从该表中读取一行。这可能是最好的联接类型，除了 const 类型。它用在一个索引的所有部分被联接使用并且索引是 UNIQUE 或 PRIMARY KEY”。eq_ref 可以用于使用=比较带索引的列。 在下面例子中，MySQL 可以使用 eq_ref 来处理 ref_tables：  SELECT * FROM ref_table,other_table WHERE ref_table.key_cloumn = other_table.cloumn;  SELECT * FROM ref_table, other_tbale WHERE ref_table.key_cloumn_part1 = other_table.cloumn AND ref_table.key_cloumn_part2 = 1;    ref 对于来自前面的表的任意组合，将从该表中读取所有匹配的行。这种类型用于索引既不是 UNIQUE 也不是 PRIMARY KEY 的情况，或者查询中使用了索引列在左子集，既索引中左边的部分列组合。ref 可以用于使用=或者&lt;=&gt;操作符的带索引的列。 以下的几个例子中，mysql 将使用 ref 来处理 ref_table：  select * from ref_table where key_column=expr; select * from ref_table,other_table where ref_table.key_column=other_table.column; select * from ref_table,other_table where ref_table.key_column_part1=other_table.column and ref_table.key_column_part2=1;    ref_or_null 这种连接类型类似 ref，不同的是 mysql 会在检索的时候额外的搜索包含 null 值的记录。在解决子查询中经常使用该链接类型的优化。 在以下的例子中，mysql 使用 ref_or_null 类型来处理 ref_table：  select * from ref_table where key_column=expr or key_column is null;   上面这五种情况都是很理想的索引使用情况。   index_merge 该链接类型表示使用了索引合并优化方法。在这种情况下，key 列包含了使用的索引的清单，key_len 包含了使用的索引的最长的关键元素。   unique_subquery 该类型替换了下面形式的 IN 子查询的 ref：  value in (select primary_key from single_table where some_expr)   index_subquery 这种连接类型类似 unique_subquery。可以替换 IN 子查询，不过它用于在子查询中没有唯一索引的情况下， 例如以下形式： value in (select key_column from single_table where some_expr)   range 只检索给定范围的行，使用一个索引来选择行。key 列显示使用了哪个索引。ken_len 包含所使用索引的最长关键元素。当使用 =, &lt;&gt;, &gt;,&gt;=, &lt;, &lt;=, is null, &lt;=&gt;, between, 或 in 操作符，用常量比较关键字列时，类型为 range。 下面介绍几种检索制定行的情况： select * from tbl_name where key_column = 10; select * from tbl_name where key_column between 10 and 20; select * from tbl_name where key_column in (10,20,30); select * from tbl_name where key_part1= 10 and key_part2 in (10,20,30);    ​   index 连接类型跟 ALL 一样，不同的是它只扫描索引树。它通常会比 ALL 快点，因为索引文件通常比数据文件小。   ALL （性能最差） 对于前面的表的任意行组合，进行完整的表扫描。如果第一个表没有被标识为 const 的话就不大好了，在其他情况下通常是非常糟糕的。正常地，可以通过增加索引使得能从表中更快的取得记录以避免 ALL。 possible_keys possible_keys 字段是指 MySQL 在搜索表记录时可能使用哪个索引。如果这个字段的值是 NULL，就表示没有索引被用到。这种情况下，就可以检查 WHERE 子句中哪些字段哪些字段适合增加索引以提高查询的性能。创建一下索引，然后再用 explain 检查一下。 key key 字段显示了 MySQL 实际上要用的索引。当没有任何索引被用到的时候，这个字段的值就是 NULL。想要让 MySQL 强行使用或者忽略在 possible_keys 字段中的索引列表，可以在查询语句中使用关键字 force index, use index 或 ignore index。参考 SELECT 语法。 key_len ​key_len 字段显示了 mysql 使用索引的长度。当 key 字段的值为 NULL 时，索引的长度就是 NULL。注意，key_len 的值可以告诉你在联合索引中 MySQL 会真正使用了哪些索引。 ref ​表示使用哪个列或常数与索引一起来查询记录。 rows 显示 MySQL 在表中进行查询时必须检查的行数。 Extra 本字段显示了查询中 mysql 的附加信息。以下是这个字段的几个不同值的解释：    distinct  MySQL 当找到当前记录的匹配联合结果的第一条记录之后，就不再搜索其他记录了。    not exists  MySQL 在查询时做一个 LEFT JOIN 优化时，当它在当前表中找到了和前一条记录符合 LEFT JOIN 条件后，就不再搜索更多的记录了。下面是一个这种类型的查询例子：  select * from t1 left join t2 on t1.id=t2.id where t2.id is null;   假使 t2.id 定义为 not null。这种情况下，MySQL 将会扫描表 t1 并且用 t1.id 的值在 t2 中查找记录。当在 t2 中找到一条匹配的记录时，这就意味着 t2.id 肯定不会都是 null，就不会再在 t2 中查找相同 id 值的其他记录了。也可以这么说，对于 t1 中的每个记录，mysql 只需要在 t2 中做一次查找，而不管在 t2 中实际有多少匹配的记录。    range checked for each record (index map: #)  MySQL 没找到合适的可用的索引。取代的办法是，对于前一个表的每一个行连接，它会做一个检验以决定该使用哪个索引（如果有的话），并且使用这个索引来从表里取得记录。这个过程不会很快，但总比没有任何索引时做表连接来得快。    using filesort  MySQL 需要额外的做一遍从已排好的顺序取得记录。排序程序根据连接的类型遍历所有的记录，并且将所有符合 where 条件的记录的要排序的键和指向记录的指针存储起来。这些键已经排完序了，对应的记录也会按照排好的顺序取出来。  using index  字段的信息直接从索引树中的信息取得，而不再去扫描实际的记录。这种策略用于查询时的字段是一个独立索引的一部分。  using temporary  MySQL 需要创建临时表存储结果以完成查询。这种情况通常发生在查询时包含了 group by 和 order by 子句，它以不同的方式列出了各个字段。  using where  where 子句将用来限制哪些记录匹配了下一个表或者发送给客户端。除非你特别地想要取得或者检查表中的所有记录，否则的话当查询的 extra 字段值不是 using where 并且表连接类型是 all 或 index 时可能表示有问题。  Using sort_union(…), Using union(…), Using intersect(…)  这些函数说明如何为 index_merge 联接类型合并索引扫描   u0019Using index for group-by  类似于访问表的 Using index 方式,Using index for group-by 表示 MySQL 发现了一个索引,可以用来查 询 GROUP BY 或 DISTINCT 查询的所有列,而不要额外搜索硬盘访问实际的表。 如果你想要让查询尽可能的快，那么就应该注意 extra 字段的值为 using filesort 和 using temporary 的情况。 说明 SQL 需要优化了。 2.3 使用索引查询需要注意 索引可以提供查询的速度，但并不是使用了带有索引的字段查询都会生效，有些情况下是不生效的，需要注意。 2.3.1 使用 LIKE 关键字的查询 在使用 LIKE 关键字进行查询的查询语句中，如果匹配字符串的第一个字符为“%”，索引不起作用。只有“%”不在第一个位置，索引才会生效。 2.3.2 使用联合索引的查询 MySQL 可以为多个字段创建索引，一个索引可以包括 16 个字段。对于联合索引，只有查询条件中使用了这些字段中第一个字段时，索引才会生效。  2.3.3 使用 OR 关键字的查询 查询语句的查询条件中只有 OR 关键字，且 OR 前后的两个条件中的列都是索引时，索引才会生效，否则，索引不生效。 2.4 子查询优化 MySQL 从 4.1 版本开始支持子查询，使用子查询进行 SELECT 语句嵌套查询，可以一次完成很多逻辑上需要多个步骤才能完成的 SQL 操作。 子查询虽然很灵活，但是执行效率并不高。 执行子查询时，MYSQL 需要创建临时表，查询完毕后再删除这些临时表，所以，子查询的速度会受到一定的影响。   优化： 可以使用连接查询（JOIN）代替子查询，连接查询时不需要建立临时表，其速度比子查询快。 3. 数据库结构优化 一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。 需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。 3.1 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。 因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。 3.2 增加中间表 对于需要经常联合查询的表，可以建立中间表以提高查询效率。 通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 3.3 增加冗余字段 设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。 表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。 注意： 冗余字段的值在一个表中修改，就要想办法在其他表中更新，否则就会导致数据不一致的问题。 4. 插入数据的优化 插入数据时，影响插入速度的主要是索引、唯一性校验、一次插入的数据条数等。  插入数据的优化，不同的存储引擎优化手段不一样，在 MySQL 中常用的存储引擎有，MyISAM 和 InnoDB，两者的区别： 参考：http://www.cnblogs.com/panfeng412/archive/2011/08/16/2140364.html 4.1 MyISAM 4.1.1 禁用索引 对于非空表，插入记录时，MySQL 会根据表的索引对插入的记录建立索引。如果插入大量数据，建立索引会降低插入数据速度。 为了解决这个问题，可以在批量插入数据之前禁用索引，数据插入完成后再开启索引。 禁用索引的语句： ALTER TABLE table_name DISABLE KEYS 开启索引语句： ALTER TABLE table_name ENABLE KEYS 对于空表批量插入数据，则不需要进行操作，因为 MyISAM 引擎的表是在导入数据后才建立索引。 4.1.2 禁用唯一性检查 唯一性校验会降低插入记录的速度，可以在插入记录之前禁用唯一性检查，插入数据完成后再开启。   禁用唯一性检查的语句： SET UNIQUE_CHECKS = 0;  开启唯一性检查的语句: SET UNIQUE_CHECKS = 1; 4.1.3 批量插入数据 插入数据时，可以使用一条 INSERT 语句插入一条数据，也可以插入多条数据。 第二种方式的插入速度比第一种方式快。 4.1.4 使用 LOAD DATA INFILE 当需要批量导入数据时，使用 LOAD DATA INFILE 语句比 INSERT 语句插入速度快很多。 4.2 InnoDB 4.2.1 禁用唯一性检查 用法和 MyISAM 一样。 4.2.2 禁用外键检查 插入数据之前执行禁止对外键的检查，数据插入完成后再恢复，可以提供插入速度。 禁用： SET foreign_key_checks = 0; 开启： SET foreign_key_checks = 1; 4.2.3 禁止自动提交 插入数据之前执行禁止事务的自动提交，数据插入完成后再恢复，可以提高插入速度。  禁用： SET autocommit = 0; 开启： SET autocommit = 1; 5. 服务器优化 5.1 优化服务器硬件 服务器的硬件性能直接决定着 MySQL 数据库的性能，硬件的性能瓶颈，直接决定 MySQL 数据库的运行速度和效率。   需要从以下几个方面考虑： 1、 配置较大的内存。足够大的内存，是提高 MySQL 数据库性能的方法之一。内存的 IO 比硬盘快的多，可以增加系统的缓冲区容量，使数据在内存停留的时间更长，以减少磁盘的 IO。 2、 配置高速磁盘，比如 SSD。 3、 合理分配磁盘 IO，把磁盘 IO 分散到多个设备上，以减少资源的竞争，提高并行操作能力。 4、 配置多核处理器，MySQL 是多线程的数据库，多处理器可以提高同时执行多个线程的能力。 5.2 优化 MySQL 参数 通过优化 MySQL 的参数可以提高资源利用率，从而达到提高 MySQL 服务器性能的目的。 MySQL 的配置参数都在 my.conf 或者 my.ini 文件的 [mysqld] 组中，常用的参数如下： 6. 优化 LIMIT 分页 在分页偏移量很大的时候，如 LIMIT 10000,20 这样的查询，MySQL 需要查询 10020 条记录然后只返回最后 20 条，前面 10000 条记录都被抛弃，这样代价非常高。 优化的最简单的办法就是尽可能地使用索引覆盖扫描，而不是查询所有的列。然后根据需要做一次关联操作再返回所需的列，对于偏移量很大的时候，这样做的效率回提升很大，如下： SELECT file_id, description FROM film ORDER BY title LIMIT 50, 5; 修改为： SELECT film.film_id, film.description FROM film  INNER JOIN (SELCT film_id FROM film ORDER BY title LIMIT 50, 5) AS lim USING(film_id); 这里“延迟关联”将大大提升查询效率，它让 MySQL 扫描尽可能少的页面，获取需要访问的记录后在根据关联列回原表查询需要的所有列。 ",
      "url"      : "http://zhangjinmiao.github.io/mysql/2018/04/03/Mysql%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96.html",
      "keywords" : "mysql, 优化"
    } ,
  
    {
      "title"    : "1.volatile 关键字 内存可见性",
      "category" : "JUC",
      "content": "volatile 关键字 内存可见性 内存可见性  内存可见性（Memory Visibility） 是指当某个线程正在使用对象状态而另一个线程在同时修改该状态，需要确保当一个线程修改了对象状态后，其他线程能够看到发生的状态变化。   可见性错误是指当读操作与写操作在不同的线程中执行时，我们无法确保执行读操作的线程能适时地看到其他线程写入的值，有时甚至是根本不可能的事情。   我们可以通过同步来保证对象被安全地发布。除此之外我们也可以使用一种更加轻量级的 volatile 变量。 代码示例： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 /* * 一、volatile 关键字：当多个线程进行操作共享数据时，可以保证内存中的数据可见。 *  t t t t t 相较于 synchronized 是一种较为轻量级的同步策略。 * * 注意： * 1. volatile 不具备“互斥性” * 2. volatile 不能保证变量的“原子性” */ public class TestVolatile {  t  tpublic static void main(String[] args) {  t tThreadDemo td = new ThreadDemo();  t tnew Thread(td).start();  t t  t twhile(true){  t t tif(td.isFlag()){  t t t tSystem.out.println(------------------);  t t t tbreak;  t t t}  t t}  t t  t} } class ThreadDemo implements Runnable {  tprivate volatile boolean flag = false;  t@Override  tpublic void run() {  t t  t ttry {  t t tThread.sleep(200);  t t} catch (InterruptedException e) {  t t}  t tflag = true;  t t  t tSystem.out.println(flag= + isFlag());  t}  tpublic boolean isFlag() {  t treturn flag;  t}  tpublic void setFlag(boolean flag) {  t tthis.flag = flag;  t} } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/1.volatile%E5%85%B3%E9%94%AE%E5%AD%97%E5%86%85%E5%AD%98%E5%8F%AF%E8%A7%81%E6%80%A7.html",
      "keywords" : "juc, 并发, volatile"
    } ,
  
    {
      "title"    : "10.线程八锁",
      "category" : "JUC",
      "content": "线程八锁  一个对象里面如果有多个 synchronized 方法，某一个时刻内，只要一个线程去调用其中的一个synchronized 方法了，其它的线程都只能等待，换句话说，某一个时刻内，只能有唯一一个线程去访问这些 synchronized 方法   锁的是当前对象 this，被锁定后，其它的线程都不能进入到当前对象的其它的 synchronized 方法   加个普通方法后发现和同步锁无关   换成两个对象后，不是同一把锁了，情况立刻变化   都换成静态同步方法后，情况又变化   所有的非静态同步方法用的都是同一把锁——实例对象本身，也就是说如果一个实例对象的非静态同步方法获取锁后，该实例对象的其他非静态同步方法必须等待获取锁的方法释放锁后才能获取锁，可是别的实例对象的非静态同步方法因为跟该实例对象的非静态同步方法用的是不同的锁，所以毋须等待该实例对象已获取锁的非静态同步方法释放锁就可以获取他们自己的锁。   所有的静态同步方法用的也是同一把锁——类对象本身，这两把锁是两个不同的对象，所以静态同步方法与非静态同步方法之间是不会有竞态条件的。但是一旦一个静态同步方法获取锁后，其他的静态同步方法都必须等待该方法释放锁后才能获取锁，而不管是同一个实例对象的静态同步方法之间，还是不同的实例对象的静态同步方法之间，只要它们同一个类的实例对象 ​ 代码示例： /* * 题目：判断打印的 one or two ？ * * 1. 两个普通同步方法，两个线程，标准打印， 打印? //one two * 2. 新增 Thread.sleep() 给 getOne() ,打印? //one two * 3. 新增普通方法 getThree() , 打印? //three one two * 4. 两个普通同步方法，两个 Number 对象，打印? //two one * 5. 修改 getOne() 为静态同步方法，打印? //two one * 6. 修改两个方法均为静态同步方法，一个 Number 对象? //one two * 7. 一个静态同步方法，一个非静态同步方法，两个 Number 对象? //two one * 8. 两个静态同步方法，两个 Number 对象? //one two * * 线程八锁的关键： * ①非静态方法的锁默认为 this, 静态方法的锁为 对应的 Class 实例 * ②某一个时刻内，只能有一个线程持有锁，无论几个方法。 */ public class TestThread8Monitor {  t  tpublic static void main(String[] args) {  t tNumber number = new Number();  t tNumber number2 = new Number();  t t  t tnew Thread(new Runnable() {  t t t@Override  t t tpublic void run() {  t t t tnumber.getOne();  t t t}  t t}).start();  t t  t tnew Thread(new Runnable() {  t t t@Override  t t tpublic void run() { // t t t tnumber.getTwo();  t t t tnumber2.getTwo();  t t t}  t t}).start();  t t  t t/*new Thread(new Runnable() {  t t t@Override  t t tpublic void run() {  t t t tnumber.getThree();  t t t}  t t}).start();*/  t t  t} } class Number{  t  tpublic static synchronized void getOne(){//Number.class  t ttry {  t t tThread.sleep(3000);  t t} catch (InterruptedException e) {  t t}  t t  t tSystem.out.println(one);  t}  t  tpublic synchronized void getTwo(){//this  t tSystem.out.println(two);  t}  t  tpublic void getThree(){  t tSystem.out.println(three);  t}  t } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/10.%E7%BA%BF%E7%A8%8B%E5%85%AB%E9%94%81.html",
      "keywords" : "juc, 并发, 线程八锁"
    } ,
  
    {
      "title"    : "11.线程池",
      "category" : "JUC",
      "content": "线程池  第四种获取线程的方法：线程池，一个 ExecutorService，它使用可能的几个池线程之一执行每个提交的任务，通常使用 Executors 工厂方法配置。   线程池可以解决两个不同问题：由于减少了每个任务调用的开销，它们通常可以在执行大量异步任务时提供增强的性能，并且还可以提供绑定和管理资源（包括执行 任务集时使用的线程）的方法。每个 ThreadPoolExecutor 还维护着一些基本的统计数 据，如完成的任务数。   为了便于跨大量上下文使用，此类提供了很多可调整的参数和扩展钩子 (hook)。但是，强烈建议程序员使用较为方便的 Executors 工厂方法：     u0004Executors.newCachedThreadPool()（无界线程池，可以进行自动线程回收）     Executors.newFixedThreadPool(int)（固定大小线程池）     Executors.newSingleThreadExecutor()（单个后台线程）  ​   它们均为大多数使用场景预定义了设置。 ​ 代码示例： /* * 一、线程池：提供了一个线程队列，队列中保存着所有等待状态的线程。避免了创建与销毁额外开销，提高了响应的速度。 * * 二、线程池的体系结构： *  tjava.util.concurrent.Executor : 负责线程的使用与调度的根接口 *  t t|--**ExecutorService 子接口: 线程池的主要接口 *  t t t|--ThreadPoolExecutor 线程池的实现类 *  t t t|--ScheduledExecutorService 子接口：负责线程的调度 *  t t t t|--ScheduledThreadPoolExecutor ：继承 ThreadPoolExecutor， 实现 ScheduledExecutorService * * 三、工具类 : Executors * ExecutorService newFixedThreadPool() : 创建固定大小的线程池 * ExecutorService newCachedThreadPool() : 缓存线程池，线程池的数量不固定，可以根据需求自动的更改数量。 * ExecutorService newSingleThreadExecutor() : 创建单个线程池。线程池中只有一个线程 * * ScheduledExecutorService newScheduledThreadPool() : 创建固定大小的线程，可以延迟或定时的执行任务。 */ public class TestThreadPool {  t  tpublic static void main(String[] args) throws Exception {  t t//1. 创建线程池  t tExecutorService pool = Executors.newFixedThreadPool(5);  t t  t tList&lt;Future&lt;Integer&gt;&gt; list = new ArrayList&lt;&gt;();  t t  t tfor (int i = 0; i &lt; 10; i++) {  t t tFuture&lt;Integer&gt; future = pool.submit(new Callable&lt;Integer&gt;(){  t t t t@Override  t t t tpublic Integer call() throws Exception {  t t t t tint sum = 0;  t t t t t  t t t t tfor (int i = 0; i &lt;= 100; i++) {  t t t t t tsum += i;  t t t t t}  t t t t t  t t t t treturn sum;  t t t t}  t t t t  t t t});  t t tlist.add(future);  t t}  t t  t tpool.shutdown();  t t  t tfor (Future&lt;Integer&gt; future : list) {  t t tSystem.out.println(future.get());  t t}  t t  t t  t t  t t/*ThreadPoolDemo tpd = new ThreadPoolDemo();  t t  t t//2. 为线程池中的线程分配任务  t tfor (int i = 0; i &lt; 10; i++) {  t t tpool.submit(tpd);  t t}  t t  t t//3. 关闭线程池  t tpool.shutdown();*/  t}  t // tnew Thread(tpd).start(); // tnew Thread(tpd).start(); } class ThreadPoolDemo implements Runnable{  tprivate int i = 0;  t  t@Override  tpublic void run() {  t twhile(i &lt;= 100){  t t tSystem.out.println(Thread.currentThread().getName() + : + i++);  t t}  t}  t } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/11.%E7%BA%BF%E7%A8%8B%E6%B1%A0.html",
      "keywords" : "juc, 并发, 线程池"
    } ,
  
    {
      "title"    : "12.线程调度",
      "category" : "JUC",
      "content": "ScheduledExecutorService 一个 ExecutorService，可安排在给定的延迟后运行或定期执行的命令。 代码示例： /* * 一、线程池：提供了一个线程队列，队列中保存着所有等待状态的线程。避免了创建与销毁额外开销，提高了响应的速度。 * * 二、线程池的体系结构： *  tjava.util.concurrent.Executor : 负责线程的使用与调度的根接口 *  t t|--**ExecutorService 子接口: 线程池的主要接口 *  t t t|--ThreadPoolExecutor 线程池的实现类 *  t t t|--ScheduledExecutorService 子接口：负责线程的调度 *  t t t t|--ScheduledThreadPoolExecutor ：继承 ThreadPoolExecutor， 实现 ScheduledExecutorService * * 三、工具类 : Executors * ExecutorService newFixedThreadPool() : 创建固定大小的线程池 * ExecutorService newCachedThreadPool() : 缓存线程池，线程池的数量不固定，可以根据需求自动的更改数量。 * ExecutorService newSingleThreadExecutor() : 创建单个线程池。线程池中只有一个线程 * * ScheduledExecutorService newScheduledThreadPool() : 创建固定大小的线程，可以延迟或定时的执行任务。 */ public class TestScheduledThreadPool {  tpublic static void main(String[] args) throws Exception {  t tScheduledExecutorService pool = Executors.newScheduledThreadPool(5);  t t  t tfor (int i = 0; i &lt; 5; i++) {  t t tFuture&lt;Integer&gt; result = pool.schedule(new Callable&lt;Integer&gt;(){  t t t t@Override  t t t tpublic Integer call() throws Exception {  t t t t tint num = new Random().nextInt(100);//生成随机数  t t t t tSystem.out.println(Thread.currentThread().getName() + : + num);  t t t t treturn num;  t t t t}  t t t t  t t t}, 1, TimeUnit.SECONDS);  t t t  t t tSystem.out.println(result.get());  t t}  t t  t tpool.shutdown();  t}  t } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/12.%E7%BA%BF%E7%A8%8B%E8%B0%83%E5%BA%A6.html",
      "keywords" : "juc, 并发, 线程调度, ScheduledExecutorService"
    } ,
  
    {
      "title"    : "13.ForkJoinPool 分支合并框架 工作窃取",
      "category" : "JUC",
      "content": "Fork/Join 框架  Fork/Join 框架：就是在必要的情况下，将一个大任务，进行拆分(fork)成若干个小任务（拆到不可再拆时），再将一个个的小任务运算的结果进行 join 汇总。  Fork/Join 框架与线程池的区别  采用“工作窃取”模式（work-stealing）： 当执行新的任务时它可以将其拆分分成更小的任务执行，并将小任务加到线程队列中，然后再从一个随机线程的队列中偷一个并把它放在自己的队列中。   相对于一般的线程池实现，fork/join 框架的优势体现在对其中包含的任务的处理方式上。在一般的线程池中，如果一个线程正在执行的任务由于某些原因无法继续运行，那么该线程会处于等待状态。而在 fork/join 框架实现中，如果某个子问题由于等待另外一个子问题的完成而无法继续运行。那么处理该子问题的线程会主动寻找其他尚未运行的子问题来执行。这种方式减少了线程的等待时间，提高了性能。 ​ 代码示例： public class TestForkJoinPool {  t  tpublic static void main(String[] args) {  t tInstant start = Instant.now();  t t  t tForkJoinPool pool = new ForkJoinPool();  t t  t tForkJoinTask&lt;Long&gt; task = new ForkJoinSumCalculate(0L, 50000000000L);  t t  t tLong sum = pool.invoke(task);  t t  t tSystem.out.println(sum);  t t  t tInstant end = Instant.now();  t t  t tSystem.out.println(耗费时间为： + Duration.between(start, end).toMillis());//166-1996-10590  t}  t  t@Test  tpublic void test1(){  t tInstant start = Instant.now();  t t  t tlong sum = 0L;  t t  t tfor (long i = 0L; i &lt;= 50000000000L; i++) {  t t tsum += i;  t t}  t t  t tSystem.out.println(sum);  t t  t tInstant end = Instant.now();  t t  t tSystem.out.println(耗费时间为： + Duration.between(start, end).toMillis());//35-3142-15704  t}  t  t//java8 新特性  t@Test  tpublic void test2(){  t tInstant start = Instant.now();  t t  t tLong sum = LongStream.rangeClosed(0L, 50000000000L)  t t t t t t t .parallel()  t t t t t t t .reduce(0L, Long::sum);  t t  t tSystem.out.println(sum);  t t  t tInstant end = Instant.now();  t t  t tSystem.out.println(耗费时间为： + Duration.between(start, end).toMillis());//1536-8118  t} } class ForkJoinSumCalculate extends RecursiveTask&lt;Long&gt;{  t/**  t *  t */  tprivate static final long serialVersionUID = -259195479995561737L;  t  tprivate long start;  tprivate long end;  t  tprivate static final long THURSHOLD = 10000L; //临界值  t  tpublic ForkJoinSumCalculate(long start, long end) {  t tthis.start = start;  t tthis.end = end;  t}  t@Override  tprotected Long compute() {  t tlong length = end - start;  t t  t tif(length &lt;= THURSHOLD){  t t tlong sum = 0L;  t t t  t t tfor (long i = start; i &lt;= end; i++) {  t t t tsum += i;  t t t}  t t t  t t treturn sum;  t t}else{  t t tlong middle = (start + end) / 2;  t t t  t t tForkJoinSumCalculate left = new ForkJoinSumCalculate(start, middle);  t t tleft.fork(); //进行拆分，同时压入线程队列  t t t  t t tForkJoinSumCalculate right = new ForkJoinSumCalculate(middle+1, end);  t t tright.fork(); //  t t t  t t treturn left.join() + right.join();  t t}  t}  t } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/13.ForkJoinPool-%E5%88%86%E6%94%AF%E5%90%88%E5%B9%B6%E6%A1%86%E6%9E%B6%E5%B7%A5%E4%BD%9C%E7%AA%83%E5%8F%96.html",
      "keywords" : "juc, 并发, ForkJoin"
    } ,
  
    {
      "title"    : "2.原子变量 CAS 算法",
      "category" : "JUC",
      "content": "原子变量 CAS 算法 CAS 算法  CAS (Compare-And-Swap) 是一种硬件对并发的支持，针对多处理器操作而设计的处理器中的一种特殊指令，用于管理对共享数据的并发访问。   CAS 是一种无锁的非阻塞算法的实现。 CAS 包含了 3 个操作数：  需要读写的内存值 V  进行比较的值 A  拟写入的新值 B   当且仅当 V 的值等于 A 时， CAS 通过原子方式用新值 B 来更新 V 的 值，否则不会执行任何操作。 原子变量  类的小工具包，支持在单个变量上解除锁的线程安全编程。事实上，此包中的类可将 volatile 值、字段和数组元素的概念扩展到那些也提供原子条件更新操作的类。  类 AtomicBoolean、AtomicInteger、AtomicLong 和 AtomicReference 的实例各自提供对相应类型单个变量的访问和更新。每个类也为该类型提供适当的实用工具方法。 AtomicIntegerArray、AtomicLongArray 和 AtomicReferenceArray 类进一步扩展了原子操 作，对这些类型的数组提供了支持。这些类在为其数组元素提供 volatile 访问语义方 面也引人注目，这对于普通数组来说是不受支持的。 核心方法：boolean compareAndSet(expectedValue, updateValue) **  java.util.concurrent.atomic 包下提供了一些原子操作的常用类:  AtomicBoolean、AtomicInteger、AtomicLong、 AtomicReference  AtomicIntegerArray、AtomicLongArray  AtomicMarkableReference  AtomicReferenceArray  AtomicStampedReference   代码示例： /* * 一、i++ 的原子性问题：i++ 的操作实际上分为三个步骤“读-改-写” *  t t int i = 10; *  t t i = i++; //10 * *  t t int temp = i; *  t t i = i + 1; *  t t i = temp; * * 二、原子变量：在 java.util.concurrent.atomic 包下提供了一些原子变量。 *  t t1. volatile 保证内存可见性 *  t t2. CAS（Compare-And-Swap） 算法保证数据变量的原子性 *  t t tCAS 算法是硬件对于并发操作的支持 *  t t tCAS 包含了三个操作数： *  t t t①内存值 V *  t t t②预估值 A *  t t t③更新值 B *  t t t当且仅当 V == A 时， V = B; 否则，不会执行任何操作。 */ public class TestAtomicDemo {  tpublic static void main(String[] args) {  t tAtomicDemo ad = new AtomicDemo();  t t  t tfor (int i = 0; i &lt; 10; i++) {  t t tnew Thread(ad).start();  t t}  t}  t } class AtomicDemo implements Runnable{  t // tprivate volatile int serialNumber = 0;  t  tprivate AtomicInteger serialNumber = new AtomicInteger(0);  t@Override  tpublic void run() {  t t  t ttry {  t t tThread.sleep(200);  t t} catch (InterruptedException e) {  t t}  t t  t tSystem.out.println(getSerialNumber());  t}  t  tpublic int getSerialNumber(){  t treturn serialNumber.getAndIncrement();  t} t } 模拟 CAS 的代码： public class TestCompareAndSwap {  tpublic static void main(String[] args) {  t tfinal CompareAndSwap cas = new CompareAndSwap();  t t  t tfor (int i = 0; i &lt; 10; i++) {  t t tnew Thread(new Runnable() {  t t t t  t t t t@Override  t t t tpublic void run() {  t t t t tint expectedValue = cas.get();  t t t t tboolean b = cas.compareAndSet(expectedValue, (int)(Math.random() * 101));  t t t t tSystem.out.println(b);  t t t t}  t t t}).start();  t t}  t t  t}  t } class CompareAndSwap{  tprivate int value;  t  t//获取内存值  tpublic synchronized int get(){  t treturn value;  t}  t  t//比较  tpublic synchronized int compareAndSwap(int expectedValue, int newValue){  t tint oldValue = value;  t t  t tif(oldValue == expectedValue){  t t tthis.value = newValue;  t t}  t t  t treturn oldValue;  t}  t  t//设置  tpublic synchronized boolean compareAndSet(int expectedValue, int newValue){  t treturn expectedValue == compareAndSwap(expectedValue, newValue);  t} } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/2.%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8FCAS%E7%AE%97%E6%B3%95.html",
      "keywords" : "juc, 并发, CAS"
    } ,
  
    {
      "title"    : "3.ConcurrentHashMap 锁分段机制",
      "category" : "JUC",
      "content": "ConcurrentHashMap  Java 5.0 在 java.util.concurrent 包中提供了多种并发容器类来改进同步容器的性能。   ConcurrentHashMap 同步容器类是 Java 5 增加的一个线程安全的哈希表。对与多线程的操作，介于 HashMap 与 Hashtable 之间。内部采用“锁分段 ”机制替代 Hashtable 的独占锁。进而提高性能。   此包还提供了设计用于多线程上下文中的 Collection 实现：   ConcurrentHashMap、ConcurrentSkipListMap、ConcurrentSkipListSet、CopyOnWriteArrayList 和 CopyOnWriteArraySet。当期望许多线程访问一个给定 collection 时，ConcurrentHashMap 通常优于同步的 HashMap， ConcurrentSkipListMap 通常优于同步的 TreeMap。当期望的读数和遍历远远大于列表的更新数时，CopyOnWriteArrayList 优于同步的 ArrayList。   代码示例： /* * CopyOnWriteArrayList/CopyOnWriteArraySet : “写入并复制” * 注意：添加操作多时，效率低，因为每次添加时都会进行复制，开销非常的大。并发迭代操作多时可以选择。 */ public class TestCopyOnWriteArrayList {  tpublic static void main(String[] args) {  t tHelloThread ht = new HelloThread();  t t  t tfor (int i = 0; i &lt; 10; i++) {  t t tnew Thread(ht).start();  t t}  t}  t } class HelloThread implements Runnable{  t // tprivate static List&lt;String&gt; list = Collections.synchronizedList(new ArrayList&lt;String&gt;());  t  tprivate static CopyOnWriteArrayList&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;();  t  tstatic{  t tlist.add(AA);  t tlist.add(BB);  t tlist.add(CC);  t}  t@Override  tpublic void run() {  t t  t tIterator&lt;String&gt; it = list.iterator();  t t  t twhile(it.hasNext()){  t t tSystem.out.println(it.next());  t t t  t t tlist.add(AA);  t t}  t t  t} } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/3.ConcurrentHashMap%E9%94%81%E5%88%86%E6%AE%B5%E6%9C%BA%E5%88%B6.html",
      "keywords" : "juc, 并发, ConcurrentHashMap"
    } ,
  
    {
      "title"    : "4.CountDownLatch 闭锁",
      "category" : "JUC",
      "content": "CountDownLatch  Java 5.0 在 java.util.concurrent 包中提供了多种并发容器类来改进同步容器的性能。   CountDownLatch 一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。   闭锁可以延迟线程的进度直到其到达终止状态，闭锁可以用来确保某些活动直到其他活动都完成才继续执行：   确保某个计算在其需要的所有资源都被初始化之后才继续执行;  确保某个服务在其依赖的所有其他服务都已经启动之后才启动;  等待直到某个操作所有参与者都准备就绪再继续执行   ​ 代码示例： /* * CountDownLatch ：闭锁，在完成某些运算是，只有其他所有线程的运算全部完成，当前运算才继续执行 */ public class TestCountDownLatch {  tpublic static void main(String[] args) {  t tfinal CountDownLatch latch = new CountDownLatch(50);  t tLatchDemo ld = new LatchDemo(latch);  t tlong start = System.currentTimeMillis();  t tfor (int i = 0; i &lt; 50; i++) {  t t tnew Thread(ld).start();  t t}  t ttry {  t t tlatch.await();  t t} catch (InterruptedException e) {  t t}  t tlong end = System.currentTimeMillis();  t tSystem.out.println(耗费时间为： + (end - start));  t} } class LatchDemo implements Runnable {  tprivate CountDownLatch latch;  tpublic LatchDemo(CountDownLatch latch) {  t tthis.latch = latch;  t}  t@Override  tpublic void run() {  t ttry {  t t tfor (int i = 0; i &lt; 50000; i++) {  t t t tif (i % 2 == 0) {  t t t t tSystem.out.println(i);  t t t t}  t t t}  t t} finally {  t t tlatch.countDown();  t t}  t} } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/4.CountDownLatch%E9%97%AD%E9%94%81.html",
      "keywords" : "juc, 并发, CountDownLatch"
    } ,
  
    {
      "title"    : "5.实现 Callable 接口",
      "category" : "JUC",
      "content": "Callable 接口  Java 5.0 在 java.util.concurrent 提供了一个新的创建执行线程的方式：Callable 接口   Callable 接口类似于 Runnable，两者都是为那些其实例可能被另一个线程执行的类设计的。但是 Runnable 不会返回结果，并且无法抛出经过检查的异常。   Callable 需要依赖 FutureTask ，FutureTask 也可以用作闭锁。 ​ 代码示例： /* * 一、创建执行线程的方式三：实现 Callable 接口。 相较于实现 Runnable 接口的方式，方法可以有返回值，并且可以抛出异常。 * * 二、执行 Callable 方式，需要 FutureTask 实现类的支持，用于接收运算结果。 FutureTask 是 Future 接口的实现类 */ public class TestCallable {  t  tpublic static void main(String[] args) {  t tThreadDemo td = new ThreadDemo();  t t  t t//1.执行 Callable 方式，需要 FutureTask 实现类的支持，用于接收运算结果。  t tFutureTask&lt;Integer&gt; result = new FutureTask&lt;&gt;(td);  t t  t tnew Thread(result).start();  t t  t t//2.接收线程运算后的结果  t ttry {  t t tInteger sum = result.get(); //FutureTask 可用于 闭锁  t t tSystem.out.println(sum);  t t tSystem.out.println(------------------------------------);  t t} catch (InterruptedException | ExecutionException e) {  t t te.printStackTrace();  t t}  t} } class ThreadDemo implements Callable&lt;Integer&gt;{  t@Override  tpublic Integer call() throws Exception {  t tint sum = 0;  t t  t tfor (int i = 0; i &lt;= 100000; i++) {  t t tsum += i;  t t}  t t  t treturn sum;  t}  t } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/5.%E5%AE%9E%E7%8E%B0-Callable-%E6%8E%A5%E5%8F%A3.html",
      "keywords" : "juc, 并发, Callable"
    } ,
  
    {
      "title"    : "6.Lock 同步锁",
      "category" : "JUC",
      "content": "现实锁 Lock  在 Java 5.0 之前，协调共享对象的访问时可以使用的机制只有 synchronized 和 volatile。Java 5.0 后增加了一些新的机制，但并不是一种替代内置锁的方法，而是当内置锁不适用时，作为一种可选择的高级功能。   ReentrantLock 实现了 Lock 接口，并提供了与 synchronized 相同的互斥性和内存可见性。但相较于 synchronized 提供了更高的处理锁的灵活性。 代码示例： /* * 一、用于解决多线程安全问题的方式： * * synchronized:隐式锁 * 1. 同步代码块 * * 2. 同步方法 * * jdk 1.5 后： * 3. 同步锁 Lock * 注意：是一个显示锁，需要通过 lock() 方法上锁，必须通过 unlock() 方法进行释放锁 */ public class TestLock {  t  tpublic static void main(String[] args) {  t tTicket ticket = new Ticket();  t t  t tnew Thread(ticket, 1号窗口).start();  t tnew Thread(ticket, 2号窗口).start();  t tnew Thread(ticket, 3号窗口).start();  t} } class Ticket implements Runnable{  t  tprivate int tick = 100;  t  tprivate Lock lock = new ReentrantLock();  t@Override  tpublic void run() {  t twhile(true){  t t t  t t tlock.lock(); //上锁  t t t  t t ttry{  t t t tif(tick &gt; 0){  t t t t ttry {  t t t t t tThread.sleep(200);  t t t t t} catch (InterruptedException e) {  t t t t t}  t t t t t  t t t t tSystem.out.println(Thread.currentThread().getName() + 完成售票，余票为： + --tick);  t t t t}  t t t}finally{  t t t tlock.unlock(); //释放锁  t t t}  t t}  t}  t } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/6.Lock%E5%90%8C%E6%AD%A5%E9%94%81.html",
      "keywords" : "juc, 并发, Lock"
    } ,
  
    {
      "title"    : "7.Condition 控制线程通信",
      "category" : "JUC",
      "content": "Condition  Condition 接口描述了可能会与锁有关联的条件变量。这些变量在用法上与使用 Object.wait 访问的隐式监视器类似，但提供了更强大的功能。需要特别指出的是，单个 Lock 可能与多个 Condition 对象关联。为了避免兼容性问题，Condition 方法的名称与对应的 Object 版本中的不同。 在 Condition 对象中，与 wait、notify 和 notifyAll 方法对应的分别是 await、signal 和 signalAll。 Condition 实例实质上被绑定到一个锁上。要为特定 Lock 实例获得 Condition 实例，请使用其 newCondition() 方法。 代码示例： 这里采用生产者-消费者模式。 public class Test1 { public static void main(String[] args) {  Clerk clerk = new Clerk();  Productor pro = new Productor(clerk);  Consumer cus = new Consumer(clerk);  new Thread(pro, 生产者 A).start();  new Thread(cus, 消费者 B).start();  } } //店员 class Clerk {  private int product = 0; //进货  public synchronized void get() {  if (product &gt;= 10) {   System.out.println(产品已满！);  }else{   System.out.println(Thread.currentThread().getName() + : + ++product);  }  } //卖货  public synchronized void sale() {  if (product &lt;= 0) {   System.out.println(缺货！);  }else{   System.out.println(Thread.currentThread().getName() + : + --product);  }  } } //生产者 class Productor implements Runnable {  private Clerk clerk; public Productor(Clerk clerk) {  this.clerk = clerk;  } @Override  public void run() {  for (int i = 0; i &lt; 20; i++) {   clerk.get();  }  } } 输出： 缺货！ 生产者 A : 1 生产者 A : 2 生产者 A : 3 生产者 A : 4 生产者 A : 5 生产者 A : 6 生产者 A : 7 生产者 A : 8 生产者 A : 9 生产者 A : 10 产品已满！ 产品已满！ 产品已满！ 产品已满！ 产品已满！ 产品已满！ 产品已满！ 产品已满！ 产品已满！ 产品已满！ 消费者 B : 9 消费者 B : 8 消费者 B : 7 消费者 B : 6 消费者 B : 5 消费者 B : 4 消费者 B : 3 消费者 B : 2 消费者 B : 1 消费者 B : 0 缺货！ 缺货！ 缺货！ 缺货！ 缺货！ 缺货！ 缺货！ 缺货！ 出现的问题： 生产者和消费者没有唤醒机制  生产者发现没产品会不断的生产 消费者发现没有产品还会继续消费 改进： public class Test2 { public static void main(String[] args) {  Clerk clerk = new Clerk();  Productor pro = new Productor(clerk);  Consumer cus = new Consumer(clerk);  new Thread(pro, 生产者 A).start();  new Thread(cus, 消费者 B).start();  } } //店员 class Clerk {  private int product = 0; //进货  public synchronized void get() {  if (product &gt;= 10) {   System.out.println(产品已满！);   try {    this.wait();   } catch (InterruptedException e) {   }  }else{   System.out.println(Thread.currentThread().getName() + : + ++product);   this.notifyAll();  }  } //卖货  public synchronized void sale() {  if (product &lt;= 0) {   System.out.println(缺货！);   try {    this.wait();   } catch (InterruptedException e) {   }  }else{   System.out.println(Thread.currentThread().getName() + : + --product);   this.notifyAll();  }  } } //生产者 class Productor implements Runnable {  private Clerk clerk; public Productor(Clerk clerk) {  this.clerk = clerk;  } @Override  public void run() {  for (int i = 0; i &lt; 20; i++) {   clerk.get();  }  } } //消费者 class Consumer implements Runnable {  private Clerk clerk; public Consumer(Clerk clerk) {  this.clerk = clerk;  } @Override  public void run() {  for (int i = 0; i &lt; 20; i++) {   clerk.sale();  }  } } 输出： 缺货！ 生产者 A : 1 生产者 A : 2 生产者 A : 3 生产者 A : 4 生产者 A : 5 生产者 A : 6 生产者 A : 7 生产者 A : 8 生产者 A : 9 生产者 A : 10 产品已满！ 消费者 B : 9 消费者 B : 8 消费者 B : 7 消费者 B : 6 消费者 B : 5 消费者 B : 4 消费者 B : 3 消费者 B : 2 消费者 B : 1 消费者 B : 0 缺货！ 生产者 A : 1 生产者 A : 2 生产者 A : 3 生产者 A : 4 生产者 A : 5 生产者 A : 6 生产者 A : 7 生产者 A : 8 生产者 A : 9 消费者 B : 8 消费者 B : 7 消费者 B : 6 消费者 B : 5 消费者 B : 4 消费者 B : 3 消费者 B : 2 消费者 B : 1 可以解决 test1 的问题，但是将 get() 方法中的 10 改为 1 ， //进货  public synchronized void get() {  if (product &gt;= 1) { // 10 -&gt; 1   System.out.println(产品已满！);   try {    this.wait();   } catch (InterruptedException e) {   }  }else{   System.out.println(Thread.currentThread().getName() + : + ++product);   this.notifyAll();  }  } 生产者的 run() 方法延时 0.2 秒 @Override  public void run() {  for (int i = 0; i &lt; 20; i++) {   try {    Thread.sleep(200);   } catch (InterruptedException e) {   }   clerk.get();  }  } 出现的问题：  生产者和消费者任务结束，但是线程还在运行 ，问题在 get() 和 sale() 的 else 段查看 1.gif，生产者处于等待状态，需要线程唤醒他，但是消费者已结束，所以程序一直运行  解决办法： 将 else 移除。 class Clerk {  private int product = 0; //进货  public synchronized void get() {  if (product &gt;= 1) { // 1 只卖1 个   System.out.println(产品已满！);   try {    this.wait();   } catch (InterruptedException e) {   }  }  System.out.println(Thread.currentThread().getName() + : + ++product);  this.notifyAll();  } //卖货  public synchronized void sale() {  if (product &lt;= 0) { // 1   System.out.println(缺货！);   try {    this.wait();   } catch (InterruptedException e) {   }  }  System.out.println(Thread.currentThread().getName() + : + --product);  this.notifyAll();  } } 再看上面代码，再增加一个生产者和一个消费者，出现了产品为负数的情况。 原因是：产生了虚假唤醒 解决：将 if 改为 while,即：wait 在 while 中。 public class Test4 { public static void main(String[] args) {  Clerk clerk = new Clerk();  Productor pro = new Productor(clerk);  Consumer cus = new Consumer(clerk);  new Thread(pro, 生产者 A).start();  new Thread(cus, 消费者 B).start();  new Thread(pro, 生产者 C).start();  new Thread(cus, 消费者 D).start();  } } //店员 class Clerk {  private int product = 0; //进货  public synchronized void get() {  // 为了避免虚假唤醒，应该总是使用在循环中。  while (product &gt;= 1) { // 1 只卖1 个   System.out.println(产品已满！);   try {    this.wait();   } catch (InterruptedException e) {   }  }  System.out.println(Thread.currentThread().getName() + : + ++product);  this.notifyAll();  } //卖货  public synchronized void sale() {  while (product &lt;= 0) { // 1   System.out.println(缺货！);   try {    this.wait();   } catch (InterruptedException e) {   }  }  System.out.println(Thread.currentThread().getName() + : + --product);  this.notifyAll();  } } //生产者 class Productor implements Runnable {  private Clerk clerk; public Productor(Clerk clerk) {  this.clerk = clerk;  } @Override  public void run() {  for (int i = 0; i &lt; 20; i++) {   try {    Thread.sleep(200);   } catch (InterruptedException e) {   }   clerk.get();  }  } } //消费者 class Consumer implements Runnable {  private Clerk clerk; public Consumer(Clerk clerk) {  this.clerk = clerk;  } @Override  public void run() {  for (int i = 0; i &lt; 20; i++) {   clerk.sale();  }  } } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/7.Condition%E6%8E%A7%E5%88%B6%E7%BA%BF%E7%A8%8B%E9%80%9A%E4%BF%A1.html",
      "keywords" : "juc, 并发, Condition"
    } ,
  
    {
      "title"    : "8.线程按序交替",
      "category" : "JUC",
      "content": "编写一个程序，开启 3 个线程，这三个线程的 ID 分别为 A、B、C，每个线程将自己的 ID 在屏幕上打印 10 遍，要求输出的结果必须按顺序显示。 如：ABCABCABC……依次递归 代码示例： /* * 编写一个程序，开启 3 个线程，这三个线程的 ID 分别为 A、B、C，每个线程将自己的 ID 在屏幕上打印 10 遍，要求输出的结果必须按顺序显示。 * t如：ABCABCABC…… 依次递归 */ public class TestABCAlternate {  t  tpublic static void main(String[] args) {  t tAlternateDemo ad = new AlternateDemo();  t t  t tnew Thread(new Runnable() {  t t t@Override  t t tpublic void run() {  t t t t  t t t tfor (int i = 1; i &lt;= 20; i++) {  t t t t tad.loopA(i);  t t t t}  t t t t  t t t}  t t}, A).start();  t t  t tnew Thread(new Runnable() {  t t t@Override  t t tpublic void run() {  t t t t  t t t tfor (int i = 1; i &lt;= 20; i++) {  t t t t tad.loopB(i);  t t t t}  t t t t  t t t}  t t}, B).start();  t t  t tnew Thread(new Runnable() {  t t t@Override  t t tpublic void run() {  t t t t  t t t tfor (int i = 1; i &lt;= 20; i++) {  t t t t tad.loopC(i);  t t t t t  t t t t tSystem.out.println(-----------------------------------);  t t t t}  t t t t  t t t}  t t}, C).start();  t} } class AlternateDemo{  t  tprivate int number = 1; //当前正在执行线程的标记  t  tprivate Lock lock = new ReentrantLock();  tprivate Condition condition1 = lock.newCondition();  tprivate Condition condition2 = lock.newCondition();  tprivate Condition condition3 = lock.newCondition();  t  t/**  t * @param totalLoop : 循环第几轮  t */  tpublic void loopA(int totalLoop){  t tlock.lock();  t t  t ttry {  t t t//1. 判断  t t tif(number != 1){  t t t tcondition1.await();  t t t}  t t t  t t t//2. 打印  t t tfor (int i = 1; i &lt;= 1; i++) {  t t t tSystem.out.println(Thread.currentThread().getName() +   t + i +   t + totalLoop);  t t t}  t t t  t t t//3. 唤醒  t t tnumber = 2;  t t tcondition2.signal();  t t} catch (Exception e) {  t t te.printStackTrace();  t t} finally {  t t tlock.unlock();  t t}  t}  t  tpublic void loopB(int totalLoop){  t tlock.lock();  t t  t ttry {  t t t//1. 判断  t t tif(number != 2){  t t t tcondition2.await();  t t t}  t t t  t t t//2. 打印  t t tfor (int i = 1; i &lt;= 1; i++) {  t t t tSystem.out.println(Thread.currentThread().getName() +   t + i +   t + totalLoop);  t t t}  t t t  t t t//3. 唤醒  t t tnumber = 3;  t t tcondition3.signal();  t t} catch (Exception e) {  t t te.printStackTrace();  t t} finally {  t t tlock.unlock();  t t}  t}  t  tpublic void loopC(int totalLoop){  t tlock.lock();  t t  t ttry {  t t t//1. 判断  t t tif(number != 3){  t t t tcondition3.await();  t t t}  t t t  t t t//2. 打印  t t tfor (int i = 1; i &lt;= 1; i++) {  t t t tSystem.out.println(Thread.currentThread().getName() +   t + i +   t + totalLoop);  t t t}  t t t  t t t//3. 唤醒  t t tnumber = 1;  t t tcondition1.signal();  t t} catch (Exception e) {  t t te.printStackTrace();  t t} finally {  t t tlock.unlock();  t t}  t}  t } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/8.%E7%BA%BF%E7%A8%8B%E6%8C%89%E5%BA%8F%E4%BA%A4%E6%9B%BF.html",
      "keywords" : "juc, 并发, 线程按序交替"
    } ,
  
    {
      "title"    : "9.ReadWriteLock 读写锁",
      "category" : "JUC",
      "content": "ReadWriteLock 读写锁  ReadWriteLock 维护了一对相关的锁，一个用于只读操作，另一个用于写入操作。只要没有 writer，读取锁可以由多个 reader 线程同时保持。写入锁是独占的。   ReadWriteLock 读取操作通常不会改变共享资源，但执行写入操作时，必须独占方式来获取锁。对于读取操作占多数的数据结构。ReadWriteLock 能提供比独占锁更高的并发性。而对于只读的数据结构，其中包含的不变性可以完全不需要考虑加锁操作。 ​ 代码示例： /* * 1. ReadWriteLock : 读写锁 * * 写写/读写 需要“互斥” * 读读 不需要互斥 * */ public class TestReadWriteLock {  tpublic static void main(String[] args) {  t tReadWriteLockDemo rw = new ReadWriteLockDemo();  t t  t tnew Thread(new Runnable() {  t t t  t t t@Override  t t tpublic void run() {  t t t trw.set((int)(Math.random() * 101));  t t t}  t t}, Write:).start();  t t  t t  t tfor (int i = 0; i &lt; 100; i++) {  t t tnew Thread(new Runnable() {  t t t t  t t t t@Override  t t t tpublic void run() {  t t t t trw.get();  t t t t}  t t t}).start();  t t}  t}  t } class ReadWriteLockDemo{  t  tprivate int number = 0;  t  tprivate ReadWriteLock lock = new ReentrantReadWriteLock();  t  t//读  tpublic void get(){  t tlock.readLock().lock(); //上锁  t t  t ttry{  t t tSystem.out.println(Thread.currentThread().getName() + : + number);  t t}finally{  t t tlock.readLock().unlock(); //释放锁  t t}  t}  t  t//写  tpublic void set(int number){  t tlock.writeLock().lock();  t t  t ttry{  t t tSystem.out.println(Thread.currentThread().getName());  t t tthis.number = number;  t t}finally{  t t tlock.writeLock().unlock();  t t}  t} } ",
      "url"      : "http://zhangjinmiao.github.io/juc/2018/04/06/9.ReadWriteLock%E8%AF%BB%E5%86%99%E9%94%81.html",
      "keywords" : "juc, 并发, ReadWriteLock"
    } ,
  
    {
      "title"    : "Redis 面试",
      "category" : "Redis",
      "content": "1. Redis有哪几种数据淘汰策略  noeviction:返回错误当内存限制达到并且客户端尝试执行会让更多内存被使用的命令（大部分的写入指令，但DEL和几个例外）   allkeys-lru: 尝试回收最少使用的键（LRU），使得新添加的数据有空间存放。 volatile-lru: 尝试回收最少使用的键（LRU），但仅限于在过期集合的键,使得新添加的数据有空间存放。  allkeys-random: 回收随机的键使得新添加的数据有空间存放。   volatile-random: 回收随机的键使得新添加的数据有空间存放，但仅限于在过期集合的键。 volatile-ttl: 回收在过期集合的键，并且优先回收存活时间（TTL）较短的键,使得新添加的数据有空间存放。 2.为什么Redis需要把所有数据放到内存中 Redis 为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。所以 redis 具有快速和数据持久化的特征。如果不将数据放在内存中，磁盘 I/O速度为严重影响 redis 的性能。在内存越来越便宜的今天，redis 将会越来越受欢迎。 如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值。 3.Redis集群方案应该怎么做？都有哪些方案?  twemproxy，大概概念是，它类似于一个代理方式，使用方法和普通redis无任何区别，设置好它下属的多个redis实例后，使用时在本需要连接redis的地方改为连接twemproxy，它会以一个代理的身份接收请求并使用一致性hash算法，将请求转接到具体redis，将结果再返回twemproxy。使用方式简便(相对redis只需修改连接端口)，对旧项目扩展的首选。 问题：twemproxy自身单端口实例的压力，使用一致性hash后，对redis节点数量改变时候的计算值的改变，数据无法自动移动到新的节点。   codis，目前用的最多的集群方案，基本和twemproxy一致的效果，但它支持在 节点数量改变情况下，旧节点数据可恢复到新hash节点。   redis cluster3.0自带的集群，特点在于他的分布式算法不是一致性hash，而是hash槽的概念，以及自身支持节点设置从节点。具体看官方文档介绍。   在业务代码层实现，起几个毫无关联的redis实例，在代码层，对key 进行hash计算，然后去对应的redis实例操作数据。 这种方式对hash层代码要求比较高，考虑部分包括，节点失效后的替代算法方案，数据震荡后的自动脚本恢复，实例的监控，等等。 ​ 4.说说Redis哈希槽的概念 Redis 集群没有使用一致性hash,而是引入了哈希槽的概念，Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽。 5.Redis事务相关的命令有哪几个 MULTI、EXEC、DISCARD、WATCH 6.Redis如何做内存优化 尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key,而是应该把这个用户的所有信息存储到一张散列表里面。 7. Redis回收进程如何工作的 一个客户端运行了新的命令，添加了新的数据。 Redi检查内存使用情况，如果大于maxmemory的限制, 则根据设定好的策略进行回收。 一个新的命令被执行，等等。 所以我们不断地穿越内存限制的边界，通过不断达到边界然后不断地回收回到边界以下。 如果一个命令的结果导致大量内存被使用（例如很大的集合的交集保存到一个新的键），不用多久内存限制就会被这个内存使用量超越。 ",
      "url"      : "http://zhangjinmiao.github.io/redis/2018/04/07/redis-interview-note.html",
      "keywords" : "redis"
    } ,
  
    {
      "title"    : "JSON Web Token - 在 Web 应用间安全地传递信息",
      "category" : "JWT",
      "content": "JSON Web Token（JWT）是一个非常轻巧的规范。这个规范允许我们使用 JWT 在用户和服务器之间传递安全可靠的信息。 让我们来假想一下一个场景。在 A 用户关注了 B 用户的时候，系统发邮件给 B 用户，并且附有一个链接“点此关注 A 用户”。链接的地址可以是这样的 https://your.awesome-app.com/make-friend/?from_user=B&amp;target_user=A 上面的 URL 主要通过 URL 来描述这个当然这样做有一个弊端，那就是要求用户 B 用户是一定要先登录的。可不可以简化这个流程，让 B 用户不用登录就可以完成这个操作。JWT 就允许我们做到这点。 JWT 的组成 一个 JWT 实际上就是一个字符串，它由三部分组成，头部、载荷与签名。 载荷（Payload） 我们先将上面的添加好友的操作描述成一个 JSON 对象。其中添加了一些其他的信息，帮助今后收到这个 JWT 的服务器理解这个 JWT。 {    iss: John Wu JWT,    iat: 1441593502,    exp: 1441594722,    aud: www.example.com,    sub: jrocket@example.com,    from_user: B,    target_user: A} 这里面的前五个字段都是由 JWT 的标准所定义的。  iss: 该 JWT 的签发者 sub: 该 JWT 所面向的用户 aud: 接收该 JWT 的一方 exp(expires): 什么时候过期，这里是一个 Unix 时间戳 iat(issued at): 在什么时候签发的 这些定义都可以在标准中找到。 将上面的 JSON 对象进行[base64 编码]可以得到下面的字符串。这个字符串我们将它称作 JWT 的Payload（载荷）。 eyJpc3MiOiJKb2huIFd1IEpXVCIsImlhdCI6MTQ0MTU5MzUwMiwiZXhwIjoxNDQxNTk0NzIyLCJhdWQiOiJ3d3cuZXhhbXBsZS5jb20iLCJzdWIiOiJqcm9ja2V0QGV4YW1wbGUuY29tIiwiZnJvbV91c2VyIjoiQiIsInRhcmdldF91c2VyIjoiQSJ9 如果你使用 Node.js，可以用 Node.js 的包 base64url 来得到这个字符串。 var base64url = require('base64url')var header = {    from_user: B,    target_user: A}console.log(base64url(JSON.stringify(header)))// 输出：eyJpc3MiOiJKb2huIFd1IEpXVCIsImlhdCI6MTQ0MTU5MzUwMiwiZXhwIjoxNDQxNTk0NzIyLCJhdWQiOiJ3d3cuZXhhbXBsZS5jb20iLCJzdWIiOiJqcm9ja2V0QGV4YW1wbGUuY29tIiwiZnJvbV91c2VyIjoiQiIsInRhcmdldF91c2VyIjoiQSJ9 小知识：Base64 是一种编码，也就是说，它是可以被翻译回原来的样子来的。它并不是一种加密过程。 头部（Header） JWT 还需要一个头部，头部用于描述关于该 JWT 的最基本的信息，例如其类型以及签名所用的算法等。这也可以被表示成一个 JSON 对象。 {  typ: JWT,  alg: HS256} 在这里，我们说明了这是一个 JWT，并且我们所用的签名算法（后面会提到）是 HS256 算法。 对它也要进行 Base64 编码，之后的字符串就成了 JWT 的Header（头部）。 eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9 签名（签名） 将上面的两个编码后的字符串都用句号 .连接在一起（头部在前），就形成了 eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcm9tX3VzZXIiOiJCIiwidGFyZ2V0X3VzZXIiOiJBIn0 这一部分的过程在 node-jws 的源码中有体现 最后，我们将上面拼接完的字符串用 HS256 算法进行加密。在加密的时候，我们还需要提供一个密钥（secret）。如果我们用 mystar作为密钥的话，那么就可以得到我们加密后的内容 rSWamyAYwuHCo7IFAgd1oRpSP7nzL7BF5t7ItqpKViM 这一部分又叫做签名。 最后将这一部分签名也拼接在被签名的字符串后面，我们就得到了完整的 JWT eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcm9tX3VzZXIiOiJCIiwidGFyZ2V0X3VzZXIiOiJBIn0.rSWamyAYwuHCo7IFAgd1oRpSP7nzL7BF5t7ItqpKViM 于是，我们就可以将邮件中的 URL 改成 https://your.awesome-app.com/make-friend/?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcm9tX3VzZXIiOiJCIiwidGFyZ2V0X3VzZXIiOiJBIn0.rSWamyAYwuHCo7IFAgd1oRpSP7nzL7BF5t7ItqpKViM 这样就可以安全地完成添加好友的操作了！ 且慢，我们一定会有一些问题：  签名的目的是什么？ Base64 是一种编码，是可逆的，那么我的信息不就被暴露了吗？ 让我逐一为你说明。 签名的目的 最后一步签名的过程，实际上是对头部以及载荷内容进行签名。一般而言，加密算法对于不同的输入产生的输出总是不一样的。对于两个不同的输入，产生同样的输出的概率极其地小（有可能比我成世界首富的概率还小）。所以，我们就把“不一样的输入产生不一样的输出”当做必然事件来看待吧。 所以，如果有人对头部以及载荷的内容解码之后进行修改，再进行编码的话，那么新的头部和载荷的签名和之前的签名就将是不一样的。而且，如果不知道服务器加密的时候用的密钥的话，得出来的签名也一定会是不一样的。 服务器应用在接受到 JWT 后，会首先对头部和载荷的内容用同一算法再次签名。那么服务器应用是怎么知道我们用的是哪一种算法呢？别忘了，我们在 JWT 的头部中已经用 alg字段指明了我们的加密算法了。 如果服务器应用对头部和载荷再次以同样方法签名之后发现，自己计算出来的签名和接受到的签名不一样，那么就说明这个 Token 的内容被别人动过的，我们应该拒绝这个 Token，返回一个 HTTP 401 Unauthorized 响应。 信息会暴露 是的。 所以，在 JWT 中，不应该在载荷里面加入任何敏感的数据。在上面的例子中，我们传输的是用户的 User ID。这个值实际上不是什么敏感内容，一般情况下被知道也是安全的。 但是像密码这样的内容就不能被放在 JWT 中了。如果将用户的密码放在了 JWT 中，那么怀有恶意的第三方通过 Base64 解码就能很快地知道你的密码了。 JWT 的适用场景 我们可以看到，JWT 适合用于向 Web 应用传递一些非敏感信息。例如在上面提到的完成加好友的操作，还有诸如下订单的操作等等。 其实 JWT 还经常用于设计用户认证和授权系统，甚至实现 Web 应用的单点登录。在下一次的文章中，我将为大家系统地总结 JWT 在用户认证和授权上的应用。 说明：  本文转自公众号“程序猿 DD”，查看更多文章请扫码关注。 ",
      "url"      : "http://zhangjinmiao.github.io/jwt/2018/04/18/JSON-Web-Token-%E5%9C%A8Web%E5%BA%94%E7%94%A8%E9%97%B4%E5%AE%89%E5%85%A8%E5%9C%B0%E4%BC%A0%E9%80%92%E4%BF%A1%E6%81%AF.html",
      "keywords" : "jwt"
    } ,
  
    {
      "title"    : "八幅漫画理解使用 JSON Web Token 设计单点登录系统",
      "category" : "JWT",
      "content": "上次在《JSON Web Token - 在 Web 应用间安全地传递信息》中我提到了 JSON Web Token 可以用来设计单点登录系统。我尝试用八幅漫画先让大家理解如何设计正常的用户认证系统，然后再延伸到单点登录系统。 如果还没有阅读《JSON Web Token - 在 Web 应用间安全地传递信息》，我强烈建议你花十分钟阅读它，理解 JWT 的生成过程和原理。 用户认证八步走 所谓用户认证（Authentication），就是让用户登录，并且在接下来的一段时间内让用户访问网站时可以使用其账户，而不需要再次登录的机制。  小知识：可别把用户认证和用户授权（Authorization）搞混了。用户授权指的是规定并允许用户使用自己的权限，例如发布帖子、管理站点等。 首先，服务器应用（下面简称“应用”）让用户通过 Web 表单将自己的用户名和密码发送到服务器的接口。这一过程一般是一个 HTTP POST 请求。建议的方式是通过 SSL 加密的传输（https 协议），从而避免敏感信息被嗅探。 接下来，应用和数据库核对用户名和密码。 核对用户名和密码成功后，应用将用户的 id（图中的 user_id）作为 JWT Payload 的一个属性，将其与头部分别进行 Base64 编码拼接后签名，形成一个 JWT。这里的 JWT 就是一个形同 lll.zzz.xxx的字符串。 应用将 JWT 字符串作为该请求 Cookie 的一部分返回给用户。注意，在这里必须使用 HttpOnly属性来防止 Cookie 被 JavaScript 读取，从而避免跨站脚本攻击（XSS 攻击）。 在 Cookie 失效或者被删除前，用户每次访问应用，应用都会接受到含有 jwt的 Cookie。从而应用就可以将 JWT 从请求中提取出来。 应用通过一系列任务检查 JWT 的有效性。例如，检查签名是否正确；检查 Token 是否过期；检查 Token 的接收方是否是自己（可选）。 应用在确认 JWT 有效之后，JWT 进行 Base64 解码（可能在上一步中已经完成），然后在 Payload 中读取用户的 id 值，也就是 user_id属性。这里用户的 id为 1025。 应用从数据库取到 id为 1025 的用户的信息，加载到内存中，进行 ORM 之类的一系列底层逻辑初始化。 应用根据用户请求进行响应。 和 Session 方式存储 id 的差异 Session 方式存储用户 id 的最大弊病在于要占用大量服务器内存，对于较大型应用而言可能还要保存许多的状态。一般而言，大型应用还需要借助一些 KV 数据库和一系列缓存机制来实现 Session 的存储。 而 JWT 方式将用户状态分散到了客户端中，可以明显减轻服务端的内存压力。除了用户 id 之外，还可以存储其他的和用户相关的信息，例如该用户是否是管理员、用户所在的分桶（见[《你所应该知道的 A/B 测试基础》一文](http://blog.leapoahead.com/2015/08/27/introduction-to-ab-testing/）等。 虽说 JWT 方式让服务器有一些计算压力（例如加密、编码和解码），但是这些压力相比磁盘 I/O 而言或许是半斤八两。具体是否采用，需要在不同场景下用数据说话。 单点登录 Session 方式来存储用户 id，一开始用户的 Session 只会存储在一台服务器上。对于有多个子域名的站点，每个子域名至少会对应一台不同的服务器，例如：  www.taobao.com nv.taobao.com nz.taobao.com login.taobao.com 所以如果要实现在 login.taobao.com登录后，在其他的子域名下依然可以取到 Session，这要求我们在多台服务器上同步 Session。 使用 JWT 的方式则没有这个问题的存在，因为用户的状态已经被传送到了客户端。因此，我们只需要将含有 JWT 的 Cookie 的 domain设置为顶级域名即可，例如 Set-Cookie: jwt=lll.zzz.xxx; HttpOnly; max-age=980000; domain=.taobao.com 注意 domain必须设置为一个点加顶级域名，即 .taobao.com。这样，taobao.com 和*.taobao.com 就都可以接受到这个 Cookie，并获取 JWT 了。 说明：  作者：John Wu 原文：http://blog.leapoahead.com/2015/09/07/user-authentication-with-jwt/ ",
      "url"      : "http://zhangjinmiao.github.io/jwt/2018/04/18/%E5%85%AB%E5%B9%85%E6%BC%AB%E7%94%BB%E7%90%86%E8%A7%A3%E4%BD%BF%E7%94%A8JSON-Web-Token%E8%AE%BE%E8%AE%A1%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95%E7%B3%BB%E7%BB%9F.html",
      "keywords" : "jwt, 单点登录"
    } ,
  
    {
      "title"    : "设计模式之单例模式",
      "category" : "DesignMode",
      "content": "单例模式:  定义：确保一个类只有一个实例，而且自行实例化并向整个系统提供这个实例。   类型：创建类模式   类图：  类图知识点：  类图分为三部分，依次是类名、属性、方法 以«开头和以»结尾的为注释信息 修饰符+代表public，-代表private，#代表protected，什么都没有代表包可见。 带下划线的属性或方法代表是静态的。 单例模式应该是23种设计模式中最简单的一种模式了。它有以下几个要素：  私有的构造方法   指向自己实例的私有静态引用   以自己实例为返回值的静态的公有的方法 单例模式根据实例化对象时机的不同分为两种：一种是饿汉式单例，一种是懒汉式单例。饿汉式单例在单例类被 加载时候，就实例化一个对象交给自己的引用；而懒汉式在调用取得实例方法的时候才会实例化对象。 单例模式的优点：  在内存中只有一个对象，节省内存空间。   避免频繁的创建销毁对象，可以提高性能。   避免对共享资源的多重占用。   可以全局访问。 单例模式注意事项  只能使用单例类提供的方法得到单例对象，不要使用反射，否则将会实例化一个新对象。   不要做断开单例类对象与类中静态引用的危险操作。-   多线程使用单例使用共享资源时，注意线程安全问题。 适用场景  需要频繁实例化然后销毁的对象。   创建对象时耗时过多或者耗资源过多，但又经常用到的对象。   有状态的工具类对象。   频繁访问数据库或文件的对象。   以及其他我没用过的所有要求只有一个对象的场景。 代码实现 饿汉模式 public class Singleton {  private static Singleton instance = new Singleton();  private Singleton() {  }  public static Singleton getInstance(){  return instance;  } } 懒汉模式 public class Singleton {  tprivate static Singleton singleton;  tprivate Singleton(){}  tpublic static synchronized Singleton getInstance(){  t tif(singleton==null){  t t tsingleton = new Singleton();  t t}  t treturn singleton;  t} } 懒汉模式也不是安全的。 静态内部类 也是一种懒汉模式。 /** * @author jimzhang * &lt;&gt;静态内部类&lt;/&gt; * @version V1.0.0 * @date 2018-04-08 11:01 */ public class Singleton {  private static class SingletonHolder{  private static final Singleton INSTANCE = new Singleton();  }  private Singleton() {  }  public static final Singleton getInstance(){  return SingletonHolder.INSTANCE;  } } 双重校验锁 /** * @author jimzhang * &lt;&gt;双重校验锁&lt;/&gt; * @version V1.0.0 * @date 2018-04-08 11:23 */ public class Singleton {  private volatile static Singleton instance;  private Singleton(){}  public static Singleton getInstance(){  if (instance == null) {   synchronized (Singleton.class) {    if (instance == null) {    instance = new Singleton();    }   }  }  return instance;  } } 枚举 /** * @author: jimzhang * &lt;&gt;枚举&lt;/&gt; 推荐 * @date: 2018-04-08 11:11 * @version: V1.0.0 */ public enum Singleton {  INSTANCE;  public void whateverMethod() {  } } 使用 CAS 实现单例模式 /** * @author jimzhang * &lt;&gt;使用 CAS 实现单例&lt;/&gt; * 非阻塞： * 用CAS的好处在于不需要使用传统的锁机制来保证线程安全,CAS是一种基于忙等待的算法,依赖底层硬件的实现,相对于锁它没有线程切换和阻塞的额外消耗,可以支持较大的并行度。 * CAS 的一个重要缺点在于如果忙等待一直执行不成功(一直在死循环中),会对CPU造成较大的执行开销。 * @version V1.0.0 * @date 2018-04-08 11:29 */ public class Singleton {  private static final AtomicReference&lt;Singleton&gt; INSTANCE = new AtomicReference&lt;&gt;();  private Singleton(){}  public static Singleton getInstance(){  for (;;) {   Singleton singleton = INSTANCE.get();   if (null != singleton) {    return singleton;   }   singleton = new Singleton();   if (INSTANCE.compareAndSet(null,singleton)) {    return singleton;   }  }  } } AtomicReference：原子引用 赋值操作不是线程安全的。若想不用锁来实现，可以用 AtomicReference 这个类，实现对象引用的原子更新。 使用场景： 一个线程使用 student 对象，另一个线程负责定时读表，更新这个对象。那么就可以用AtomicReference 这个类。 ### ### 具体案例请见github。 ",
      "url"      : "http://zhangjinmiao.github.io/designmode/2018/04/20/singleton.html",
      "keywords" : "designMode, singleton"
    } ,
  
    {
      "title"    : "设计模式之策略模式",
      "category" : "DesignMode",
      "content": "策略模式:  定义：定义一组算法，将每个算法都封装起来，并且使他们之间可以互换。   类型：行为类模式   类图：  策略模式的结构  封装类：也叫上下文，对策略进行二次封装，目的是避免高层模块对策略的直接调用。 抽象策略：通常情况下为一个接口，当各个实现类中存在着重复的逻辑时，则使用抽象类来封装这部分公共 的代码，此时，策略模式看上去更像是模版方法模式。 具体策略：具体策略角色通常由一组封装了算法的类来担任，这些类之间可以根据需要自由替换。 策略模式的主要优点有：  策略类之间可以自由切换，由于策略类实现自同一个抽象，所以他们之间可以自由切换。 易于扩展，增加一个新的策略对策略模式来说非常容易，基本上可以在不改变原有代码的基础上进行扩展。 避免使用多重条件，如果不使用策略模式，对于所有的算法，必须使用条件语句进行连接，通过条件判断来 决定使用哪一种算法，在上一篇文章中我们已经提到，使用多重条件判断是非常不容易维护的。 策略模式的缺点主要有两个：  维护各个策略类会给开发带来额外开销，可能大家在这方面都有经验：一般来说，策略类的数量超过5个，就 比较令人头疼了。 必须对客户端（调用者）暴露所有的策略类，因为使用哪种策略是由客户端来决定的，因此，客户端应该知 道有什么策略，并且了解各种策略之间的区别，否则，后果很严重。 适用场景：  几个类的主要逻辑相同，只在部分逻辑的算法和行为上稍有区别的情况。 有几种相似的行为，或者说算法，客户端需要动态地决定使用哪一种，那么可以使用策略模式，将这些算法 封装起来供客户端调用。 策略模式是一种简单常用的模式，我们在进行开发的时候，会经常有意无意地使用它，一般来说，策略模式不会 单独使用，跟模版方法模式、工厂模式等混合使用的情况比较多。 代码实现 抽象策略： /** * @author jimzhang * &lt;&gt;抽象策略：通常情况下为一个接口，当各个实现类中存在着重复的逻辑时，则使用抽象类来封装这部分公共的代码，此时， * 策略模式看上去更像是模版方法模式。&lt;/&gt; * @version V1.0.0 * @date 2018-04-20 9:00 */ public interface IStrategy {  public void doSomething(); } 上下文： /** * @author jimzhang * &lt;&gt;封装类：也叫上下文，对策略进行二次封装，目的是避免高层模块对策略的直接调用。&lt;/&gt; * @version V1.0.0 * @date 2018-04-20 9:04 */ public class Context { private IStrategy strategy; public Context(IStrategy strategy) {  this.strategy = strategy;  } public void execute() {  strategy.doSomething();  } } 具体策略1： /** * @author jimzhang * &lt;&gt;具体实现&lt;/&gt; * @version V1.0.0 * @date 2018-04-20 9:01 */ public class ConcreteStrategy1 implements IStrategy {  @Override  public void doSomething() {  System.out.println(具体策略1);  } } 具体策略2：  客户端使用： public class Client { public static void main(String[] args) {  Context context;  context = new Context(new ConcreteStrategy1());  System.out.println(执行策略1);  context.execute();  context = new Context(new ConcreteStrategy2());  System.out.println(执行策略2);  context.execute(); } } 具体案例请见github。 ",
      "url"      : "http://zhangjinmiao.github.io/designmode/2018/04/20/strategy.html",
      "keywords" : "designMode, strategy"
    } ,
  
    {
      "title"    : "常用 Linux 命令",
      "category" : "Linux",
      "content": "进程和端口相关  查看端口占用 查看端口 8080 的使用情况： netstat -tln | grep 8080 查看端口属于哪个程序 (list open files) 是一个列出当前系统打开文件的工具 lsof -i :8080 用于显示tcp，udp的端口和进程等相关情况：netstat -tunlp 新开端口 su 切换到 root 下，输入密码：XXXX，直接执行以下命令： 0.查看状态：iptables -L -n 1.以6016 端口为例：/sbin/iptables -I INPUT -p tcp --dport 6016 -j ACCEPT 2.保存命令：/etc/rc.d/init.d/iptables save 3.查看状态：/etc/init.d/iptables status 查看进程  表示查看所有进程里 CMD 是 java 的进程信息： ps -ef | grep java 查看java进程，-aux 显示所有状态：ps -aux|grep java  查看所有进程：ps aux 用于查看指定端口号的进程情况：netstat -tunlp|grep 端口号 查看程序是否运行  查看 java 进程： ps -ef|grep java 查看 tomcat 进程： ps -ef|grep tomcat 终止进程  终止进程号位19979的进程：kill -9 19979 优雅的停止：kill 进程号 netstat 命令介绍 netstat 命令用于显示与 IP、TCP、UDP 和 ICMP 协议相关的统计数据，一般用于检验本机各端口的网络连接情况   -a 显示一个所有的有效连接信息列表(包括已建立的连接，也包括监听连接请求的那些连接)   -n 显示所有已建立的有效连接   -t tcp 协议   -u udp 协议   -l 查询正在监听的程序   -p 显示正在使用 socket 的程序识别码和程序名称   ​  通过端口号获取进程号 方式一： netstat -tunlp|grep 6019 netstat -tunlp 用于显示 tcp，udp 的端口和进程等相关情况 方式二： lsof -i:6019 lsof -i 用以显示符合条件的进程情况，lsof(list open files) 是一个列出当前系统打开文件的工具。  获取某个进程的网络端口号 查看进程号为 20446 的端口号，如图箭头所指“6016”即是；方框显示的是该程序所调用的端口，包括 mongodb 的 27017，mysql (显示了别名，其实是 3306)，rabbitmq 的 amqp(其实是 5672) netstat -ap | grep 20446  不显示别名： netstat -anp | grep 20446  显示当前正在监听的服务名称  netstat -tl 查看开启了哪些端口  netstat -lntp  查看系统监听的端口  netstat -na|grep -i listen   查看已建立的连接进程，所占用的端口  netstat -antup  查看后台运行的进程总数  ps -ef|wc -l  查看 java 进程  ps -aux | grep java 或 ps -ef | grep java (grep ——global search regular expression(RE) and print out the line 全文正则匹配并打印) ps：将某个进程显示出来 -A 　显示所有程序。 -e 　此参数的效果和指定A参数相同。 -f 　显示UID,PPIP,C与STIME栏位。  grep命令是查找 中间的|是管道命令 是指ps命令与grep同时执行 UID PID PPID C STIME TTY TIME CMD 各相关信息的意义： UID 程序被该 UID 所拥有 PID 就是这个程序的 ID PPID 则是其上级父程序的ID C CPU 使用的资源百分比 STIME 系统启动时间 TTY 登入者的终端机位置 TIME 使用掉的 CPU 时间。 CMD 所下达的指令为何 查看连接状态 各种连接  netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}' 解释: 返回结果示例： LAST_ACK 5 (正在等待处理的请求数) SYN_RECV 30 ESTABLISHED 1597 (正常数据传输状态) FIN_WAIT1 51 FIN_WAIT2 504 TIME_WAIT 1057 (处理完毕，等待超时结束的请求数) 状态：描述 CLOSED：无连接是活动的或正在进行 LISTEN：服务器在等待进入呼叫 SYN_RECV：一个连接请求已经到达，等待确认 SYN_SENT：应用已经开始，打开一个连接 ESTABLISHED：正常数据传输状态 FIN_WAIT1：应用说它已经完成 FIN_WAIT2：另一边已同意释放 ITMED_WAIT：等待所有分组死掉 CLOSING：两边同时尝试关闭 TIME_WAIT：另一边已初始化一个释放 LAST_ACK：等待所有分组死掉   并发连接数 netstat -nat|grep ESTABLISHED|wc -l   查看某个端口的连接数 netstat -nat | grep -iw 9090 | wc -l   查看连接状况 netstat -nat | grep -iw 9090   网络操作 查看网络通不通 ping www.baidu.com ubutu@10-10-80-30:~$ ping www.baidu.com PING www.a.shifen.com (180.97.33.108) 56(84) bytes of data. 64 bytes from 180.97.33.108: icmp_seq=1 ttl=48 time=27.6 ms 64 bytes from 180.97.33.108: icmp_seq=2 ttl=48 time=27.6 ms 64 bytes from 180.97.33.108: icmp_seq=3 ttl=48 time=27.5 ms 64 bytes from 180.97.33.108: icmp_seq=4 ttl=48 time=27.6 ms telnet 域名/IP 端口号 test_user@10-10-80-30:~$ ttelnet www.baidu.com 80 Trying 61.135.169.125… Connected to www.a.shifen.com. Escape character is ‘^]’. hosts 加入域名  1.使用root sudo vi /etc/hosts 按 “Inter”输入， 按 “Esc”,”:wq” 保存退出 搜索相关 grep 统计某个字符串的行数  grep -wi -c Eleme info-20180322.log_0 统计 emele 的行数，忽略大小写  使用 grep 搜索 查询关键字 eleme、412、413 并高亮  grep -wi --color 'eleme  |412  |413' info-20180322.log_0 xxxxxxxxxx    查询字符串出现次 grep -o '复制' info.log | wc -l  文件操作相关   文件复制  复制文件：cp source dest source ：原文件， dest : 要复制到的目录 递归复制整个文件夹：cp -r sourceFolder targetFolder 远程拷贝：scp sourecFile romoteUserName@remoteIp:remoteAddr 删除目录  删除空目录：rmdir deleteEmptyFolder 递归删除目录中所有内容：rm -rf deleteFile 删除除指定文件以外的文件及文件夹： rm -rf `ls |grep -v redis.conf ` 创建文件  vi filename 移动文件  mv /temp/movefile /targetFolder 重命名  mv oldNameFile newNameFile 修改文件权限  file.java的权限 -rwx-rwx-rwx，r 表示读、w 表示写、x 表示可执行：chmod 777 file.java 文件查看  查看文件头10 行：head -n 10 info.log 查看文件尾10 行：tail -n 10 info.log 查看文件尾10 行并监听文件：tail -10f info.log 文件查找  find /dir -name filename [在目录 dir 下查找文件名为 filename 的文件] find . -name “*.log” [在当前目录下查找扩展名为 log 的文件] 打包文件夹 zip 方式 zip -r ./eleme.zip ./eleme/ 压缩当前文件夹下的 eleme 文件夹 为 eleme.zip zip -r ./eleme.zip ./*/  t 压缩当前文件夹下的所有文件 为 eleme.zip unzip test.zip 将压缩文件 text.zip 在当前目录下解压缩 unzip -v test.zip 查看压缩文件目录，但不解压 unzip -n test.zip -d /tmp 将压缩文件 text.zip 在指定目录 /tmp 下解压缩，如果已有相同的文件存在，要求 unzip 命令不覆盖原先的文件 unzip -o test.zip -d /tmp 将压缩文件 test.zip 在指定目录 /tmp 下解压缩，如果已有相同的文件存在，要求 unzip 命令覆盖原先的文件 tar 方式： -z : gzip压缩 tar -czvf log.tar log.info : 压缩单个文件  tar -czvf log.tar log.info 2018-03-22-info.log 压缩多个文件 tar -czvf my.tar dir1 dir2 多个目录压缩打包 解包至当前目录：tar -zxvf my.tar tar -ztvf log.tar.gz 查阅 tar 包内有哪些文件 tar -zxvf test.tar test.txt 解压部分文件 &gt;注：c 参数代表 create（创建），x 参数代表 extract（解包），v 参数代表 verbose（详细信息），f 参数代表 filename（文件名），所以 f 后必须接文件名。   传包 scp Alipay.jar 用户名@127.0.0.1:/usr/work/alipay   拷贝 从其他目录拷贝文件到当前目录：  先进入当前目录，然后：cp 源文件目录 .  ( .代表当前目录) 日志查看 按行号查看，过滤出关键字附近的日志   ①先查看关键字所在的行号：  cat -n info-20180319.log_0 | grep '权限'   ②‘权限’所在行号是 102，查看这个关键字前 10 行和后 10 行的日志：  cat -n info-20180319.log_0 | tail -n +92|head -n 20  说明： tail -n + 92 表示查询 92 行之后的日志 head -n 20 则表示在前面的查询结果里再查前 20 条记录 如果日志比较多可以分页查看： cat -n info-20180319.log_0 | grep '权限' | more 或者写入文件： cat -n info-20180319.log_0 | grep '权限' &gt; xxx.log 2.按日期查看 sed '/2018-03-19 00:02:40/,/2018-03-19 01:02:40/p' info-20180319.log_0 linux 打开文件数 too many open files 解决方法 参考  查看每个用户最大允许打开文件数量 ulimit -a zhxdidi@web-mod md-consume]$ ulimit -a core file size  (blocks, -c) 0 data seg size   (kbytes, -d) unlimited scheduling priority   (-e) 0 file size   (blocks, -f) unlimited pending signals    (-i) 172032 max locked memory  (kbytes, -l) 32 max memory size  (kbytes, -m) unlimited open files    (-n) 1024 pipe size  (512 bytes, -p) 8 POSIX message queues  (bytes, -q) 819200 real-time priority   (-r) 0 stack size   (kbytes, -s) 10240 cpu time   (seconds, -t) unlimited max user processes   (-u) 172032 virtual memory  (kbytes, -v) unlimited file locks    (-x) unlimited   其中 open files (-n) 1024 表示每个用户最大允许打开的文件数量是 1024   查看当前系统打开的文件数量 (list open files)是一个列出当前系统打开文件的工具 lsof | wc -l 或 watch lsof | wc -l 每 2 秒监控   查看某一进程的打开文件数量 lsof -p pid | wc -l pid 为进程号   设置 open files 数值方法 方式一:临时 ulimit -n 2048   方式二：永久 vim /etc/security/limits.conf 在最后加入 * soft nofile 4096 * hard nofile 4096 最前的 * 表示所有用户，可根据需要设置某一用户 重启系统，通过shutdown -r now 命令重启即可。   系统管理相关  查看空间大小  磁盘空间：df -h 目录大小：du -sh * chmod 命令 该命令用于改变文件的权限，一般的用法如下： chmod [-R] xyz 文件或目录 -R：进行递归的持续更改，即连同子目录下的所有文件都会更改 同时，chmod 还可以使用 u（user）、g（group）、o（other）、a（all）和+（加入）、-（删除）、=（设置）跟 rwx 搭配来对文件的权限进行更改。 # 例如： chmod 0755 file # 把file的文件权限改变为-rxwr-xr-x chmod g+w file # 向file的文件权限中加入用户组可写权限 权限范围的表示法如下： u User，即文件或目录的拥有者； g Group，即文件或目录的所属群组； o Other，除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围； a All，即全部的用户，包含拥有者，所属群组以及其他用户； r 读取权限，数字代号为“4”; w 写入权限，数字代号为“2”； x 执行或切换权限，数字代号为“1”； - 不具任何权限，数字代号为“0”； s 特殊功能说明：变更文件或目录的权限。 777 代表： rwxrwxrwx ，意思是该登录用户（可以用命令 id 查看）、他所在的组和其他人都有最高权限。 r=4，w=2，x=1  若要 rwx 属性则 4+2+1=7； 若要 rw- 属性则 4+2=6； 若要 r-x 属性则 4+1=5。  -rw------- (600) -- 只有属主有读写权限。   -rw-r--r-- (644) -- 只有属主有读写权限；而属组用户和其他用户只有读权限。   -rwx------ (700) -- 只有属主有读、写、执行权限。   -rwxr-xr-x (755) -- 属主有读、写、执行权限；而属组用户和其他用户只有读、执行权限。   -rwx--x--x (711) -- 属主有读、写、执行权限；而属组用户和其他用户只有执行权限。   -rw-rw-rw- (666) -- 所有用户都有文件读、写权限。这种做法不可取。   -rwxrwxrwx (777) -- 所有用户都有读、写、执行权限。更不可取的做法。   查看 Linux 系统位数 方法一： file /sbin/init 或者 file /bin/ls  方法二： uname -a  方法三： getconf LONG_BIT  查看内核数 方法一：  cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c  方法二： cat /proc/cpuinfo |grep processor|wc -l t   查看磁盘大小 df -h    df -hl 查看磁盘剩余空间 df -h 查看每个根路径的分区大小 du -sh [目录名] 返回该目录的大小 du -sm [文件夹] 返回该文件夹总 M 数   更多功能可以输入一下命令查看：   df –help du –help   查看内存大小  free -m 按M计算  查看 CPU 使用率 实时 CPU 使用率： top t  CPU 处理器使用率: cat /proc/stat cpu  平均 CPU 使用率: cat /proc/loadavg   查看 CPU 基本信息 cat /proc/cpuinfo   命令 Crontab linux 自带的定时任务  查看是否安装了 crontab  rpm -qa | grep crontab   服务启动、关闭  启动：/etc/init.d/crond start 关闭：/etc/init.d/crond stop 重启：/etc/init.d/crond restart 重新载入配置：/etc/init.d/crond reload   配置文件 在 etc/目录下存在 cron.hourly,cron.daily,cron.weekly,cron.monthly,cron.d 五个目录和 crontab,cron.deny 二 个文件  查看 cron  crontab -l   编辑  crontab -e   新增用户 1.新增用户 username sudo useradd username 2.给用户 username 设置密码： sudo passwd username  useradd 命令添加用户，则默认用户名和组都是相同的名称； /etc/passwd 文件纪录的系统所有用户列表； /etc/group 文件纪录系统的组信息。 3.将用户添加到 wheel 组，否则无法使用 sudo 命令 sudo usermod -G wheel username 4.查看用户所属组 groups 禁止 root 远程登录 1.修改 ssh 配置文件：sudo vi /etc/sshd_config，将 PermitRootLogin 设置为 no 2.重启 ssh 服务：sudo systemctl restart sshd ",
      "url"      : "http://zhangjinmiao.github.io/linux/2018/04/22/%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4.html",
      "keywords" : "Linux, 命令"
    } ,
  
    {
      "title"    : "dubbo 问题集锦",
      "category" : "Dubbo",
      "content": "1 面试题：Dubbo 中 zookeeper 做注册中心，如果注册中心集群都挂掉，发布者和订阅者之间还能通信么？  可以的，启动 dubbo 时，消费者会从 zk 拉取注册的生产者的地址接口等数据，缓存在本地。每次调用时，按照本地存储的地址进行调用 注册中心对等集群，任意一台宕掉后，会自动切换到另一台 注册中心全部宕掉，服务提供者和消费者仍可以通过本地缓存通讯 服务提供者无状态，任一台 宕机后，不影响使用 服务提供者全部宕机，服务消费者会无法使用，并无限次重连等待服务者恢复 2 dubbo 连接注册中心和直连的区别  在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连， 点对点直联方式，将以服务接口为单位，忽略注册中心的提供者列表， 服务注册中心，动态的注册和发现服务，使服务的位置透明，并通过在消费方获取服务提供方地址列表，实现软负载均衡和 Failover， 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，服务消费者向注册中心获取服务提供者地址列表，并根据负载算法直接调用提供者，注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外，注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者 注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表 注册中心和监控中心都是可选的，服务消费者可以直连服务提供者 3、Dubbo 在安全机制方面是如何解决的  Dubbo 通过 Token 令牌防止用户绕过注册中心直连，然后在注册中心上管理授权。Dubbo 还提供服务黑白名单，来控制服务所允许的调用方。 ",
      "url"      : "http://zhangjinmiao.github.io/dubbo/2018/04/24/dubbo.html",
      "keywords" : "dubbo, 分布式"
    } ,
  
    {
      "title"    : "21 分钟 MySQL 教程",
      "category" : "MySQL",
      "content": "为什么只需要 21 分钟呢？因为在我们大天朝有句话叫做三七二十一，你可以不管三七二十一开始使用 MySQL 及快速的方式入门 MySQL。其实 21 分钟把下面语句之行一遍是没有问题的，要理解的话估计不止 21 分钟，对于初学者来说只需满足自己需求可以增删改查等简易的维护即可。 开始使用 我下面所有的 SQL 语句是基于 MySQL 5.6+运行。 MySQL 为关系型数据库(Relational Database Management System)，一个关系型数据库由一个或数个表格组成, 如图所示的一个表格：  表头(header): 每一列的名称; 列(col): 具有相同数据类型的数据的集合; 行(row): 每一行用来描述某个人/物的具体信息; 值(value): 行的具体信息, 每个值必须与该列的数据类型相同; 键(key): 表中用来识别某个特定的人  物的方法, 键的值在当前列中具有唯一性。 登录 MySQL mysql -h 127.0.0.1 -u 用户名 -p mysql -D 所选择的数据库名 -h 主机名 -u 用户名 -p mysql&gt; exit # 退出 使用 “quit;” 或 “  q;” 一样的效果 mysql&gt; status; # 显示当前mysql的version的各种信息 mysql&gt; select version(); # 显示当前mysql的version信息 mysql&gt; show global variables like 'port'; # 查看MySQL端口号 创建数据库 对于表的操作需要先进入库use 库名; -- 创建一个名为 samp_db 的数据库，数据库字符编码指定为 gbk create database samp_db character set gbk; drop database samp_db; -- 删除 库名为samp_db的库 show databases;  -- 显示数据库列表。 use samp_db;  -- 选择创建的数据库samp_db show tables;  -- 显示samp_db下面所有的表名字 describe 表名; -- 显示数据表的结构 delete from 表名; -- 清空表中记录 创建数据库表  使用 create table 语句可完成对表的创建, create table 的常见形式: 语法：create table 表名称(列声明); -- 如果数据库中存在user_accounts表，就把它从数据库中drop掉 DROP TABLE IF EXISTS `user_accounts`; CREATE TABLE `user_accounts` ( `id`   int(100) unsigned NOT NULL AUTO_INCREMENT primary key, `password`  varchar(32)  NOT NULL DEFAULT '' COMMENT '用户密码', `reset_password` tinyint(32)  NOT NULL DEFAULT 0 COMMENT '用户类型：0－不需要重置密码；1-需要重置密码', `mobile`  varchar(20)  NOT NULL DEFAULT '' COMMENT '手机', `create_at` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6), `update_at` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6), -- 创建唯一索引，不允许重复 UNIQUE INDEX idx_user_mobile(`mobile`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='用户表信息'; 数据类型的属性解释  NULL：数据列可包含 NULL 值； NOT NULL：数据列不允许包含 NULL 值； DEFAULT：默认值； PRIMARY：KEY 主键； AUTO_INCREMENT：自动递增，适用于整数类型； UNSIGNED：是指数值类型只能为正数； CHARACTER SET name：指定一个字符集； COMMENT：对表或者字段说明； 增删改查 SELECT  SELECT 语句用于从表中选取数据。 语法：SELECT 列名称 FROM 表名称 语法：SELECT * FROM 表名称 -- 表station取个别名叫s，表station中不包含 字段id=13或者14 的，并且id不等于4的 查询出来，只显示id SELECT s.id from station s WHERE id in (13,14) and id not in (4); -- 从表 Persons 选取 LastName 列的数据 SELECT LastName FROM Persons -- 从表 users 选取 id=3 的数据，并只拉一条数据(据说能优化性能) SELECT * FROM users where id=3 limit 1 -- 结果集中会自动去重复数据 SELECT DISTINCT Company FROM Orders -- 表 Persons 字段 Id_P 等于 Orders 字段 Id_P 的值， -- 结果集显示 Persons表的 LastName、FirstName字段，Orders表的OrderNo字段 SELECT p.LastName, p.FirstName, o.OrderNo FROM Persons p, Orders o WHERE p.Id_P = o.Id_P -- gbk 和 utf8 中英文混合排序最简单的办法 -- ci是 case insensitive, 即 “大小写不敏感” SELECT tag, COUNT(tag) from news GROUP BY tag order by convert(tag using gbk) collate gbk_chinese_ci; SELECT tag, COUNT(tag) from news GROUP BY tag order by convert(tag using utf8) collate utf8_unicode_ci; UPDATE  Update 语句用于修改表中的数据。 语法：UPDATE 表名称 SET 列名称 = 新值 WHERE 列名称 = 某值 -- update语句设置字段值为另一个结果取出来的字段 update user set name = (select name from user1 where user1 .id = 1 ) where id = (select id from user2 where user2 .name='小苏'); -- 更新表 orders 中 id=1 的那一行数据更新它的 title 字段 UPDATE `orders` set title='这里是标题' WHERE id=1; INSERT  INSERT INTO 语句用于向表格中插入新的行。 语法：INSERT INTO 表名称 VALUES (值1, 值2,....) 语法：INSERT INTO 表名称 (列1, 列2,...) VALUES (值1, 值2,....) -- 向表 Persons 插入一条字段 LastName = JSLite 字段 Address = shanghai INSERT INTO Persons (LastName, Address) VALUES ('JSLite', 'shanghai'); -- 向表 meeting 插入 字段 a=1 和字段 b=2 INSERT INTO meeting SET a=1,b=2; -- -- SQL实现将一个表的数据插入到另外一个表的代码 -- 如果只希望导入指定字段，可以用这种方法： -- INSERT INTO 目标表 (字段1, 字段2, ...) SELECT 字段1, 字段2, ... FROM 来源表; INSERT INTO orders (user_account_id, title) SELECT m.user_id, m.title FROM meeting m where m.id=1; -- 向表 charger 插入一条数据，已存在就对表 charger 更新 `type`,`update_at` 字段； INSERT INTO `charger` (`id`,`type`,`create_at`,`update_at`) VALUES (3,2,'2017-05-18 11:06:17','2017-05-18 11:06:17') ON DUPLICATE KEY UPDATE `id`=VALUES(`id`), `type`=VALUES(`type`), `update_at`=VALUES(`update_at`); DELETE  DELETE 语句用于删除表中的行。 语法：DELETE FROM 表名称 WHERE 列名称 = 值 -- 在不删除table_name表的情况下删除所有的行，清空表。 DELETE FROM table_name -- 或者 DELETE * FROM table_name -- 删除 Person表字段 LastName = 'JSLite' DELETE FROM Person WHERE LastName = 'JSLite' -- 删除 表meeting id 为2和3的两条数据 DELETE from meeting where id in (2,3); WHERE  WHERE 子句用于规定选择的标准。 语法：SELECT 列名称 FROM 表名称 WHERE 列 运算符 值 -- 从表 Persons 中选出 Year 字段大于 1965 的数据 SELECT * FROM Persons WHERE Year&gt;1965 AND 和 OR  AND - 如果第一个条件和第二个条件都成立； OR - 如果第一个条件和第二个条件中只要有一个成立； AND -- 删除 meeting 表字段 -- id=2 并且 user_id=5 的数据 和 -- id=3 并且 user_id=6 的数据 DELETE from meeting where id in (2,3) and user_id in (5,6); -- 使用 AND 来显示所有姓为 Carter 并且名为 Thomas 的人： SELECT * FROM Persons WHERE FirstName='Thomas' AND LastName='Carter'; OR -- 使用 OR 来显示所有姓为 Carter 或者名为 Thomas 的人： SELECT * FROM Persons WHERE firstname='Thomas' OR lastname='Carter' ORDER BY  语句默认按照升序对记录进行排序。 ORDER BY - 语句用于根据指定的列对结果集进行排序。 DESC - 按照降序对记录进行排序。 ASC - 按照顺序对记录进行排序。 -- Company在表Orders中为字母，则会以字母顺序显示公司名称 SELECT Company, OrderNumber FROM Orders ORDER BY Company -- 后面跟上 DESC 则为降序显示 SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC -- Company以降序显示公司名称，并OrderNumber以顺序显示 SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC, OrderNumber ASC IN  IN - 操作符允许我们在 WHERE 子句中规定多个值。 IN - 操作符用来指定范围，范围中的每一条，都进行匹配。IN 取值规律，由逗号分割，全部放置括号中。 语法：SELECT 字段名FROM 表格名WHERE 字段名 IN ('值一', '值二', ...); -- 从表 Persons 选取 字段 LastName 等于 Adams、Carter SELECT * FROM Persons WHERE LastName IN ('Adams','Carter') NOT  NOT - 操作符总是与其他操作符一起使用，用在要过滤的前面。 SELECT vend_id, prod_name FROM Products WHERE NOT vend_id = 'DLL01' ORDER BY prod_name; UNION  UNION - 操作符用于合并两个或多个 SELECT 语句的结果集。 -- 列出所有在中国表（Employees_China）和美国（Employees_USA）的不同的雇员名 SELECT E_Name FROM Employees_China UNION SELECT E_Name FROM Employees_USA -- 列出 meeting 表中的 pic_url， -- station 表中的 number_station 别名设置成 pic_url 避免字段不一样报错 -- 按更新时间排序 SELECT id,pic_url FROM meeting UNION ALL SELECT id,number_station AS pic_url FROM station ORDER BY update_at; -- 通过 UNION 语法同时查询了 products 表 和 comments 表的总记录数，并且按照 count 排序 SELECT 'product' AS type, count(*) as count FROM `products` union select 'comment' as type, count(*) as count FROM `comments` order by count; AS  as - 可理解为：用作、当成，作为；别名 一般是重命名列名或者表名。 语法：select column_1 as 列1,column_2 as 列2 from table as 表 SELECT * FROM Employee AS emp -- 这句意思是查找所有Employee 表里面的数据，并把Employee表格命名为 emp。 -- 当你命名一个表之后，你可以在下面用 emp 代替 Employee. -- 例如 SELECT * FROM emp. SELECT MAX(OrderPrice) AS LargestOrderPrice FROM Orders -- 列出表 Orders 字段 OrderPrice 列最大值， -- 结果集列不显示 OrderPrice 显示 LargestOrderPrice -- 显示表 users_profile 中的 name 列 SELECT t.name from (SELECT * from users_profile a) AS t; -- 表 user_accounts 命名别名 ua，表 users_profile 命名别名 up -- 满足条件 表 user_accounts 字段 id 等于 表 users_profile 字段 user_id -- 结果集只显示mobile、name两列 SELECT ua.mobile,up.name FROM user_accounts as ua INNER JOIN users_profile as up ON ua.id = up.user_id; JOIN  用于根据两个或多个表中的列之间的关系，从这些表中查询数据。 JOIN: 如果表中有至少一个匹配，则返回行 INNER JOIN:在表中存在至少一个匹配时，INNER JOIN 关键字返回行。 LEFT JOIN: 即使右表中没有匹配，也从左表返回所有的行 RIGHT JOIN: 即使左表中没有匹配，也从右表返回所有的行 FULL JOIN: 只要其中一个表中存在匹配，就返回行 SELECT Persons.LastName, Persons.FirstName, Orders.OrderNo FROM Persons INNER JOIN Orders ON Persons.Id_P = Orders.Id_P ORDER BY Persons.LastName; SQL 函数 COUNT  COUNT 让我们能够数出在表格中有多少笔资料被选出来。 语法：SELECT COUNT(字段名) FROM 表格名; -- 表 Store_Information 有几笔 store_name 栏不是空白的资料。 -- IS NOT NULL 是 这个栏位不是空白 的意思。 SELECT COUNT (Store_Name) FROM Store_Information WHERE Store_Name IS NOT NULL; -- 获取 Persons 表的总数 SELECT COUNT(1) AS totals FROM Persons; -- 获取表 station 字段 user_id 相同的总数 select user_id, count(*) as totals from station group by user_id; MAX  MAX 函数返回一列中的最大值。NULL 值不包括在计算中。 语法：SELECT MAX(字段名) FROM 表格名 -- 列出表 Orders 字段 OrderPrice 列最大值， -- 结果集列不显示 OrderPrice 显示 LargestOrderPrice SELECT MAX(OrderPrice) AS LargestOrderPrice FROM Orders 触发器  语法： create trigger { before | after}   # 之前或者之后出发 insert | update | delete # 指明了激活触发程序的语句的类型 on    # 操作哪张表 for each row   # 触发器的执行间隔，for each row 通知触发器每隔一行执行一次动作，而不是对整个表执行一次。  delimiter $ CREATE TRIGGER set_userdate BEFORE INSERT on `message` for EACH ROW BEGIN set @statu = new.status; -- 声明复制变量 statu if @statu = 0 then  -- 判断 statu 是否等于 0  UPDATE `user_accounts` SET status=1 WHERE openid=NEW.openid; end if; END $ DELIMITER ; -- 恢复结束符号 OLD 和 NEW 不区分大小写 NEW 用 NEW.col_name，没有旧行。在 DELETE 触发程序中，仅能使用 OLD.col_name，没有新行。 OLD 用 OLD.col_name 来引用更新前的某一行的列 添加索引 普通索引(INDEX)  语法：ALTER TABLE 表名字 ADD INDEX 索引名字 ( 字段名字 ) -- –直接创建索引 CREATE INDEX index_user ON user(title) -- –修改表结构的方式添加索引 ALTER TABLE table_name ADD INDEX index_name ON (column(length)) -- 给 user 表中的 name字段 添加普通索引(INDEX) ALTER TABLE `table` ADD INDEX index_name (name) -- –创建表的时候同时创建索引 CREATE TABLE `table` (  `id` int(11) NOT NULL AUTO_INCREMENT ,  `title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL ,  `content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL ,  `time` int(10) NULL DEFAULT NULL ,  PRIMARY KEY (`id`),  INDEX index_name (title(length)) ) -- –删除索引 DROP INDEX index_name ON table 主键索引(PRIMARY key)  语法：ALTER TABLE 表名字 ADD PRIMARY KEY ( 字段名字 ) -- 给 user 表中的 id字段 添加主键索引(PRIMARY key) ALTER TABLE `user` ADD PRIMARY key (id); 唯一索引(UNIQUE)  语法：ALTER TABLE 表名字 ADD UNIQUE (字段名字) -- 给 user 表中的 creattime 字段添加唯一索引(UNIQUE) ALTER TABLE `user` ADD UNIQUE (creattime); 全文索引(FULLTEXT)  语法：ALTER TABLE 表名字 ADD FULLTEXT (字段名字) -- 给 user 表中的 description 字段添加全文索引(FULLTEXT) ALTER TABLE `user` ADD FULLTEXT (description); 添加多列索引  语法： ALTER TABLE table_name ADD INDEX index_name ( column1, column2, column3) -- 给 user 表中的 name、city、age 字段添加名字为name_city_age的普通索引(INDEX) ALTER TABLE user ADD INDEX name_city_age (name(10),city,age); 建立索引的时机 在WHERE和JOIN中出现的列需要建立索引，但也不完全如此：  MySQL 只对&lt;，&lt;=，=，&gt;，&gt;=，BETWEEN，IN使用索引 某些时候的LIKE也会使用索引。 在LIKE以通配符%和_开头作查询时，MySQL 不会使用索引。 -- 此时就需要对city和age建立索引， -- 由于mytable表的userame也出现在了JOIN子句中，也有对它建立索引的必要。 SELECT t.Name FROM mytable t LEFT JOIN mytable m ON t.Name=m.username WHERE m.age=20 AND m.city='上海'; SELECT * FROM mytable WHERE username like'admin%'; -- 而下句就不会使用： SELECT * FROM mytable WHEREt Name like'%admin'; -- 因此，在使用LIKE时应注意以上的区别。 索引的注意事项  索引不会包含有 NULL 值的列 使用短索引 不要在列上进行运算 索引会失效 创建后表的修改 添加列  语法：alter table 表名 add 列名 列数据类型 [after 插入位置]; 示例: -- 在表students的最后追加列 address: alter table students add address char(60); -- 在名为 age 的列后插入列 birthday: alter table students add birthday date after age; -- 在名为 number_people 的列后插入列 weeks: alter table students add column `weeks` varchar(5) not null default after `number_people`; 修改列  语法：alter table 表名 change 列名称 列新名称 新数据类型; -- 将表 tel 列改名为 telphone: alter table students change tel telphone char(13) default -; -- 将 name 列的数据类型改为 char(16): alter table students change name name char(16) not null; -- 修改 COMMENT 前面必须得有类型属性 alter table students change name name char(16) COMMENT '这里是名字'; -- 修改列属性的时候 建议使用modify,不需要重建表 -- change用于修改列名字，这个需要重建表 alter table meeting modify `weeks` varchar(20) NOT NULL DEFAULT COMMENT 开放日期 周一到周日：0~6，间隔用英文逗号隔开; -- `user`表的`id`列，修改成字符串类型长度50，不能为空，`FIRST`放在第一列的位置 alter table `user` modify COLUMN `id` varchar(50) NOT NULL FIRST ; 删除列  语法：alter table 表名 drop 列名称; -- 删除表students中的 birthday 列: alter table students drop birthday; 重命名表  语法：alter table 表名 rename 新表名; -- 重命名 students 表为 workmates: alter table students rename workmates; 清空表数据  方法一：delete from 表名; 方法二：truncate from 表名; DELETE:1. DML 语言;2. 可以回退;3. 可以有条件的删除; TRUNCATE:1. DDL 语言;2. 无法回退;3. 默认所有的表内容都删除;4. 删除速度比 delete 快。 -- 清空表为 workmates 里面的数据，不删除表。 delete from workmates; -- 删除workmates表中的所有数据，且无法恢复 truncate from workmates; 删除整张表  语法：drop table 表名; -- 删除 workmates 表: drop table workmates; 删除整个数据库  语法：drop database 数据库名; -- 删除 samp_db 数据库: drop database samp_db; 其它实例 SQL 删除重复记录 转载 -- 查找表中多余的重复记录，重复记录是根据单个字段（peopleId）来判断 select * from people where peopleId in (select peopleId from people group by peopleId having count(peopleId) &gt; 1) -- 删除表中多余的重复记录，重复记录是根据单个字段（peopleId）来判断，只留有rowid最小的记录 delete from people where peopleId in (select peopleId from people group by peopleId having count(peopleId) &gt; 1) and rowid not in (select min(rowid) from people group by peopleId having count(peopleId )&gt;1) -- 查找表中多余的重复记录（多个字段） select * from vitae a where (a.peopleId,a.seq) in (select peopleId,seq from vitae group by peopleId,seq having count(*) &gt; 1) -- 删除表中多余的重复记录（多个字段），只留有rowid最小的记录 delete from vitae a where (a.peopleId,a.seq) in (select peopleId,seq from vitae group by peopleId,seq having count(*) &gt; 1) and rowid not in (select min(rowid) from vitae group by peopleId,seq having count(*)&gt;1) -- 查找表中多余的重复记录（多个字段），不包含rowid最小的记录 select * from vitae a where (a.peopleId,a.seq) in (select peopleId,seq from vitae group by peopleId,seq having count(*) &gt; 1) and rowid not in (select min(rowid) from vitae group by peopleId,seq having count(*)&gt;1) 参考手册  http://www.w3school.com.cn/sql/index.asp http://www.1keydata.com/cn/sql/sql-count.php ",
      "url"      : "http://zhangjinmiao.github.io/mysql/2018/04/24/21%E5%88%86%E9%92%9FMysql%E6%95%99%E7%A8%8B.html",
      "keywords" : "mysql, 21 分钟"
    } ,
  
    {
      "title"    : "58 到家 MySQL 军规升级版",
      "category" : "MySQL",
      "content": "一、基础规范  表存储引擎必须使用 InnoDB    表字符集默认使用 utf8，必要时候使用 utf8mb4 解读： （1）通用，无乱码风险，汉字 3 字节，英文 1 字节 （2）utf8mb4 是 utf8 的超集，有存储 4 字节例如表情符号时，使用它    禁止使用存储过程，视图，触发器，Event 解读： （1）对数据库性能影响较大，互联网业务，能让站点层和服务层干的事情，不要交到数据库层 （2）调试，排错，迁移都比较困难，扩展性较差    禁止在数据库中存储大文件，例如照片，可以将大文件存储在对象存储系统，数据库中存储路径 禁止在线上环境做数据库压力测试 测试，开发，线上数据库环境必须隔离   二、命名规范  库名，表名，列名必须用小写，采用下划线分隔 解读：abc，Abc，ABC 都是给自己埋坑    库名，表名，列名必须见名知义，长度不要超过 32 字符 解读：tmp，wushan 谁 TM 知道这些库是干嘛的    库备份必须以 bak 为前缀，以日期为后缀 从库必须以-s 为后缀 备库必须以-ss 为后缀   三、表设计规范  单实例表个数必须控制在 2000 个以内 单表分表个数必须控制在 1024 个以内 表必须有主键，推荐使用 UNSIGNED 整数为主键 潜在坑：删除无主键的表，如果是 row 模式的主从架构，从库会挂住    禁止使用外键，如果要保证完整性，应由应用程式实现 解读：外键使得表之间相互耦合，影响 update/delete 等 SQL 性能，有可能造成死锁，高并发情况下容易成为数据库瓶颈    建议将大字段，访问频度低的字段拆分到单独的表中存储，分离冷热数据 解读：具体参加《如何实施数据库垂直拆分》   四、列设计规范  根据业务区分使用 tinyint/int/bigint，分别会占用 1/4/8 字节 根据业务区分使用 char/varchar 解读： （1）字段长度固定，或者长度近似的业务场景，适合使用 char，能够减少碎片，查询性能高 （2）字段长度相差较大，或者更新较少的业务场景，适合使用 varchar，能够减少空间    根据业务区分使用 datetime/timestamp 解读：前者占用 5 个字节，后者占用 4 个字节，存储年使用 YEAR，存储日期使用 DATE，存储时间使用 datetime    必须把字段定义为 NOT NULL 并设默认值 解读： （1）NULL 的列使用索引，索引统计，值都更加复杂，MySQL 更难优化 （2）NULL 需要更多的存储空间 （3）NULL 只能采用 IS NULL 或者 IS NOT NULL，而在=/!=/in/not in 时有大坑    使用 INT UNSIGNED 存储 IPv4，不要用 char(15)   使用 varchar(20)存储手机号，不要使用整数 解读： （1）牵扯到国家代号，可能出现+/-/()等字符，例如+86 （2）手机号不会用来做数学运算 （3）varchar 可以模糊查询，例如 like ‘138%’    使用 TINYINT 来代替 ENUM 解读：ENUM 增加新值要进行 DDL 操作   五、索引规范  唯一索引使用 uniq_[字段名]来命名 非唯一索引使用 idx_[字段名]来命名 单张表索引数量建议控制在 5 个以内 解读： （1）互联网高并发业务，太多索引会影响写性能 （2）生成执行计划时，如果索引太多，会降低性能，并可能导致 MySQL 选择不到最优索引 （3）异常复杂的查询需求，可以选择 ES 等更为适合的方式存储    组合索引字段数不建议超过 5 个 解读：如果 5 个字段还不能极大缩小 row 范围，八成是设计有问题    不建议在频繁更新的字段上建立索引 非必要不要进行 JOIN 查询，如果要进行 JOIN 查询，被 JOIN 的字段必须类型相同，并建立索引 解读：踩过因为 JOIN 字段类型不一致，而导致全表扫描的坑么？    理解组合索引最左前缀原则，避免重复建设索引，如果建立了(a,b,c)，相当于建立了(a), (a,b), (a,b,c)   六、SQL 规范  禁止使用 select *，只获取必要字段 解读： （1）select *会增加 cpu/io/内存/带宽的消耗 （2）指定字段能有效利用索引覆盖 （3）指定字段查询，在表结构变更时，能保证对应用程序无影响    insert 必须指定字段，禁止使用 insert into T values() 解读：指定字段插入，在表结构变更时，能保证对应用程序无影响    隐式类型转换会使索引失效，导致全表扫描    禁止在 where 条件列使用函数或者表达式 解读：导致不能命中索引，全表扫描    禁止负向查询以及%开头的模糊查询 解读：导致不能命中索引，全表扫描    禁止大表 JOIN 和子查询 同一个字段上的 OR 必须改写问 IN，IN 的值必须少于 50 个 应用程序必须捕获 SQL 异常 解读：方便定位线上问题  说明：本文转自公众号“架构师之路”，扫描下面二维码关注有更多精彩内容。 ",
      "url"      : "http://zhangjinmiao.github.io/mysql/2018/04/24/58%E5%88%B0%E5%AE%B6MySQL%E5%86%9B%E8%A7%84%E5%8D%87%E7%BA%A7%E7%89%88.html",
      "keywords" : "58, mysql"
    } ,
  
    {
      "title"    : "面试被问到的问题",
      "category" : "Interview",
      "content": "基本概念  java 接口的修饰符：   接口的方法默认是 public abstract  接口的属性默认是 public static final 常量，且必须赋初值   分析：    接口用于描述系统对外提供的所有服务,因此接口中的成员常量和方法都必须是公开(public)类型的,确保外部使用者能访问它们；     接口仅仅描述系统能做什么,但不指明如何去做,所以接口中的方法都是抽象(abstract)方法     接口不涉及和任何具体实例相关的细节,因此接口没有构造方法,不能被实例化,没有实例变量，只有静态（static）变量     接口的中的变量是所有实现类共有的，既然共有，肯定是不变的东西，因为变化的东西也不能够算共有。所以变量是不可变(final)类型，也就是常量了  ​     System.out.println(5 + 2)   输出：52 任何和字符串进行+运算的结果都相当于字符串的连接。 ​   IO 流 Java的IO操作中有面向 字节(Byte) 和面向 字符(Character) 两种方式。   面向字节的操作为以8位为单位对二进制的数据进行操作，对数据不进行转换，这些类都是InputStream 和OutputStream 的子类。    面向字符的操作为以字符为单位对数据进行操作，在读的时候将二进制数据转为字符，在写的时候将字符转为二进制数据，这些类都是 Reader 和 Writer 的子类。 ​ 总结：以InputStream（输入）/OutputStream（输出）为后缀的是字节流； 以Reader（输入）/Writer（输出）为后缀的是字符流。  ​   ArrayList 和 LinkedList 区别   ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。  对于随机访问 get 和 set，ArrayList 优于LinkedList，因为LinkedList 要移动指针。  对于新增和删除操作 add 和 remove，LinedList 比较占优势，因为 ArrayList 要移动数据。   ​   volatile 和 synchronized 的区别   volatile 轻量级，只能修饰变量。synchronized 重量级，还可修饰方法   volatile 只能保证数据的可见性，不能用来同步，因为多个线程并发访问volatile修饰的变量不会阻塞。  仅仅使用 volatile 并不能保证线程安全性。而 synchronized 则可实现线程的安全性。 ​   异常 Java标准库内建了一些通用的异常，这些类以Throwable 为顶层父类。 Throwable 又派生出 Error 类和 Exception 类。   错误：Error 类以及他的子类的实例，代表了JVM本身的错误。错误不能被程序员通过代码处理，Error 很少出现。因此，程序员应该关注Exception为父类的分支下的各种异常类。  异常：Exception 以及他的子类，代表程序运行时发送的各种不期望发生的事件。可以被Java异常处理机制使用，是异常处理的核心。    ​ 总体上根据Javac对异常的处理要求，将异常类分为2类。    非检查异常（unckecked exception）：Error 和 RuntimeException 以及他们的子类。javac 在编译时，不会提示和发现这样的异常，不要求在程序处理这些异常。所以如果愿意，我们可以编写代码处理（使用try…catch…finally）这样的异常，也可以不处理。    对于这些异常，我们应该修正代码，而不是去通过异常处理器处理 。这样的异常发生的原因多半是代码写的有问题。如除0错误ArithmeticException，错误的强制类型转换错误ClassCastException，数组索引越界ArrayIndexOutOfBoundsException，使用了空对象NullPointerException等等。      检查异常（checked exception）：除了Error 和 RuntimeException的其它异常。javac强制要求程序员为这样的异常做预备处理工作（使用try…catch…finally或者throws）。在方法中要么用try-catch语句捕获它并处理，要么用throws子句声明抛出它，否则编译不会通过。    这样的异常一般是由程序的运行环境导致的。因为程序可能被运行在各种未知的环境下，而程序员无法干预用户如何使用他编写的程序，于是程序员就应该为这样的异常时刻准备着。如SQLException , IOException,ClassNotFoundException 等。         参考：http://www.importnew.com/26613.html ​   AIO 和 NIO 有什么区别 ​   继承、重载、重写 public class A {  public void whoAmI(){  System.out.println(I am A);  } } public class B extends A {  // 重写父类方法  public void whoAmI() {  System.out.println(I am B);  } } public class TestMain {  public static void main(String[] args) {  A a = new B();  test(a); }  public static void test(A a){  System.out.println(test A);  a.whoAmI();  }  public static void test(B b){  System.out.println(test B);  b.whoAmI();  } /* * test() 方法共有2个属于重载。 */ }   输出: test A I am B ​ 改造1： 将 test(A a)方法注释掉，test(a) 编译报错：a 对象不能应用于 test(B b)方法上 改造2： 将 main 方法改为如下： public static void main(String[] args) {  B a = new B();  test(a); }   输出： test B I am B ​ 改造3： 在 2 的基础上，将test(B b) 注释掉 输出： test A I am B 由此可知：方法重载时，子类对象找不到精确匹配时，会找匹配父类的方法 关于继承、重载和重写的概念：   继承：继承的作用在于代码的复用。由于继承意味着父类的所有方法亦可在子类中使用，所以发给父类的消息亦可发给衍生类。  方法重载(Overloading):  　　　 Java允许在一个类中，多个方法拥有相同的名字，但在名字相同的同时，必须有不同的参数，这就是重载。  　　　　编译时编译器会根据实际情况挑选出正确的方法，如果编译器找不到匹配的参数或者找出多个可能的匹配就会产生编译时错误，这个过程被称为重载的解析　　　　  　　　　重载规则:  　　　　　　（一）再使用方法重载的时候，必须通过方法中不同的参数列表来实现方法的重载。如：方法的参数个数不同或者方法的参数类型不同。  　　　　　　（二）不能通过访问权限，返回值类型和抛出的异常来实现重载  　　　　　　（三）方法的异常类型和抛出异常的数目不会影响方法的重载，也就是说重载的方法中允许抛出不同的异常  　　　　　　（四）可以有不同的返回值类型，只要方法的参数列表不同即可  　　　　　　（五）可以有不同的访问修饰符  方法重写(Overiding)：  　　　　Java程序中类的继承特性可以产生一个子类，子类继承父类就拥有了父类的非私有的属性（方法和变量），在子类中可以增加自己的属性（方法和变量），同时也可以对父类中的方法进行扩展，以增强自己的功能，这样就称之为重写，也称为复写或者覆盖。  　　　　重写规则:　　　　  　　　　　　在进行方法重写的时候需要遵循以下规则才能实现方法重写：  　　　　　　（一）子类方法的参数列表必须和父类中被重写的方法的参数列表相同（参数个数和参数类型），否则只能实现方法的重载。  　　　　　　（二）子类方法的返回值类型必须和父类中被重写的方法返回值类型相同，否则只能实现方法重载。  　　　　　　（三）在Java规定，子类方法的访问权限不能比父类中被重写的方法的访问权限更小，必须大于或等于父类的访问权限。  　　　　　　（四）在重写的过程中，如果父类中被重写的方法抛出异常，则子类中的方法也要抛出异常。但是抛出的异常也有一定的约束—&gt;子类不能抛出比父类更多的异常，只能抛出比父类更小的异常，或者不抛出异常。   这里需要注意：构造方法不能被继承，因此不能被重写，在子类中只能通过super关键字调用父类的构造方法；但是可以被重载。 ​   Java中equals()和hashCode()的区别和联系 Java的基类Object提供了一些方法，其中equals()方法用于判断两个对象是否相等，hashCode()方法用于计算对象的哈希码。equals()和hashCode()都不是final方法，都可以被重写(overwrite)。 ​ equls() Object类中equals()方法实现如下： public boolean equals(Object obj) {  return (this == obj); }   ​ 重写equals()方法应该遵守的约定： （1）自反性：x.equals(x)必须返回true。 （2）对称性：x.equals(y)与y.equals(x)的返回值必须相等。 （3）传递性：x.equals(y)为true，y.equals(z)也为true，那么x.equals(z)必须为true。 （4）一致性：如果对象x和y在equals()中使用的信息都没有改变，那么x.equals(y)值始终不变。 （5）非null：x不是null，y为null，则x.equals(y)必须为false。 ​ hashCode()： Object类中hashCode()方法的声明如下： public native int hashCode();   可以看出，hashCode()是一个native方法，而且返回值类型是整形；实际上，该native方法将对象在内存中的地址作为哈希码返回，可以保证不同对象的返回值不同。 ​ JDK中对hashCode()方法的作用，以及实现时的注意事项做了说明： （1）hashCode()在哈希表中起作用，如java.util.HashMap。 （2）如果对象在equals()中使用的信息都没有改变，那么hashCode()值始终不变。 （3）如果两个对象使用equals()方法判断为相等，则hashCode()方法也应该相等。 （4）如果两个对象使用equals()方法判断为不相等，则不要求hashCode()也必须不相等；但是开发人员应该认识到，不相等的对象产生不相同的hashCode可以提高哈希表的性能。 ​ hashCode()的作用   当我们向哈希表(如HashSet、HashMap等)中添加对象object时，首先调用hashCode()方法计算object的哈希码，通过哈希码可以直接定位object在哈希表中的位置(一般是哈希码对哈希表大小取余)。如果该位置没有对象，可以直接将object插入该位置；如果该位置有对象(可能有多个，通过链表实现)，则调用equals()方法比较这些对象与object是否相等，如果相等，则不需要保存object；如果不相等，则将该对象加入到链表中。   ​ String中equals()和hashCode()的实现 public boolean equals(Object anObject) {  if (this == anObject) {   return true;  }  if (anObject instanceof String) {   String anotherString = (String)anObject;   int n = value.length;   if (n == anotherString.value.length) {    char v1[] = value;    char v2[] = anotherString.value;    int i = 0;    while (n-- != 0) {    if (v1[i] != v2[i])     return false;    i++;    }    return true;   }  }  return false; } public int hashCode() {  int h = hash;  if (h == 0 &amp;&amp; value.length &gt; 0) {   char val[] = value;   for (int i = 0; i &lt; value.length; i++) {    h = 31 * h + val[i];   }   hash = h;  }  return h; }   通过代码可以看出以下几点： 1、String的数据是final的，即一个String对象一旦创建，便不能修改；形如String s = “hello”; s = “world”;的语句，当s = “world”执行时，并不是字符串对象的值变为了”world”，而是新建了一个String对象，s引用指向了新对象。 2、String类将hashCode()的结果缓存为hash值，提高性能。 3、String对象equals()相等的条件是二者同为String对象，长度相同，且字符串值完全相同；不要求二者是同一个对象。 4、String的hashCode()计算公式为：s[0]31^(n-1) + s[1]31^(n-2) + … + s[n-1] 关于hashCode()计算过程中，为什么使用了数字31，主要有以下原因： 1、使用质数计算哈希码，由于质数的特性，它与其他数字相乘之后，计算结果唯一的概率更大，哈希冲突的概率更小。 2、使用的质数越大，哈希冲突的概率越小，但是计算的速度也越慢；31是哈希冲突和性能的折中，实际上是实验观测的结果。 3、JVM会自动对31进行优化：31 * i == (i « 5) – i ​ 总结：   如果两个对象相同，那么它们的hashCode值一定要相同；  如果两个对象的hashCode相同，它们并不一定相同     ​   ThreadLocal 原理和使用场景 参考：    http://cmsblogs.com/?p=2442  http://blog.xiaohansong.com/2016/08/06/ThreadLocal-memory-leak/#  https://www.jianshu.com/p/ee8c9dccc953   ​ 原理： threadLocal是一个保存线程本地化变量的容器，当在多线程环境下使用 ThreadLocal 维护变量时，其会为每个线程分配一个独立的变量副本，这样一来每个线程都只能对其变量副本进行读写而不会影响到其他线程的变量副本，从而保证了线程安全。   ThreadLocal相当于提供了一种线程隔离，将变量与线程相绑定。 ​  ThredLocal的实现是这样的：   首先在每个线程对象内部保存了一个map，这个map的key是ThreadLocal实例，value是ThreadLocal中要保存的值，每当使用ThreadLocal对变量副本进行set的时候，首先会从当前线程对象内部拿到相应的map，然后将ThreadLocal实例自身作为key，要保存的值作为value，put进map中，这样一来就实现了每个线程保存了独立的变量副本，它们之间互不影响。   ​  ThreadLocal的实现是这样的：每个Thread 维护一个 ThreadLocalMap 映射表，这个映射表的 key 是 ThreadLocal 实例本身，value 是真正需要存储的 Object。 也就是说 ThreadLocal 本身并不存储值，它只是作为一个 key 来让线程从 ThreadLocalMap 获取 value。值得注意的是图中的虚线，表示 ThreadLocalMap 是使用 ThreadLocal 的弱引用作为 Key 的，弱引用的对象在 GC 时会被回收。 常用方法 public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) {  ThreadLocalMap.Entry e = map.getEntry(this);  if (e != null) {  @SuppressWarnings(unchecked)  T result = (T)e.value;  return result;  } } return setInitialValue(); } public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null)  map.set(this, value); else  createMap(t, value); } protected T initialValue() { return null; } private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null)  map.set(this, value); else  createMap(t, value); return value; }   ​ 内存泄漏 ThreadLocalMap使用ThreadLocal的弱引用作为key，如果一个ThreadLocal没有外部强引用来引用它，那么系统 GC 的时候，这个ThreadLocal势必会被回收，这样一来，ThreadLocalMap中就会出现key为null的Entry，就没有办法访问这些key为null的Entry的value，如果当前线程再迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：Thread Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value永远无法回收，造成内存泄漏。 ​ 其实，ThreadLocalMap的设计中已经考虑到这种情况，也加上了一些防护措施：在ThreadLocal的get(),set(),remove()的时候都会清除线程ThreadLocalMap里所有key为null的value。 但是这些被动的预防措施并不能保证不会内存泄漏：   使用static的ThreadLocal，延长了ThreadLocal的生命周期，可能导致的内存泄漏（参考ThreadLocal 内存泄露的实例分析）。  分配使用了ThreadLocal又不再调用get(),set(),remove()方法，那么就会导致内存泄漏。   根源 ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key`就会导致内存泄漏，而不是因为弱引用。 ​ 使用： 每次使用完ThreadLocal，都调用它的remove()方法，清除数据。 ​ 应用场景： 当我们只想在本身的线程内使用的变量，可以用 ThreadLocal 来实现，并且这些变量是和线程的生命周期密切相关的，线程结束，变量也就销毁了。   比如线程中处理一个非常复杂的业务，可能方法有很多，那么，使用 ThreadLocal 可以代替一些参数的显式传递；  上下文管理器、数据库连接等可以用到 ThreadLocal;   总结：    ThreadLocal 不是用于解决共享变量的问题的，也不是为了协调线程同步而存在，而是为了方便每个线程处理自己的状态而引入的一个机制。这点至关重要。  每个Thread内部都有一个ThreadLocal.ThreadLocalMap类型的成员变量，该成员变量用来存储实际的ThreadLocal变量副本。  ThreadLocal并不是为线程保存对象的副本，它仅仅只起到一个索引的作用。它的主要目的是为每一个线程隔离一个类的实例，这个实例的作用范围仅限于线程内部。   ​   ​ ​ 写出运行结果  运行结果： pong ping public static void main(String[] args) { Thread t = new Thread(){  @Override  public void run() {  pong();  } }; t.run(); System.out.println(ping); } static void pong(){ System.out.println(pong); }   这道题是考了Thread 的 start() 和 run() 方法的区别：    start():       使该线程开始执行；Java 虚拟机调用该线程的 run 方法。      结果是两个线程并发地运行；当前线程（从调用返回给 start 方法）和另一个线程（执行其 run 方法，本例中为 main 方法）。      多次启动一个线程是非法的。特别是当线程已经结束执行后，不能再重新启动。    用start方法来启动线程，真正实现了多线程运行，这时无需等待run方法体代码执行完毕而直接继续执行下面的代码。通过调用Thread类的 start()方法来启动一个线程，这时此线程处于就绪（可运行）状态，并没有运行，一旦得到cpu时间片，就开始执行run()方法，这里方法 run()称为线程体，它包含了要执行的这个线程的内容，Run方法运行结束，此线程随即终止。  通过调用run() 方法才是正确的使用线程的方式。    run()    如果该线程是使用独立的 Runnable 运行对象构造的，则调用该 Runnable 对象的 run 方法；否则，该方法不执行任何操作并返回.  Thread 的子类应该重写该方法。   run()方法只是类的一个普通方法而已，如果直接调用Run方法，程序中依然只有主线程这一个线程，其程序执行路径还是只有一条，还是要顺序执行，还是要等待run方法体执行完毕后才可继续执行下面的代码，这样就没有达到写线程的目的。   总结： start() 方法的作用是启动一个新线程，新线程会执行相应的run()方法，start()不能被重复调用。 而run()方法则只是普通的方法调用，在调用线程中顺序运行而已。 ​ JDK1.8.0_152中关于start() 和 run() 的源码 public synchronized void start() {  /**  * This method is not invoked for the main method thread or system  * group threads created/set up by the VM. Any new functionality added  * to this method in the future may have to also be added to the VM.  *  * A zero status value corresponds to state NEW.  */  t t // 如果线程不是就绪状态，则抛出异常！    if (threadStatus != 0)   throw new IllegalThreadStateException();  /* Notify the group that this thread is about to be started  * so that it can be added to the group's list of threads  * and the group's unstarted count can be decremented. */  t t// 将线程添加到ThreadGroup中   group.add(this);  boolean started = false;  try {    // 通过start0()启动线程,新线程会调用run()方法     start0();  // 设置started标记=true   started = true;  } finally {   try {    if (!started) {    group.threadStartFailed(this);    }   } catch (Throwable ignore) {    /* do nothing. If start0 threw a Throwable then    it will be passed up the call stack */   }  }  } private native void start0();   /** * If this thread was constructed using a separate * &lt;code&gt;Runnable&lt;/code&gt; run object, then that * &lt;code&gt;Runnable&lt;/code&gt; object's &lt;code&gt;run&lt;/code&gt; method is called; * otherwise, this method does nothing and returns. * &lt;p&gt; * Subclasses of &lt;code&gt;Thread&lt;/code&gt; should override this method. * * @see  #start() * @see  #stop() * @see  #Thread(ThreadGroup, Runnable, String) */  @Override  public void run() {  if (target != null) {   target.run();  }  }   ​   输出：   static A static B I’m A class HelloA I’m B class HelloB   public class HelloA {  public HelloA(){  System.out.println(HelloA);  }  {  System.out.println(I'm A class);  }  static {  System.out.println(static A);  } } public class HelloB extends HelloA {  public HelloB(){  System.out.println(HelloB);  }  {  System.out.println(I'm B class);  }  static {  System.out.println(static B);  } public static void main(String[] args) {  new HelloB();  } }   分析：   首先去看父类里面有没有 静态代码块，如果有，它先去执行父类里面静态代码块里面的内容，当父类的静态代码块里面的内容执行完毕之后， 接着去执行子类(自己这个类)里面的 静态代码块，  当子类的静态代码块执行完毕之后，它接着又去看父类有没有 非静态代码块，如果有就执行父类的非静态代码块，父类的非静态代码块执行完毕，接着执行父类的 构造方法，  父类的构造方法执行完毕之后，它接着去看子类有没有非静态代码块，如果有就执行子类的非静态代 码块，子类的非静态代码块执行完毕再去执行子类的构造方法   总结： 父类静态代码块——&gt; 子类静态代码块——&gt;父类非静态代码块，构造方法——&gt;子类非静态代码块，构造方法 这个就是一个对象的初始化顺序。    静态代码优先于非静态的代码,是因为被 static 修饰的成员都是类成员,会随着 JVM 加载类的时候加载而执行,而没有被 static 修饰的成员也被称为实例成员,需要创建对象才会随之加载到堆内存。所以静态的会优先非静态的。    执行构造器(构造方法)的时候,在执行方法体之前存在隐式三步: 1,super语句,2,初始化非静态变量; 3,构造代码块。 ​ 并发相关  什么是 CopyOnWrite 容器 ​   ​ 数据库  事务隔离级别有哪些，区别 ​   ​ Spring  spring 事务的传播行为，区别 ​   ​ JVM  对象在 JVM 有哪些状态 ​   常用垃圾收集器 ​   遇到 OOM 如何处理 ​   如何减少上下文切换 编程  二叉树迭代：先根，中根，后根 ​   简单实现一个 HashMap 思路：      HashMap有3个要素：hash函数+数组+单链表      对于hash函数而言，需要考虑:     要快，对于给定的Key，要能够快速计算出在数组中的index。那么什么运算够快呢？显然是位运算！   要均匀分布，要较少碰撞。说白了，我们希望通过hash函数，让数据均匀分布在数组中，不希望大量数据发生碰撞，导致链表过长。那么怎么办到呢？也是利用位运算，通过对数据的二进制的位进行移动，让hash函数得到的数据散列开来，从而减低了碰撞的概率。       发生了碰撞怎么办  JDK的HashMap是通过单链表解决的。那么除了这个方法，还有其他思路么？     如果发生冲突，那么记下这个冲突的位置为index，然后在加上固定步长，即index+step，找到这个位置，看一下是否仍然冲突，如果继续冲突，那么按照这个思路，继续加上固定步长。其实这就是所谓的线性探测来解决Hash冲突的方法       ​   简单实现一个阻塞队列 架构设计  设计一个发号器，毫秒级不能重复并且可支持多业务 方案一： 如果没有并发，订单号只在一个线程内产生，那么由于程序是顺序执行的，不同订单的生成时间戳正常不同，因此用时间戳+随机数（或自增数）就可以区分各个订单。 如果存在并发，且订单号是由一个进程中的多个线程产生的，那么只要把线程ID添加到序列号中就可以保证订单号唯一。 如果存在并发，且订单号是由同一台主机中的多个进程产生的，那么只要把进程ID添加到序列号中就可以保证订单号唯一。 如果存在并发，且订单号是由不同台主机产生的，那么MAC地址、IP地址或CPU序列号等能够区分主机的号码添加到序列号中就可以保证订单号唯一。 ​ 方案二： 时间戳+用户ID+几个随机数+乐观锁。 ​ 方案三： 用redis的原子递增，做好高可用集群。 ​ 方案四（非纯数字）： java自带uuid。 缺点：性能比较差，并且 UUID 比较长，占用空间大，间接导致数据库性能下降，更重要的是，UUID 并不具有有序性，这导致 B+ 树索引在写的时候会有过多的随机写操作（连续的ID会产生部分顺序写） ​ ​ 方案五：snowflake算法 snowflake是twitter开源的分布式ID生成算法，其核心思想为，一个long型的ID：   41bit作为毫秒数  10bit作为机器编号  12bit作为毫秒内序列号   算法单机每秒内理论上最多可以生成1000*(2^12)，也就是400W的ID，完全能满足业务的需求。 参考：vesta-id-generator ​ 总之，思路如下： 思路一：基于数据库生成 标识的生成方法有很多，有集中式的，分布式的；有后端的，前端的，当然还有人工的。 并没有一种通用的生成方法来适应各种应用场景。 人工生成的确是一种方式，比如电子邮箱，微信ID，各种论坛的账号。在人想出标识的那一刻，是无法判断是否是唯一的，对这种生成方式的结果，显然在录入时都需要进行唯一性校验。所以，下面描述的几种生成方式，是在生成的那一刻就在一个命名空间内唯一，而不再需要进行唯一性校验。 而基于数据库生成，一般包含以下几种：  MySQL(5.6) AUTO_INCREMENT 特性 Postgres(REL 9.6 Stable) SEQUENCE 特性 Oracle 数据库的 SEQUENCE 特性，有知道这一特性如何实现的，可以在 知乎 做一下解答。  Flickr Ticket Servers ，同时支持Sharding (文章发表于2010年2月8日，算法上线于2006年1月13日)。 一般地，这种类型的生成方案，都可以设置其实初始值，以及增量步长。 ​ 思路二：基于分布式集群协调器生成 在不使用数据库的情况下，通过一个后台服务对外提供高可用的、固定步长标识生成，则需要分布式的集群协调器进行。 一般的，主流协调器有两类： 以强一致性为目标的：ZooKeeper为代表  以最终一致性为目标的：Consul为代表 ZooKeeper的强一致性，是由Paxos协议保证的；Consul的最终一致性，是由Gossip协议保证的。 在步长累计型生成算法中，最核心的就是保持一个累计值在整个集群中的「强一致性」。同时，这也 会为唯一性标识的生成带来新的形成瓶颈。 ​ 思路三：划分命名空间并行生成 似乎对于分布式的ID生成，以Twitter Snowflake为代表的， Flake 系列算法，经常可以被搜索引擎找到，但似乎MongoDB的ObjectId算法，更早地采用了这种思路。MongoDB 1.0 是在2009年8月27日 发布 的，并且0.9.10(2009年8月24日发布)和1.0两个版本没有差异。 在StackOverflow上，最早的一个关于ObjectId的问题（http://stackoverflow.com/questions/2138687/whats-mongodb-hashs-size/2146071），时间是2010年1月27日。不知道Twitter的同学，是不是受此启发呢？ https://docs.mongodb.com/manual/reference/method/ObjectId/ ​   ​   ​   ​  ",
      "url"      : "http://zhangjinmiao.github.io/interview/2018/04/29/interview-qa.html",
      "keywords" : "Interview, 面试"
    } ,
  
    {
      "title"    : "微服务架构的分布式事务解决方案",
      "category" : "Distributed",
      "content": "详细内容见：微服务架构的分布式事务解决方案（Dubbo 分布式事务处理）        ",
      "url"      : "http://zhangjinmiao.github.io/distributed/2018/04/29/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%AE%80%E4%BB%8B.html",
      "keywords" : "transaction, 分布式事务"
    } ,
  
    {
      "title"    : "JavaSE 之 IO 流",
      "category" : "JavaSE",
      "content": "File 类 1.java.io 包下 File 类：java 程序中的此类的一个对象，就对应着硬盘中的一个文件或网络中的一个资源。 File file1 = new File(“d:  io  helloworld.txt”); File file2 = new File(“d:  io  io1”); File 既可以表示一个文件（.doc .xls .mp3 .avi .jpg .dat），也可以表示一个文件目录！ File 类的对象是与平台无关的。 File 类针对于文件或文件目录，只能进行新建、删除、重命名、上层目录等等的操作。如果涉及到访问文件的内容，File 是无能为力的，只能使用 IO 流下提供的相应的输入输出流来实现。 常把 File 类的对象作为形参传递给相应的输入输出流的构造器中！ 访问文件名：  getName()  getPath()  getAbsoluteFile()  getAbsolutePath()  getParent()  renameTo(File newName) 文件检测  exists() canWrite() canRead() isFile() isDirectory() 获取常规文件信息  lastModified()  length() 文件操作相关  createNewFile()  delete() 目录操作相关  mkDir()  mkDirs()  list()  listFiles() Java IO 原理及流的分类 IO 原理  IO 流用来处理设备之间的数据传输。 Java 程序中，对于数据的输入/输出操作以”流(stream)” 的方式进行。 java.io 包下提供了各种“流”类和接口，用以获取不同种类的数据，并通过标准的方法输入或输出数据。 输入 input：读取外部数据（磁盘、光盘等存储设备的数据）到程序（内存）中。 输出 output：将程序（内存）数据输出到磁盘、光盘等存储设备中 流的分类  按操作数据单位不同分为：字节流(8 bit)，字符流(16 bit) （纯文本文件使用字符流 ，除此之外使用字节流） 按数据流的流向不同分为：输入流，输出流 (站位于程序的角度) 按流的角色的不同分为：节点流，处理流 （流直接作用于文件上是节点流（4 个），除此之外都是处理流）   (抽象基类)  字节流  字符流     输入流  InputStream  Reader    输出流  OutputStream  Writer   Java 的 IO 流共涉及 40 多个类，实际上非常规则，都是从如下 4 个抽象基类派生的。 由这四个类派生出来的子类名称都是以其父类名作为子类名后缀。  重点掌握    抽象基类  节点流（文件流）  缓冲流（处理流的一种,可以提升文件操作的效率）     InputStream  FileInputStream （intread(byte[] b)）  BufferedInputStream (int read(byte[] b))    OutputStream  FileOutputStream （void write(b,0,len)）  BufferedOutputStream (flush()) (void write(b,0,len))    Reader  FileReader （int read(char[] c)）  BufferedReader (readLine()) (int read(char[] c))或 String readLine()    Writer  FileWriter （void write(c,0,len)）  BufferedWriter (flush()) (void write(c,0,len)或 void write(String str))   注：  从硬盘中读入一个文件，要求此文件一定得存在。若不存在，报 FileNotFoundException 的异常 从程序中输出一个文件到硬盘，此文件可以不存在。若不存在，就创建一个实现输出。若存在，则将已存在的文件覆盖 真正开发时，就使用缓冲流来代替节点流 主要最后要关闭相应的流。先关闭输出流，再关闭输入流。将此操作放入 finally。 其他的流 1.转换流：实现字节流与字符流之间的转换 InputStreamReader:输入时，实现字节流到字符流的转换，提高操作的效率（前提是，数据是文本文件） ===&gt;解码：字节数组---&gt;字符串 OutputStreamWriter：输出时，实现字符流到字节流的转换。 ===&gt;编码： 字符串----&gt;字节数组 例子：从键盘输入字符串，要求将读取到的整行字符串转成大写输出。然后继续进行输入操作，直至当输入“e”或者“exit”时，退出程序。 2.标准的输入输出流 System.in: The standard input stream:从键盘输入数据 System.out:The standard output stream：从显示器输出数据 3.打印流 (都是输出流) PrintStream(处理字节) PrintWriter(处理字符)  t可以使用 System.setOut（PrintStream p）重新设置一下输出的位置。  tPrintStream p = new PrintStream(new FileOutputStream(“hello.txt”),true); 4.数据流（处理基本数据类型、String 类、字节数组） DataInputStream DataOutputStream 5.对象流(用来处理对象的)  对象的序列化机制：允许把内存中的 Java 对象转换成平台无关的二进制流，从而允许把这种二进制流持久地保存在磁盘上， 或通过网络将这种二进制流传输到另一个网络节点。当其它程序获取了这种二进制流，就可以恢复成原来的 Java 对象 ObjectInputStream（Object readObject();） ObjectOutputStream (void writeObject(Object obj)) 如何创建流的对象： ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(new File(person.txt)));  t t t ObjectInputStream ois = new ObjectInputStream(new FileInputStream(new File(person.txt))); 实现序列化机制的对象对应的类的要求：  ①要求类要实现 Serializable 接口   ②同样要求类的所有属性也必须实现 Serializable 接口   ③ 要求给类提供一个序列版本号：private static final long serialVersionUID;   ④属性声明为 static 或 transient 的，不可以实现序列化 6.随机存取文件流:RandomAccessFile  既可以充当一个输入流，又可以充当一个输出流：public RandomAccessFile(File file, String mode)   支持从文件的开头读取、写入。若输出的文件不存在，直接创建。若存在，则是对原有文件内容的覆盖。   支持任意位置的“插入”。 ​ ",
      "url"      : "http://zhangjinmiao.github.io/javase/2018/05/04/IO.html",
      "keywords" : "io, file"
    } ,
  
    {
      "title"    : "分布式事务解决方案",
      "category" : "Distributed",
      "content": "  单体应用拆分为分布式系统后，进程间的通讯机制和故障处理措施变的更加复杂 方案：  随着 RPC 框架的成熟，该问题已得到解决   dubbo 可以支持多种通讯协议  springcloud 可以非常好的支持 restful 调用     系统微服务化后，一个看似简单的功能，内部可能需要调用多个服务并操作多个数据库实现，服务调用的==分布式事务==问题变的非常突出 方案：    参考阿里开源分布式事务解决方案—-GTS    TCC 型事务 java 实现 针对这个项目的源码分析——项目实战   一小伙伴写的开源项目—-TCC 分布式事务框架  蚂蚁金融云 —— 分布式事务服务（DTS） —— 场景介绍  基于 XA 协议的两阶段提交方案(不适合)  TCC 方案(金融、电商较多)  基于消息的最终一致性方案(依赖中间件)   芋道源码 ： ​ ​ &gt; 1. tcc ：tcc-transaction ​ &gt; 2. mq ：rocketmq ，需要等 4.3 版；目前版本是阉割的。 ​ &gt; 3. bed ：sharding-jdbc ​ &gt; 4. xa ：mycat 弱 xa ；目前版本性能有问题。  微服务数量众多，其测试、部署、监控等都变的更加困难 方案：   随着 docker、devops 技术的发展以及各公有云 paas 平台自动化运维工具的推出，微服务的测试、部署与运维会变得越来越容易   分布式事务  分布式事务是指会涉及到操作==多个数据库==的事务。其实就是将对同一库事务的概念扩大到了对多个库的事务。目的是为了保证分布式系统中的==数据一致性==。 在分布式系统中，各个节点之间在物理上相互独立，通过网络进行沟通和协调。由于存在事务机制，可以保证每个独立节点上的数据操作可以满足 ACID。但是，相互独立的节点之间无法准确的知道其他节点中的事务执行情况。所以从理论上讲，两台机器理论上无法达到一致的状态。如果想让分布式部署的多台机器中的数据保持一致性，那么就要保证在所有节点的数据写操作，要不全部都执行，要么全部的都不执行。但是，一台机器在执行本地事务的时候无法知道其他机器中的本地事务的执行结果。所以他也就不知道本次事务到底应该 commit 还是 roolback。所以，常规的解决办法就是引入一个“==协调者==”的组件来统一调度所有分布式节点的执行。 基于 XA 协议的两阶段（2PC）提交方案 交易中间件与数据库通过 XA 接口规范，使用两阶段提交来完成一个全局事务，XA 规范的基础是两阶段提交协议。 第一阶段是表决阶段，所有参与者都将本地事务能否成功的信息反馈发给协调者； 第二阶段是执行阶段，协调者根据所有参与者的反馈，通知所有参与者，步调一致地在所有分支上提交或者回滚。 两阶段提交方案应用非常广泛，几乎所有商业 OLTP 数据库都支持 XA 协议。 缺点： ​ 两阶段提交方案存在==锁定资源时间长（同步阻塞）==，==对性能影响很大==，、==单点==、==脑裂==等问题基本不适合解决微服务事务问题。  同步阻塞问题： 执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。  单点故障： 由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）  数据不一致： 在二阶段提交的阶段二中，当协调者向参与者发送 commit 请求之后，发生了局部网络异常或者在发送 commit 请求过程中协调者发生了故障，这会导致只有一部分参与者接受到了 commit 请求。而在这部分参与者接到 commit 请求之后就会执行 commit 操作。但是其他部分未接到 commit 请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。  二阶段无法解决的问题： 协调者在发出 commit 消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否已经被提交。 三阶段提交（3PC） 与两阶段提交不同之处在于  引入超时机制。同时在协调者和参与者中都引入超时机制。  在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。 也就是说，除了引入超时机制之外，3PC 把 2PC 的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。  CanCommit 阶段： 协调者向参与者发送 commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。 这里有 2 步：    ==协调者询问参与者==：协调者向参与者发送 CanCommit 请求询问是否可以执行事务，然后等待参与者响应。  ==参与者响应协调者==：正常情况下，参与者若可以顺利执行事务，则返回 Yes，并进入预备状态，否则返回 No。     PreCommit 阶段： 协调者根据参与者的反应情况来决定是否可以执行事务的 PreCommit 操作。有两种情况： 假如协调者从所有的参与者获得的反馈都是 Yes 响应，那么就会执行事务的预执行。    ==发送预提交请求== 协调者向参与者发送 PreCommit 请求，并进入 Prepared 阶段。  ==事务预提交== 参与者接收到 PreCommit 请求后，会执行事务操作，并将 undo 和 redo 信息记录到事务日志中。  ==响应反馈== 如果参与者成功的执行了事务操作，则返回 ACK 响应，同时开始等待最终指令。   假如有任何一个参与者向协调者发送了 No 响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。    ==发送中断请求== 协调者向所有参与者发送 abort 请求。  ==中断事务== 参与者收到来自协调者的 abort 请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。     doCommit 阶段 该阶段执行真正的事务提交，也分为两种情况： 执行提交    ==发送提交请求== 协调者接收到参与者发送的 ACK 响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送 doCommit 请求。  ==事务提交== 参与者接收到 doCommit 请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。  ==响应反馈== 事务提交完之后，向协调者发送 Ack 响应。  ==完成事务== 协调者接收到所有参与者的 ack 响应之后，完成事务。   中断事务： 协调者没有接收到参与者发送的 ACK 响应（可能是接受者发送的不是 ACK 响应，也可能响应超时），那么就会执行中断事务。    ==发送中断请求== 协调者向所有参与者发送 abort 请求  ==事务回滚== 参与者接收到 abort 请求之后，利用其在阶段二记录的 undo 信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。  ==反馈结果== 参与者完成事务回滚之后，向协调者发送 ACK 消息  ==中断事务== 协调者接收到参与者反馈的 ACK 消息之后，执行事务的中断。   2PC 与 3PC 的区别 相对于 2PC，3PC 主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行 commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的 abort 响应没有及时被参与者接收到，那么参与者在等待超时之后执行了 commit 操作。这样就和其他接到 abort 命令并执行回滚的参与者之间存在数据不一致的情况。 TCC 方案 TCC 方案其实是两阶段提交的一种改进。其将整个业务逻辑的每个分支显式的分成了==Try==、==Confirm==、==Cancel== 三个操作。 Try 部分完成业务的准备工作， confirm 部分完成业务的提交， cancel 部分完成事务的回滚。 基本原理如下图所示。  事务开始时，业务应用会向事务协调器注册启动事务。 之后业务应用会调用所有服务的try接口，完成一阶段准备。 之后事务协调器会根据 try 接口返回情况，决定调用confirm接口或者cancel接口。如果接口调用失败，会进行重试。 优点： TCC 方案让应用自己定义数据库操作的粒度，使得降低锁冲突、提高吞吐量成为可能。 不足： 对应用的侵入性强。业务逻辑的每个分支都需要实现 try、confirm、cancel 三个操作，应用侵入性较强，改造成本高。 实现难度较大。需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求， confirm 和 cancel 接口必须实现幂等。 基于消息的最终一致性方案 消息一致性方案是通过==消息中间件==保证上、下游应用数据操作的一致性。 基本思路是： 将==本地操作和发送消息放在一个事务中==，保证本地操作和消息发送要么两者都成功或者都失败。下游应用向消息系统订阅该消息，收到消息后执行相应操作。 消息方案从本质上讲是将分布式事务转换为两个本地事务，然后依靠下游业务的重试机制达到最终一致性。 缺点： 基于消息的最终一致性方案==对应用侵入性也很高==，应用需要进行大量业务改造，成本较高。 参考：  https://mp.weixin.qq.com/s/bUtu2nTs0bybnTvk-iLt6Q  关于分布式事务、两阶段提交协议、三阶提交协议 ",
      "url"      : "http://zhangjinmiao.github.io/distributed/2018/05/05/Distributed-transaction-solution.html",
      "keywords" : "Distributed Transaction,架构,分布式"
    } ,
  
    {
      "title"    : "对Linux文件权限的理解",
      "category" : "Linux",
      "content": "本文转自：0xcafedaddy 755,775,777,ugoa 等分别代表什么含义?这些数字是如何得到的? 1.常用的linux文件权限： 444 -r--r--r-- 600 -rw------- 644 -rw-r--r-- 666 -rw-rw-rw- 700 -rwx------ 744 -rwxr--r-- 755 -rwxr-xr-x 777 -rwxrwxrwx 注:使用ll命令查看文件/文件夹属性时候,一共有10列,第一个小格表示是文件夹或者连接等等 d表示文件夹,l表示连接文件,-表示文件 2.用户分组权限概念 从左至右:  1-3位数字代表文件所有者的权限 4-6位数字代表同组用户的权限 7-9数字代表其他用户的权限 3.数字概念解析  读(r)=4   写(w)=2   执行(x)=1   读+写+执行=4+2+1=7 通过4、2、1的组合，得到以下几种权限： 0（没有权限） 4（读取权限） 5（4+1 | 读取+执行） 6（4+2 | 读取+写入） 7（4+2+1 | 读取+写入+执行）   4.rwx字符的概念 以755为例  1-3位7等于4+2+1，rwx，所有者具有读取、写入、执行权限 4-6位5等于4+1+0，r-x，同组用户具有读取、执行权限但没有写入权限 7-9位5，同上，也是r-x，其他用户具有读取、执行权限但没有写入权限 5.对ugoa的理解 ugoa简写的含义:    简称  含义     u  user(文件的所有者)    g  group(与文件相关联的组)    o  other(所有其他用户)    a  all(ugo的所有用户)   操作运算符:    operator  含义     +  为指定的用户类型添加权限    -  为指定的用户类型删除权限    =  设定或重置指定用户类型的权限   设计ugoa的含义:  我之前有个疑问以数字的形式去修改文件权限,为什么还需要有ugoa这种形式去修改权限呢? 我之前有个疑问以数字的形式去修改文件权限,为什么还需要有ugoa这种形式去修改权限呢? 原因可能是:虽然有了数字形式修改权限,但是修改起来可能不是很方便,使用ugoa的形式可以快速的去更改一个文件的权限,例如我需要去掉group和other用户的执行权限:chmod go-x hello.sh 或者给所有用户添加读写执行权限: chmod a+rwx hello.sh 等等 ",
      "url"      : "http://zhangjinmiao.github.io/linux/2018/05/06/%E5%AF%B9Linux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90%E7%9A%84%E7%90%86%E8%A7%A3.html",
      "keywords" : "linux, 命令"
    } ,
  
    {
      "title"    : "MySQL 分库分表中间件",
      "category" : "MySQL",
      "content": "数据库中间件  金山的 kingshard 百度的 heisenberg 58 同城的 Oceanus 360 的 Atlas 美团点评的 DBProxy Mycat mysql-proxy  分布式数据库  TiDB ",
      "url"      : "http://zhangjinmiao.github.io/mysql/2018/05/16/Mysql%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E4%B8%AD%E9%97%B4%E4%BB%B6.html",
      "keywords" : "mysql, 分库分表"
    } ,
  
    {
      "title"    : "List 源码解析之 ArrayList 源码分析",
      "category" : "SourceCode",
      "content": "注：文中源码为 JDK 1.8 。 ArrayList 简介 ArrayList 是基于数组实现的， 是一个动态扩展的数组，容量可自动增长。 ArrayList 是非线程安全的，只能在单线程环境下使用，多线程环境考虑使用 Collections.synchronizedList(List list) 函数返回一个线程安全的 ArrayList 类，也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类。 ArrayList 实现了 Serializable 接口，因此它支持序列化，能够通过序列化传输，实现了 RandomAccess 接口，支持快速随机访问，实际上就是通过下标序号进行快速访问，实现了 Cloneable 接口，能被克隆。 属性和构造函数 private static final int DEFAULT_CAPACITY = 10; // 默认初始值 transient Object[] elementData; // 存放数据的数组 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; // 存放的数组默认容量10 protected transient int modCount = 0; // List被修改的次数 // 构造函数 /** * Constructs an empty list with the specified initial capacity. * * @param initialCapacity the initial capacity of the list * @throws IllegalArgumentException if the specified initial capacity * is negative */ public ArrayList(int initialCapacity) {  if (initialCapacity &gt; 0) { // 创建对应容量的数组  this.elementData = new Object[initialCapacity];  } else if (initialCapacity == 0) { // 空数组  this.elementData = EMPTY_ELEMENTDATA;  } else {  throw new IllegalArgumentException(Illegal Capacity: +       initialCapacity);  } } /** * Constructs an empty list with an initial capacity of ten. */  public ArrayList() { // 默认容量10的数组  this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;  } /** * 构造一个包含指定collection 的元素的列表，这些元素按照 * 该collection 的迭代器返回它们的顺序排列的 */ public ArrayList(Collection&lt;? extends E&gt; c) { elementData = c.toArray(); if ((size = elementData.length) != 0) {  // c.toArray might (incorrectly) not return Object[] (see 6260652)  if (elementData.getClass() != Object[].class)  elementData = Arrays.copyOf(elementData, size, Object[].class); } else {  // replace with empty array.  this.elementData = EMPTY_ELEMENTDATA; } } 存储 set(int index, E element) // 用指定的元素替代此列表中指定位置上的元素，并返回以前位于该位置上的元素  public E set(int index, E element) { rangeCheck(index); // 越界检测 E oldValue = elementData(index); elementData[index] = element; // 赋值到指定位置,复制的仅仅是引用 return oldValue; } private void rangeCheck(int index) { if (index &gt;= size)  throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } add(E e) // 添加一个元素到此列表的尾部 时间复杂度O(1)，非常高效 public boolean add(E e) {  ensureCapacityInternal(size + 1); // 容量 + 1 elementData[size++] = e;  return true; } // 如有必要，增加此 ArrayList 实例的容量，以确保它至少能够容纳最小容量参数所指定的元素数 private void ensureCapacityInternal(int minCapacity) {  if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {  minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity);  }  tensureExplicitCapacity(minCapacity); } private void ensureExplicitCapacity(int minCapacity) {  modCount++; // overflow-conscious code  if (minCapacity - elementData.length &gt; 0)  grow(minCapacity); } // 增加容量，确保可容纳元素 private void grow(int minCapacity) {  // overflow-conscious code 注意溢出  int oldCapacity = elementData.length;  int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 扩容到原始容量的1.5倍  if (newCapacity - minCapacity &lt; 0)  newCapacity = minCapacity;  if (newCapacity - MAX_ARRAY_SIZE &gt; 0)  newCapacity = hugeCapacity(minCapacity);  // minCapacity is usually close to size, so this is a win:  elementData = Arrays.copyOf(elementData, newCapacity); // 扩容并复制  t// 由于Java GC自动管理了内存，这里也就不需要考虑源数组释放的问题。 // 关于Java GC这里需要特别说明一下，有了垃圾收集器并不意味着一定不会有内存泄漏。对象能否被GC的依据是是否还有引用指向它，上面代码中如果不手动赋null值，除非对应的位置被其他元素覆盖，否则原来的对象就一直不会被回收。 } private static int hugeCapacity(int minCapacity) {  if (minCapacity &lt; 0) // overflow  throw new OutOfMemoryError();  return (minCapacity &gt; MAX_ARRAY_SIZE) ?  Integer.MAX_VALUE :  MAX_ARRAY_SIZE; } 空间问题解决后，插入就变得容易了 add(int index, E element) // 将指定的元素添加到此列表中的指定位置 // 如果当前位置有元素，则向右移动当前位于该位置的元素以及所有后续元素（将其索引加1） public void add(int index, E element) {  rangeCheckForAdd(index); // 数组越界检测  t// 如果数组长度不足，将进行扩容  ensureCapacityInternal(size + 1); // Increments modCount!!  t// 将 elementData 中从Index 位置开始、长度为size-index 的元素，  t// 拷贝到从下标为index+1 位置开始的新的elementData 数组中。  t// 即将当前位于该位置的元素以及所有后续元素右移一个位置。  System.arraycopy(elementData, index, elementData, index + 1,    size - index); // 低效  elementData[index] = element;  size++; } private void rangeCheckForAdd(int index) {  if (index &gt; size || index &lt; 0)  throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } addAll(Collection&lt;? extends E&gt; c) // 按照指定collection 的迭代器所返回的元素顺序，将该collection 中的所有元素添加到此列表的尾部 public boolean addAll(Collection&lt;? extends E&gt; c) {  Object[] a = c.toArray();  int numNew = a.length;  ensureCapacityInternal(size + numNew); // Increments modCount  System.arraycopy(a, 0, elementData, size, numNew);  size += numNew;  return numNew != 0; } addAll(int index, Collection&lt;? extends E&gt; c) // 从指定的位置开始，将指定collection 中的所有元素插入到此列表中 public boolean addAll(int index, Collection&lt;? extends E&gt; c) {  rangeCheckForAdd(index); Object[] a = c.toArray();  int numNew = a.length;  ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index;  if (numMoved &gt; 0)  System.arraycopy(elementData, index, elementData, index + numNew,    numMoved); System.arraycopy(a, 0, elementData, index, numNew);  size += numNew;  return numNew != 0; } 读取 get(int index) // 返回此列表中指定位置上的元素 public E get(int index) {  rangeCheck(index); return elementData(index); } E elementData(int index) { return (E) elementData[index]; //注意类型转换 } 删除 根据下标和指定对象删除，删除时，被移除元素以后的所有元素向左移动一个位置。 remove(int index) // 删除指定位置的元素，并返回删除元素 public E remove(int index) {  rangeCheck(index)![ArrayList_add.png](http://upload-images.jianshu.io/upload_images/626005-4973ec5f1213fd0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) ; modCount++;  E oldValue = elementData(index); int numMoved = size - index - 1;  if (numMoved &gt; 0)  System.arraycopy(elementData, index+1, elementData, index,    numMoved); // 向前移动  elementData[--size] = null; // clear to let GC do its work，清除该位置的引用，让GC起作用 return oldValue; } remove(Object o) // 移除此列表中首次出现的指定元素（如果存在）。这是应为ArrayList 中允许存放重复的元素 public boolean remove(Object o) { // 由于ArrayList 中允许存放null，因此下面通过两种情况来分别处理  if (o == null) {  for (int index = 0; index &lt; size; index++)  if (elementData[index] == null) {   // 类似remove(int index)，移除列表中指定位置上的元素   fastRemove(index);   return true;  }  } else {  for (int index = 0; index &lt; size; index++)  if (o.equals(elementData[index])) {   fastRemove(index);   return true;  }  }  return false; } // 私有删除方法，不进行越界检测 private void fastRemove(int index) { modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0)  System.arraycopy(elementData, index+1, elementData, index,    numMoved); elementData[--size] = null; // clear to let GC do its work } 容量调整 如果添加元素前已经预测到了容量不足，可手动增加 ArrayList 实例的容量，以减少递增式再分配的数量。 ensureCapacity(int minCapacity) public void ensureCapacity(int minCapacity) {  int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA)  // any size if not default element table  ? 0  // larger than default for default empty table. It's already  // supposed to be at default size.  : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) {  ensureExplicitCapacity(minCapacity);  } } // 具体查看ensureExplicitCapacity 在 ensureExplicitCapacity 中，可以看出，每次扩容时，会将老数组中的元素重新拷贝一份到新的数组中（System.arraycopy()方法），每次数组容量的增长大约是其原容量的 1.5 倍。这种操作的代价是很高的，因此要尽量避免数组容量扩容，使用时尽可能的指定容量，或者根据需求，通过调用 ensureCapacity 方法来手动增加 ArrayList 实例的容量。 trimToSize() 该方法是将底层数组的容量调整为当前列表保存的实际元素的大小。 public void trimToSize() {  modCount++;  if (size &lt; elementData.length) {  elementData = (size == 0)  ? EMPTY_ELEMENTDATA  : Arrays.copyOf(elementData, size);  } } 遍历 通过迭代器遍历  Integer value = null; Iterator iter = list.iterator(); while (iter.hasNext()) {  value = (Integer)iter.next(); }  随机访问，通过索引值去遍历（推荐）  Integer value = null; int size = list.size(); for (int i=0; i&lt;size; i++) {  value = (Integer)list.get(i);  }  ForEach 循环遍历  Integer value = null; for (Integer integ:list) {  value = integ; } 经测试，耗时：ForEach 循环 &gt; 随机 &gt; 迭代器，优先使用迭代器方式 。 ForEach 的本质也是迭代器模式，可以反编译查看，而且还多了一步赋值操作，增加了开销。 总结 size(), isEmpty(), get(), set()方法均能在常数时间内完成，add()方法的时间开销跟插入位置有关，addAll()方法的时间开销跟添加元素的个数成正比。其余方法大都是线性时间。  排列有序（索引从 0 开始），可插入空值，可重复   底层数组实现   读取快，增删慢（需要移动元素，插入删除效率低）   非同步，线程不安全   初始容量默认为 10，当容量不够时，ArrayList 是当前容量 * 1.5 + 1 扩容时，需要调用 System.arraycopy，copy 本来就是一个耗时的操作，所以尽量初始化容量。 即使理论上效率还可以 （System.arraycopy() 方法是一个 native 的，最终调用了 C 语言的 memmove() 函数，比一般的复制方法的实现效率要高很多。）   创建时，初始化最小容量 ​  参考   CarpenterLee  莫等闲  兰亭风雨 ",
      "url"      : "http://zhangjinmiao.github.io/sourcecode/2019/03/19/ArrayList.html",
      "keywords" : "List, ArrayList, 源码"
    } ,
  
    {
      "title"    : "ArrayList、Vector、LinkedList 的区别",
      "category" : "SourceCode",
      "content": "注：文中源码为 JDK 1.8 。 实现方式    ArrayList，Vector 是基于数组的实现。    LinkedList 是基于链表的实现。 ​   同步  ArrayList,LinkedList 不是线程安全的。  Vector 是线程安全的，实现方式是在方法中加 synchronized 进行限定。 性能消耗  ArrayList 和 Vector 由于是基于数组实现，所以在指定位置插入和删除时间复杂度为 O(n)，还可能出现扩容问题，这比较消耗性能。  LinkedList 不会出现扩容问题，适合增删操作；查找元素需要遍历链表，时间复杂度为 O(n)。 使用场景  快速插入、删除元素，使用 LinkedList  快速随机访问元素，使用 ArrayList  单线程，使用 List，比如 ArrayList  多线程，使用 Vector  ",
      "url"      : "http://zhangjinmiao.github.io/sourcecode/2019/03/19/ArrayList-Vector%E5%92%8CLinkedList%E7%9A%84%E5%8C%BA%E5%88%AB.html",
      "keywords" : "List, Vector, 源码"
    } ,
  
    {
      "title"    : "List 源码解析之 LinkedList 源码分析",
      "category" : "SourceCode",
      "content": "注：文中源码为 JDK 1.8 。 LinkedList 简介 实现了 List 和 Deque 接口，既可以看作一个顺序容器，又可以看作一个队列（Queue），同时又可以看作一个栈（Stack）（处理栈和队列问题，首选 ArrayDeque，它的性能比 LinkedList 作栈和队列使用好很多）。 LinkedList 是一种双向链表，通过first和last引用分别指向链表的第一个和最后一个元素。 LinkedList 是非线程安全的，也就是说它不是同步的，适合单线程环境使用；需要多个线程并发访问，可以先采用Collections.synchronizedList()方法对其进行包装。 LinkedList 实现了 Serializable 接口，因此它支持序列化，能够通过序列化传输，实现了 Cloneable 接口，能被克隆。 属性和构造函数 transient int size = 0; //数量  transient Node&lt;E&gt; first; //首节点  transient Node&lt;E&gt; last; //尾节点 private static class Node&lt;E&gt; { E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) {  this.item = element;  this.next = next;  this.prev = prev; } } /** * Constructs an empty list. */ public LinkedList() { } /** * 构造一个包含指定collection 的元素的列表，这些元素按照 * 该collection 的迭代器返回它们的顺序排列的 */ public LinkedList(Collection&lt;? extends E&gt; c) { this(); addAll(c); } 存储 set(int index, E element) // 用指定的元素替代此列表中指定位置上的元素，并返回以前位于该位置上的元素 public E set(int index, E element) { checkElementIndex(index); Node&lt;E&gt; x = node(index); E oldVal = x.item; x.item = element; return oldVal; } add(E e) // 添加指定元素到链表尾部，花费时间为常数时间 public boolean add(E e) { linkLast(e); return true; } /** * Links e as last element. 链接e作为最后一个元素，默认向表尾节点加入新的元素 */ void linkLast(E e) { final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); // 当插入数据量大时，生成对象比较耗时 last = newNode; if (l == null)  first = newNode; else  l.next = newNode; size++; modCount++; } add(int index, E element) // 在指定位置插入 public void add(int index, E element) { checkPositionIndex(index); if (index == size)  linkLast(element); else  linkBefore(element, node(index)); } // 下标位置越界检查 private void checkPositionIndex(int index) {  if (!isPositionIndex(index))  throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } /** * Inserts element e before non-null Node succ. */ void linkBefore(E e, Node&lt;E&gt; succ) { // assert succ != null; final Node&lt;E&gt; pred = succ.prev; final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); succ.prev = newNode; if (pred == null)  first = newNode; else  pred.next = newNode; size++; modCount++; } addAll(Collection&lt;? extends E&gt; c) // 按照指定collection 的迭代器所返回的元素顺序，将该collection 中的所有元素添加到此列表的尾部 public boolean addAll(Collection&lt;? extends E&gt; c) { return addAll(size, c); } // 具体看源码吧 读取 get(int index) // 返回指定下标的元素 public E get(int index) { checkElementIndex(index); return node(index).item; } 删除 1.先找到要删除元素的引用，2.修改相关引用，完成删除操作 remove(int index) 使用下标计数 // 删除指定位置的元素，并返回 public E remove(int index) { checkElementIndex(index); // 检查下标是否越界 return unlink(node(index)); } /** * Unlinks non-null node x. */ E unlink(Node&lt;E&gt; x) { // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) { //删除的是第一个元素  first = next; } else {  prev.next = next;  x.prev = null; } if (next == null) { //删除的是最后一个元素  last = prev; } else {  next.prev = prev;  x.next = null; } x.item = null; //let GC work size--; modCount++; return element; } remove(Object o) 使用 equales 方法 // 删除指定元素 public boolean remove(Object o) { if (o == null) {  for (Node&lt;E&gt; x = first; x != null; x = x.next) {  if (x.item == null) {  unlink(x);  return true;  }  } } else {  for (Node&lt;E&gt; x = first; x != null; x = x.next) {  if (o.equals(x.item)) {  unlink(x);  return true;  }  } } return false; } 增删元素的时候，只需改变指针，不需要像数组那样对整体数据进行移动、复制等消耗性能的操作。 遍历 遍历的时候耗时，for 循环(无穷大) &gt; ForEach &gt; 迭代器，优先使用迭代器方式。 for(Iterator&lt;String&gt; it = list.iterator();it.hasNext();){} 队列操作 E peek() // 获取第一个元素 public E peek() {  final Node&lt;E&gt; f = first;  return (f == null) ? null : f.item;  } E peekFirst() // 获取第一个元素，同E peek() public E peekFirst() {  final Node&lt;E&gt; f = first;  return (f == null) ? null : f.item; } E peekLast() // 获取最后一个元素 public E peekLast() { final Node&lt;E&gt; l = last; return (l == null) ? null : l.item; } E pop() // 移除第一个元素并返回 public E pop() { return removeFirst(); } E pollFirst() // 移除第一个元素，同E pop() public E pollFirst() { final Node&lt;E&gt; f = first; return (f == null) ? null : unlinkFirst(f); } E pollLast() // 移除最后一个元素 public E pollLast() { final Node&lt;E&gt; l = last; return (l == null) ? null : unlinkLast(l); } push(E e) // 队首添加一个元素 public void push(E e) { addFirst(e); } 总结  基于双向循环链表实现，不存在容量不足的问题，没有扩容的方法 增删元素快，查找慢 元素排列有序，可重复，可为 null 实现了栈和队列的操作方法，因此也可以作为栈、队列和双端队列来使用。 非同步，线程不安全 参考  pzxwhc  兰亭风雨  JCFInternals  ",
      "url"      : "http://zhangjinmiao.github.io/sourcecode/2019/03/19/LinkedList.html",
      "keywords" : "List, LinkedList, 源码"
    } ,
  
    {
      "title"    : "List 源码解析之 Vector 源码分析",
      "category" : "SourceCode",
      "content": "注：文中源码为 JDK 1.8 。 Vector 简介 和 ArrayList 一样，Vector 也是基于数组实现的，是动态数组，容量可自动增长。 与 ArrayList 不同的是，它有好多方法都加入了synchronized修饰，所以是线程安全的，可用于多线程环境。 Vector 没有实现 Serializable 接口，不支持序列化，实现了 Cloneable 接口，能被克隆，实现了 RandomAccess 接口，支持快速随机访问。 属性和构造函数 protected Object[] elementData; // 存放数据的数组 protected int elementCount; // 元素个数 protected int capacityIncrement; // 容量增量 // 构造一个指定容量和增量的Vector public Vector(int initialCapacity, int capacityIncrement) { super(); if (initialCapacity &lt; 0)  throw new IllegalArgumentException(Illegal Capacity: + initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement; } // 构造一个指定容量，增量为 0 的Vector public Vector(int initialCapacity) { this(initialCapacity, 0); } // 构造一个容量为10，增量为 0 的Vector public Vector() { this(10); } 存储 synchronized boolean add(E e) 多了 synchronized 修饰。 // 末尾插入元素，注意有synchronized修饰 public synchronized boolean add(E e) { modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true; } add(int index, E element) // 指定位置插入 public void add(int index, E element) { insertElementAt(element, index); } // 具体实现查看源码... public synchronized void insertElementAt(E obj, int index) { modCount++; if (index &gt; elementCount) {  throw new ArrayIndexOutOfBoundsException(index         + &gt; + elementCount); } ensureCapacityHelper(elementCount + 1); System.arraycopy(elementData, index, elementData, index + 1, elementCount - index); elementData[index] = obj; elementCount++; } set(int index, E element) // 用指定的元素替代此列表中指定位置上的元素，并返回以前位于该位置上的元素 public synchronized E set(int index, E element) { if (index &gt;= elementCount)  throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); elementData[index] = element; return oldValue; } 读取 get(int index) 同 ArrayList // 返回此列表中指定位置上的元素 public E get(int index) { rangeCheck(index); return elementData(index); } 删除 remove(int index) // 删除指定位置元素 public synchronized E remove(int index) { modCount++; if (index &gt;= elementCount)  throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); int numMoved = elementCount - index - 1; if (numMoved &gt; 0)  System.arraycopy(elementData, index+1, elementData, index,     numMoved); elementData[--elementCount] = null; // Let gc do its work return oldValue; } remove(Object o) // 移除此列表中首次出现的指定元素（如果存在）。 public boolean remove(Object o) { return removeElement(o); } public synchronized boolean removeElement(Object obj) { modCount++; int i = indexOf(obj); if (i &gt;= 0) {  removeElementAt(i);  return true; } return false; } public synchronized void removeElementAt(int index) { modCount++; if (index &gt;= elementCount) {  throw new ArrayIndexOutOfBoundsException(index + &gt;= +         elementCount); } else if (index &lt; 0) {  throw new ArrayIndexOutOfBoundsException(index); } int j = elementCount - index - 1; if (j &gt; 0) {  System.arraycopy(elementData, index + 1, elementData, index, j); } elementCount--; elementData[elementCount] = null; /* to let gc do its work */ } 容量调整 和 ArrayList 类似，多了 synchronized 修饰。 ensureCapacity(int minCapacity) public synchronized void ensureCapacity(int minCapacity) {  if (minCapacity &gt; 0) {  modCount++;  ensureCapacityHelper(minCapacity);  } } trimToSize() public synchronized void trimToSize() {  modCount++;  int oldCapacity = elementData.length;  if (elementCount &lt; oldCapacity) {  elementData = Arrays.copyOf(elementData, elementCount);  } } 总结 默认初始大小 10，扩容也是使用 copy 原数组的方式 很多方法都加入了synchronized同步语句，来保证线程安全 允许元素为 null，也可以插入重复值 性能消耗也主要来源于 扩容 与 ArrayList 实现大同小异，基本不用 参考  pzxwhc  兰亭风雨  ",
      "url"      : "http://zhangjinmiao.github.io/sourcecode/2019/03/19/Vector.html",
      "keywords" : "List, Vector, 源码"
    } ,
  
    {
      "title"    : "Redis 系列学习",
      "category" : "Redis",
      "content": "第 1 章 简介 1.1 什么是 redis Redis—— Remote Dictionary Server，它是一个开源的使用 ANSI C 语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value 数据库，并提供多种语言的 API，我们可使用它构建高性能，可扩展的 Web 应用程序。 Redis 是目前最流行的键值对存储数据库，从 2010 年 3 月 15 日起，Redis 的开发工作由 VMware 主持。从 2013 年 5 月开始，Redis 的开发由 Pivotal 赞助。 如果你想了解 Redis 最新的资讯，可以访问 官方网站:http://redis.io/ 中文社区：http://www.redis.cn/ 1.2 应用场景 在实际生产环境中，很多公司都曾经使用过这样的架构，使用 MySQL 进行海量数据存储的，通过 Memcached 将热点数据加载到 cache，加速访问，但随着业务数据量的不断增加，和访问量的持续增长，我们遇到了很多问题： MySQL 需要不断进行拆库拆表，Memcached 也需不断跟着扩容，扩容和维护工作占据大量开发时间。 Memcached 与 MySQL 数据库数据一致性问题。 Memcached 数据命中率低或 down 机，大量访问直接穿透到 DB，MySQL 无法支撑。 跨机房 cache 同步问题。 以上问题都是非常的棘手，不过现在不用担心了，因为我们可以使用 redis 来完美解决。 1.3 特点 相对于其他的同类型数据库而言，Redis 支持更多的数据类型，除了和 string外，还支持lists（列表）、sets（集合）和zsets（有序集合）几种数据类型。 这些数据类型都支持push/pop、add/remove及取交集、并集和差集及更丰富的操作，而且这些操作都是原子性的。 Redis 具备以下特点： 异常快速: Redis 数据库完全在内存中，因此处理速度非常快，每秒能执行约 11 万集合，每秒约 81000+条记录。 数据持久化： redis 支持数据持久化，可以将内存中的数据存储到磁盘上，方便在宕机等突发情况下快速恢复。 支持丰富的数据类型: 相比许多其他的键值对存储数据库，Redis 拥有一套较为丰富的数据类型。 数据一致性： 所有 Redis 操作是原子的，这保证了如果两个客户端同时访问的 Redis 服务器将获得更新后的值。 多功能实用工具： Redis 是一个多实用的工具，可以在多个用例如缓存，消息，队列使用(Redis 原生支持发布/订阅)，任何短暂的数据，应用程序，如 Web 应用程序会话，网页命中计数等。 Redis支持数据的备份，即 master-slave 模式的数据备份。 Mysql:8000 读/s 4000 写/s Redis:100000 读写/s 1.4 Redis 与其他 key-value 存储有什么不同 Redis 有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。Redis 的数据类型都是基于基本数据结构的同时对程序员透明，无需进行额外的抽象。 Redis 运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，应为数据量不能大于硬件内存。在内存数据库方面的另一个优点是， 相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样 Redis 可以做很多内部复杂性很强的事情。 同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 第 2 章 Redis on Ubuntu 2.1 组件介绍 redis-benchmark、redis-check-dump、redis-sentinel、redis-check-aof、redis-cli、redis-server 构成了 redis 软件包，下面我们来了解一下它们分别是用来干什么的。    组件  用途     redis-server  Redis 服务器的启动程序。    redis-cli  Redis 命令行操作工具。当然，你也可以用 telnet 根据其纯文本协议来操作。    redis-benchmark  Redis 性能测试工具，测试 Redis 在你的系统及你的配置下的读写性能    redis-stat  Redis 状态检测工具，可以检测 Redis 当前状态参数及延迟状况。   本课程着重讲解 redis 的入门知识，因此我们将只会使用到 redis-server(启动服务)和 redis-cli(连接服务器进行操作)，下面我们来学习如何启动 redis 服务。 ##2.2 启动 redis 启动 redis 服务非常的简单： root@localhost:~# src/redis-server 启动 redis 服务时可以指定很多配置参数，一般情况下，我们会将配置参数写到 config 文件中，在启动 redis 服务时，指定配置文件即可。 redis 软件包中提供了一个默认的配置文件 redis.config，当我们需要指定配置文件启动时，需要按照如下方式启动： root@localhost:~# src/redis-server redis.config redis 服务启动以后，我们可以使用 redis-cli 工具来连接 redis 服务。 root@localhost:~# src/redis-cli 127.0.0.1:6379&gt; 2.3 关闭 终端：Ctrl + C t 或客户端 SHUTDOWN 2.4 配置文件 基本参数 redis 服务相关参数都需要在 redis.config 文件中进行配置，所以我们有必要花点时间来简单了解一下 config 文件中的基本参数。    参数  作用     daemonize  是否以后台 daemon 方式运行 redis 服务    port redis  服务端口，默认 6379    Timeout  请求超时时间    requirepass  连接数据库密码   redis.config 中 daemonize 参数默认为 no，为了让 redis 服务在后台运行，我们需要将 daemonize 参数设置为 yes。 第 3 章 数据类型 3.1 String 字符串 添加键值-SET string 是 redis 中最基础的数据类型， redis 字符串是二进制安全的，这意味着他们有一个已知的长度没有任何特殊字符终止，所以你可以存储任何东西，512 兆为上限。 SET 指令是将字符串值 value 关联到 key 。 语法格式： SET key value [EX seconds] [PX milliseconds] [NX|XX] 示例：添加键 page，值为‘hubwiz’。 redis&gt; SET page hubwiz OK 如果 key 已经持有其他值，SET 就覆写旧值，无视类型。因此，对于某个原本带有生存时间（TTL）的键来说， 当 SET 命令成功在这个键上执行时， 这个键原有的 TTL 将被清除。 添加值和生成时间-SETEX SETEX 指令的作用是将值 value 关联到 key ，并将 key 的生存时间设为 seconds (以秒为单位)。如果 key 已经存在， SETEX 命令将覆写旧值, 语法格式： SETEX key seconds value 示例 - 设置 page 的值为‘hubwiz’，生存时间为 60 秒。 redis&gt; SETEX page 60 hubwiz OK SETEX 指令的作用类似如下两个命令： SET name “hubwiz” EXPIRE key 60 # 设置生存时间 不同之处是，SETEX 是一个原子性(atomic)操作， 关联值 和 设置生存时间 两个动作会在同一时间内完成，该命令在 Redis 用作缓存时，非常实用。 获取字符串-GET GET 指令是返回 key 所关联的字符串值。如果 key 不存在那么返回特殊值 nil 。假如 key 储存的值不是字符串类型，返回一个错误，因为 GET 只能用于处理字符串值，语法格式： GET key 示例 - 获取 page 和 test 的值。 redis&gt; GET page hubwiz redis&gt; GET test (nil) 返回值 当 key 不存在时，返回 nil，否则返回 key 的值。 如果 key 的值不是字符串类型，那么将会返回一个错误。 追加字符串-APPEND 如果 key 已经存在并且是一个字符串，APPEND 命令将 value 追加到 key 原来的值的末尾，语法格式： APPEND key value 示例 - 向 page 追加字符‘.com’。 redis&gt; APPEND page .com # 对已存在的字符串进行 APPEND (integer) 10 redis&gt; GET name hubwiz.com 如果 key 不存在，APPEND 就简单地将给定 key 设为 value ，就像执行 SET key value 一样。 添加多个键值-MSET MSET 指令可以同时设置一个或多个 key-value 对，如果某个给定 key 已经存在，那么 MSET 会用新值覆盖原来的旧值, 语法格式： MSET key value [key value ...] 示例 - 设置 date、time 和 weather 的值。 redis&gt; MSET date 2015.5.10 time 11:00 a.m. weather sunny OK MSET 是一个原子性(atomic)操作，所有给定 key 都会在同一时间内被设置，某些给定 key 被更新而另一些给定 key 没有改变的情况，不可能发生。 获取多个键值-MGET 执行 MGET 指令，将返回所有(一个或多个)给定 key 的值, 语法格式： MGET key [key ...] 示例 - 获取 date、time、weather、year 的值。 &gt;redis&gt; MGET date time weather year 1 ) 2015.5.10 2 ) 11:00 a.m. 3 ) sunny 4 ) (nil) 如果给定的 key 里面，有某个 key 不存在，那么这个 key 返回特殊值 nil 。因此，该命令永不失败。 覆写 SETRANGE 指令是用 value 参数覆写(overwrite)给定 key 所储存的字符串值，从偏移量 offset 开始。不存在的 key 当作空白字符串处理, 语法格式： SETRANGE key offset value 示例 - 覆写 say 的值。 redis&gt; SET say hello world OK   redis&gt; SETRANGE say 6 Redis (integer) 11   redis&gt; GET say hello Redis SETRANGE 命令会确保字符串足够长以便将 value 设置在指定的偏移量上，如果给定 key 原来储存的字符串长度比偏移量小(比如字符串只有 5 个字符长，但你设置的 offset 是 10 )，那么原字符和偏移量之间的空白将用零字节(zerobytes, “  x00” )来填充。 注意你能使用的最大偏移量是 2^29-1(536870911) ，因为 Redis 字符串的大小被限制在 512 兆(megabytes)以内。如果你需要使用比这更大的空间，你可以使用多个 key 。 STRLEN 指令将会返回 key 所储存的字符串值的长度, 语法格式： STRLEN key 示例 - 获取 say 值的长度。  redis&gt; STRLEN say (integer) 11 值得注意的是当 key 储存的不是字符串值时，返回一个错误。 其它指令 |指令 t|用途| |:—:|:—:| |GETSET t| t设置键的字符串值，并返回旧值。 |GETRANGE| t t得到字符串的子字符串存放在一个键。 |GETBIT t| t对 key 所储存的字符串值，获取指定偏移量上的位(bit)。 |SETBIT t t|对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)。 |SETNX t| t将 key 的值设为 value ，当且仅当 key 不存在。 |MSETNX t| t同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。 |PSETEX t| t和 SETEX 命令相似，但它以毫秒为单位设置 key 的生存时间，而不是像 SETEX 命令那样，以秒为单位。 |INCR t| t将 key 中储存的数字值增一。 |INCRBY t| t将 key 所储存的值加上指定增量。 |INCRBYFLOAT t| t为 key 中所储存的值加上指定浮点数增量。 |DECR t| t将 key 中储存的数字值减一。 |DECRBY t| t将 key 所储存的值加上指定增量。 ##3.2 什么是 hash HSET 在 redis 中，使用 HSET 命令来将哈希表 key 中的域 field 的值设为 value ，语法如下： HSET key field value 示例 - 添加键 hubwiz，值为‘hubwiz.com’。 redis&gt; HSET site huwiz hubwiz.com # 设置一个新域 (integer) 1 如果 key 不存在，一个新的哈希表被创建并进行 HSET 操作。 如果域 field 已经存在于哈希表中，旧值将被覆盖。 HMSET 一次可以设置多个 field-value (域-值)对设置到哈希表 key 中, 语法如下： HMSET key field value [field value ...] 示例 - 添加键 www、lab。 redis&gt; HMSET site www www.hubwiz.com lab lab123.hubwiz.com OK 如果 key 不存在，将会创建一个空的哈希表并执行 HMSET 操作。 如果添加的域已存在哈希表中，那么它将被覆盖。 HGET HGET 是用来获取指定 key 值的命令， 语法如下: HGET key field 示例 - 获取域 hubwiz 的值。 redis&gt; HGET site hubwiz www.hubwiz.com 执行 HGET 命令，如果 key 存在，将返回哈希表 key 中给定域 field 的值，如果 key 不存在，则返回 (nil) 。 HMGET 作为 HMSET 命令对应的获取命令，HMGET 可以一次性获取哈希表 key 中，一个或多个给定域的值，基本语法： HMGET key field [field ...] 示例 - 获取域 www、lab、test 的值。 &gt;redis&gt; HMGET site www lab test   # 返回值的顺序和传入参数的顺序一样 1 ) www.hubwiz.com 2 ) lab.hubwiz.com 3 ) (nil)     # 不存在的域返回nil值 如果给定的域不存在于哈希表，那么返回一个 nil 值。 因为不存在的 key 被当作一个空哈希表来处理，所以对一个不存在的 key 进行 HMGET 操作将返回一个只带有 nil 值的表。 获取全部值 HGETALL 基本语法如下： HGETALL key 示例 - 获取 people 全部域的值。 redis&gt; HSET people jack Jack Sparrow (integer) 1 redis&gt; GET name redis&gt; HSET people gump Forrest Gump (integer) 1 redis&gt; HGETALL people 1 ) jack  # 域 2 ) Jack Sparrow # 值 3 ) gump 4 ) Forrest Gump 在返回值里，紧跟每个域名(field name)之后是域的值(value)，所以返回值的长度是哈希表大小的两倍。 HEXISTS 在应用环境中，我们经常会需要知道一个 key 中是否存在某个 field ，HEXISTS 命令可以帮助我们达到这个目的，基本语法： HEXISTS key field 示例 - 验证键 www 是否存在。 redis&gt; HEXISTS site www (integer) 0 查看哈希表 key 中，给定域 field 是否存在。 如果哈希表含有给定域，返回 1 。 如果哈希表不含有给定域，或 key 不存在，返回 0 。 HKEYS 我们经常会遇见这样的应用场景，比如在线用户列表、课堂列表等等，这时候我们可以使用 HKEYS 来获取哈希表 key 中的所有域,基本语法： HKEYS key 示例 - 查看键 people 中所有的域。 redis&gt; HMSET people jack Jack Sparrow gump Forrest Gump OK redis&gt; HKEYS people 1 ) jack 2 ) gump 当 key 存在时，将返回一个包含哈希表中所有域的表。 当 key 不存在时，返回一个空表。 HLEN HLEN 命令将返回哈希表 key 中域的数量,什么时候会用到它呢？比如：在线聊天室，显示在线用户数，基本语法：  HLEN key 示例 - 查看 db 键中域的个数。 redis&gt; HSET db redis redis.com (integer) 1 redis&gt; HSET db mysql mysql.com (integer) 1 redis&gt; HLEN db (integer) 2 redis&gt; HSET db mongodb mongodb.org (integer) 1 redis&gt; HLEN db (integer) 3 当 key 存在时，将返回哈希表中域的数量。 当 key 不存在时，返回 0 。 HDEL 有添加就必定有删除的需求，当我们想要删除哈希表 key 中的一个或多个指定域时，可以使用 HDEL 命令，基本语法： HDEL key field [field …] 示例 - 删除键 people 中的 jack 域。 redis&gt; HDEL people jack (integer) 1 如果是不存在的域，那么它将被忽略掉。 3.3 列表 List LPUSH 命令插入一个新的元素到头部, 而 RPUSH 插入一个新元素到尾部.当一个这两个操作在一个空的 Key 上被执行的时候一个新的列表被创建。 一个列表最多可以包含 232 - 1 个元素 (4294967295, 每个列表超过 40 亿个元素)。 从时间复杂度的角度来看 Redis 列表的主要特征是在头和尾的元素插入和删除是固定时间，即便是数以百万计的插入。. 在列表的两端访问元素是非常快的但是如果你试着访问一个非常大的列表的中间的元素是很慢的，因为那是一个 O(N)操作。 列表操作： 在一个社交网络中建立一个时间线模型，使用 LPUSH 去添加新的元素到用户的时间线， 使用 LRANGE 去接收一些最近插入的元素。 你可以将 LPUSH 和 LTRIM 一起用去创建一个永远也不会超过指定元素数目的列表，但是记住是最后的 N 个元素。 列表能够被用来作为消息传递 primitive。 LPUSH LPUSH 的作用是将一个或多个值 value 插入到列表 key 的表头，基本语法： LPUSH key value [value …] 示例：将 Tony 添加到朋友列表。 redis&gt; LPUSH friends Tony 如果有多个 value 值，那么各个 value 值按从左到右的顺序依次插入到表头,比如说，对空列表 mylist 执行命令 LPUSH mylist a b c ，列表的值将是 c b a 。 如果 key 不存在，一个空列表会被创建并执行 LPUSH 操作。 执行成功时，返回列表长度，当 key 存在但不是列表类型时，返回一个错误。 LSET LSET 可以将列表 key 下标为 index 的元素的值设置为 value ，基本语法：  LSET key index value 示例 redis&gt;LSET friends 0 Lucy ok 需要注意的是，列表 key 必须是已存在的，而且 index 不能超出列表长度范围。 移除 LPOP LPOP 命令执行时会移除列表第一个元素，并将其返回,基本语法： LPOP key 示例 - 取出 friends 中的第一个元素。 redis&gt;LPOP friends Lucy 请注意，LPOP 命令会移除列表中的元素，如果仅仅是想要获取该元素，那么就不应该使用 LPOP 操作，因为 redis 中有专门获取元素的命令。 LINDEX 如果要获取列表元素，LINDEX 命令是比较常用的，使用 LINDEX，我们可以获取到指定位置的 value ，基本语法：  LINDEX key index 示例 - 获取 friends 的第一个元素。 redis&gt;LINDEX friends 0 Tony 下标 (index)为正数时，0 表示第一个元素，1 表示第二个元素，以此类推。 下标 可以是负数，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。 Linsert 插入元素 插入元素是一个必要功能，LINSERT 可以将值 value 插入到列表 key 当中，位于值 pivot 之前或之后，基本语法：     LINSERT key BEFORE  AFTER pivot value   示例 - 将 Andy 插入到 Lucy 之前。 redis&gt; LINSERT friends BEFORE Lucy Andy (integer) 3 当 pivot 不存在于列表 key 时，不执行任何操作。 当 key 不存在时， key 被视为空列表，不执行任何操作。 如果 key 不是列表类型，返回一个错误。 LREM 在 redis 中，移除列表元素使用 LREM 命令，根据参数 count 的值，移除列表中与参数 value 相等的元素，基本语法：  LREM key count value 示例 - 移除 friends 中，所有的名叫‘Tom’的元素。 redis&gt; LREM friends 0 Tom (integer) 1 count 的值可以是以下几种： count &gt; 0 : 从表头开始向表尾搜索，移除与 value 相等的元素，数量为 count 。 count &lt; 0 : 从表尾开始向表头搜索，移除与 value 相等的元素，数量为 count 的绝对值。 count = 0 : 移除表中所有与 value 相等的值。 LLEN 现在 friends 列表中记载着我所有朋友的名字，可是要怎样才能知道我现在拥有多少个朋友呢？ 在 redis 中，LLEN 命令可以获取到列表的长度，基本语法：  LLEN key 示例 - 查看 myList 列表长度。 redis&gt; LLEN mylist (integer) 0 返回列表 key 的长度。 如果 key 不存在，则 key 被解释为一个空列表，返回 0 。 如果 key 不是列表类型，返回一个错误。 LTRIM LTRIM 可以对一个列表进行修剪，就是说，让列表只保留指定区间内的元素，不在指定区间之内的元素都将被删除，基本语法： LTRIM key start stop 示例 - 只保留列表 list 的前三个元素，其余元素全部删除。 LTRIM list 0 2 下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。 你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。 3.4 集合 Set Redis 集合（Set）是一个无序的字符串集合. 你可以快速的完成添加、删除、以及测试元素是否存在。 Redis 集合拥有令人满意的不允许包含相同成员的属性。多次添加相同的元素，最终在集合里只会有一个元素。 实际上说这些就是意味着在添加元素的时候无须检测元素是否存在。 一个 Redis 集合的非常有趣的事情是他支持一些服务端的命令从现有的集合出发去进行集合运算，因此你可以在非常短的时间内进行合并（unions）, 求交集（intersections）,求差集（differences of sets）。 操作： 使用集合追踪一件（独一无二的）事情，比如想要知道所有访问一个博客文章的独立 IP ? 每次当你处理一个页面访问时非常简单，因为可以肯定重复的 IP 是不会被插入的。 Redis 集合是很擅长表现关系的。你可以使用 Redis 集合创建一个 tagging 系统去表现每一个 tag。接下来你能够使用 SADD 命令将有一个给定 tag 的所有对象的所有 ID 添加到一个用来展现这个特定 tag 的集合里。你想要同时有三个不同 tag 的所有对象的 ID 吗？使用 SINTER 就好了。 使用 SPOP 或者 SRANDMEMBER 命令你可以使用集合去随意的抽取元素。 添加 SADD 集合操作中，SADD 命令可以将一个或多个 member 元素加入到集合 key 当中，已经存在于集合的 member 元素将被忽略，基本语法：  SADD key member [member …] 示例 - 添加‘tom’到 room 集合中。 redis&gt; SADD room tom (integer) 1 假如 key 不存在，则创建一个只包含 member 元素作成员的集合。 当 key 不是集合类型时，返回一个错误。 移除 SPOP 如果我们需要随机取出集合中的某个元素，可以使用 SPOP 命令，基本语法：  SPOP key 示例:随机取出 website 集合中的元素。 redis&gt; SPOP website hubwiz.com 需要注意的是，执行 SPOP 命令返回的元素将被移除该集合。 SMEMBERS 如果要获取集合中全部的元素，则需要使用 SMEMBERS 命令，基本语法如下：  SMEMBERS key 示例 - 获取 website 集合中全部的元素。 redis&gt; SMEMBERS website 获取website集合中全部的元素 1 ) www.hubwiz.com 2 ) lab123.hubwiz.com SMEMBERS 命令只会返回集合中的全部成员，并不会移除它们，如果集合不存在，则视为空集合。 SCARD 如果想要查看集合中元素的数量，可以使用 SCARD 命令，基本语法：  SCARD key 示例 - 查看 website 集合中元素的数量。 redis&gt;SCARD website (integer) 3 执行 SCARD 命令，当集合存在时，返回集合中元素的数量，若集合不存在，则返回 0。 SDIFF 假如现在有两个集合，我们想要获取到它们之间不同的元素，通常情况下，我们需要通过循环集合来比较，然后取得不同的元素。在 redis 里面取得集合的差集非常简单，通过 SDIFF 命令即可轻松实现，基本语法：  SDIFF key [key …] 示例 - 取得 mySet1 和 mySet2 的差集。 redis&gt; SMEMBERS mySet1 1 ) bet man 2 ) start war 3 ) 2012 redis&gt; SMEMBERS mySet2 1 ) hi, lady 2 ) Fast Five 3 ) 2012 redis&gt; SDIFF mySet1 mySet2 1 ) bet man 2 ) start war 如果 key 都存在，则返回一个集合的全部成员，该集合是所有给定集合之间的差集。 不存在的 key 被视为空集。 SINTER 在 redis 中获取集合的交集也是非常简单的，执行 SINTER 命令将返回集合的交集，基本语法：  SINTER key [key …] 示例 - 获取集合 group1 和 group2 的交集。 redis&gt; SMEMBERS group1 1 ) LI LEI 2 ) TOM 3 ) JACK redis&gt; SMEMBERS group2 1 ) HAN MEIMEI 2 ) JACK redis&gt; SINTER group1 group2 1 ) JACK 当集合都存在时，将返回一个集合的全部成员，该集合是所有给定集合的交集。 不存在的集合被视为空集。因此，当给定集合当中有一个空集时，结果也为空集(根据集合运算定律)。 SUNION 既然有差集和交集运算，当然少不了并集，在 redis 中，执行 SUNION 命令将返回给定集合的并集，基本语法：  SUNION key [key …] 示例：获取集合 songs 和 my_songs 的并集。 redis&gt; SMEMBERS songs 1 ) Billie Jean redis&gt; SMEMBERS my_songs 1 ) Believe Me redis&gt; SUNION songs my_songs 1 ) Billie Jean 2 ) Believe Me 如果给定的集合都存在，则返回一个集合的全部成员，该集合是所有给定集合的并集。 同样，不存在的集合被视为空集。 SISMEMBER 如果要判断集合是否包含某个元素也不需要循环对比了，因为 redis 提供 SISMEMBER 命令可以实现这个功能，基本语法：  SISMEMBER key member 示例 - 判断 member 元素是否集合 key 的成员。 redis&gt; SMEMBERS website 1 ) hubwiz.com 2 ) google.com 3 ) baidu.com redis&gt; SISMEMBER website hubwiz.com (integer) 1 如果集合包含给定的元素，则返回 1，反之则返回 0。 SMOVE 执行 SMOVE 可以移动元素，基本语法：  SMOVE source destination member 将 member 元素从 source 集合移动到 destination 集合。SMOVE 是原子性操作，因此可以保证数据的一致性。 示例 - 将 songs 集合中的歌曲‘Believe Me’移动到‘my_songs’集合。 redis&gt; SMEMBERS songs 1 ) Billie Jean 2 ) Believe Me redis&gt; SMEMBERS my_songs (empty list or set) redis&gt; SMOVE songs my_songs Believe Me (integer) 1 redis&gt; SMEMBERS songs 1 ) Billie Jean redis&gt; SMEMBERS my_songs 1 ) Believe Me 如果 source 集合不存在或不包含指定的 member 元素，则 SMOVE 命令不执行任何操作，仅返回 0 。否则， member 元素从 source 集合中被移除，并添加到 destination 集合中去。 当 destination 集合已经包含 member 元素时，SMOVE 命令只是简单地将 source 集合中的 member 元素删除。 当 source 或 destination 不是集合类型时，返回一个错误。 SREM 执行命令 SREM 可以将元素从集合中移除，基本语法：  SREM key member [member …] 示例 - 从 languages 集合中移除 ruby。 redis&gt; SMEMBERS languages # 测试数据 1 ) c 2 ) lisp 3 ) python 4 ) ruby redis&gt; SREM languages ruby  # 移除单个元素 (integer) 1 移除集合 key 中的一个或多个 member 元素，不存在的 member 元素会被忽略。 当 key 不是集合类型，返回一个错误。 ##3.5 有序集合 ZSet 有序集合的每个成员都关联了一个评分，这个评分被用来按照从最低分到最高分的方式排序集合中的成员。集合的成员是唯一的，但评分可以重复。 使用有序集合你可以以非常快的速度 添加 、 删除 和 更新 元素。因为元素是有序的, 所以可以很快的根据评分（score）或者次序（position）来获取一个范围的元素。 作用： 在一个大型的在线游戏中展示一个排行榜，在那里一旦一个新的分数被提交，你可以使用 ZADD 命令去更新它.你也可用使用 ZRANGE 命令来得到顶级的用户,你还可以使用 ZRANK 命令根据用户名返回该用户在排行榜中的位次。同时使用 ZRANK 和 ZRANGE 你可以显示和给定用户分数相同的所有用户。所有这些操作都非常的快速。 有序集合常常被用来索引存储在 Redis 中的数据。举个例子，如果你有许多的哈希（Hashes）来代表用户，你可以使用一个有序集合，这个集合中的元素的年龄字段被用来当做评分，而 ID 作为值。因此，使用 ZRANGEBYSCORE 命令，那是微不足道的并且能够很快的接收到给定年龄段的所有用户。 有序集合或许是最高级的 Redis 数据类型，因此花点时间查看完整的有序集合命令列表 去发现你能用 Redis 做些什么。 ZDD 在 redis 中，使用 ZADD 命令将一个或多个 member 元素及其 score 值加入到有序集 key 当中，基本语法： ZADD key score member [[score member] [score member] …] 示例 - 添加 google.com 到 rank 集合，评分 10。 添加单个元素 redis&gt; ZADD rank 10 google.com (integer) 1 如果某个 member 已经是有序集的成员，那么更新这个 member 的 score 值，并通过重新插入这个 member 元素，来保证该 member 在正确的位置上。 score 值可以是整数值或双精度浮点数。 如果 key 不存在，则创建一个空的有序集并执行 ZADD 操作。 当 key 存在但不是有序集类型时，返回一个错误。 第 4 章 安装 redis 的代码遵循 ANSI-C 编写，可以在所有 POSIX 系统（如 Linux, *BSD, Mac OS X, Solaris 等）上安装运行。而且 redis 并不依赖任何非标准库，也没有编译参数必需添加。 4.1 Windows 下安装 https://github.com/ServiceStack/redis-windows 启动 redis-server.exe redis.windows.conf  将 Redis 作为 windows 服务启动 使用命令： redis-server --service-install redis.windows.conf 启动 redis-server --service-start 停止 redis-server --service-stop 安装多个实例 redis-server --service-install –service-name redisService1 –port 10001 redis-server --service-start –service-name redisService1 redis-server --service-install –service-name redisService2 –port 10002 redis-server --service-start –service-name redisService2 redis-server --service-install –service-name redisService3 –port 10003 redis-server --service-start –service-name redisService3 卸载 edis-server --service-uninstall ##4.2 Linux 下安装 ##4.3 Unbuntu 安装 ###1、下载文件 root@localhost:~# wget http://download.redis.io/releases/redis-3.0.0.tar.gz 2、解压 root@localhost:~# tar zxvf redis-3.0.tar.gz 3、编译 root@localhost:~#cd redis-3.0.0 &amp;&amp; make ###4、查看版本  root@localhost:~# src/redis-server -v Redis server v=3.0.0 sha=00000000:0 malloc=jemalloc-3.6.0 bits=32 build=a8a321b3ed54eaaa 第 5 章 可视化工具 redis-desktop-manager：https://redisdesktop.com/download ",
      "url"      : "http://zhangjinmiao.github.io/redis/2019/03/22/Redis%E7%B3%BB%E5%88%97%E5%AD%A6%E4%B9%A0.html",
      "keywords" : "redis"
    } ,
  
    {
      "title"    : "Java 开发环境搭建",
      "category" : "Java",
      "content": "Mac 环境搭建 参考：https://mp.weixin.qq.com/s/JcIVBo6LJw_zyZolZ1MYGQ JDK 安装 参考：  https://www.linuxidc.com/Linux/2016-09/134941.htm  https://zhuanlan.zhihu.com/p/28852767 步骤：  在 usr 下新建文件夹 jdk [root@VM_0_16_centos usr]# mkdir jdk   下载 JDK 并解压 [root@VM_0_16_centos jdk]# tar -zxvf jdk-8u161-linux-x64.tar.gz    设置环境变量 # set java environment JAVA_HOME=/usr/jdk/jdk1.8.0_161 JRE_HOME=/usr/jdk/jdk1.8.0_161/jre CLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin export JAVA_HOME JRE_HOME CLASS_PATH PATH   让其生效： [root@VM_0_16_centos jdk]# source /etc/profile   验证 JDK 有效性 [root@VM_0_16_centos jdk]# java -version java version 1.8.0_161 Java(TM) SE Runtime Environment (build 1.8.0_161-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode) [root@VM_0_16_centos jdk]#  使用 root 安装，其他用户使用不了，需授权 chmod 755 设置用户的权限为： 1.文件所有者可读可写可执行 2.与文件所有者同属一个用户组的其他用户可读可执行 3.其它用户组可读可执行 [root@VM_0_16_centos jdk]# sudo chmod -R 755 /usr/jdk/jdk1.8.0_161/ [root@VM_0_16_centos jdk]# sudo chown -R zhangjm /usr/jdk/jdk1.8.0_161/ Redis 安装 参考： 单机安装：  1. http://www.redis.net.cn/tutorial/3503.html 2. http://www.redis.cn/download.html 3. https://segmentfault.com/a/1190000010709337#articleHeader0 集群安装：  http://bbs.redis.cn/forum.php?mod=viewthread&amp;tid=483  https://segmentfault.com/a/1190000010682551 单机安装 步骤：  在 usr 下新建文件夹 redis [root@VM_0_16_centos usr]# mkdir redis   下载 redis 解压、编译 wget http://download.redis.io/releases/redis-4.0.8.tar.gz $ tar xzf redis-4.0.8.tar.gz $ cd redis-4.0.8 $ make   查看版本 zhangjm@VM_0_16_centos redis-4.0.8]$ ./src/redis-server -v Redis server v=4.0.8 sha=00000000:0 malloc=jemalloc-4.0.3 bits=64 build=50ba0c8cdd44abe3 [zhangjm@VM_0_16_centos redis-4.0.8]$  连接 linux 下的 redis 服务器（如果无法连接一般是防火墙或保护模式的问题，按以下步骤操作可解决  修改 redis.conf 配置文件 在 127.0.0.1 前面加上注释（redis4.0 以下版本默认是注释掉的） 将受保护模式修改为 no（redis4.0 以下的版本没有这个模式配置，不用修改）   在 linux 下的防火墙中开放 6379 端口（与 centos7 以下版本开放端口的方式有区别） [root@localhost bin]# firewall-cmd --zone=public --add-port=6379/tcp --permanent success   若报 FirewallD is not running 参考：https://jingyan.baidu.com/article/5552ef47f509bd518ffbc933.html   重启防火墙 [root@localhost bin]# systemctl restart firewalld   启动 redis [root@localhost bin]# ./redis-server redis.conf   连接测试 脚本使用 脚本文件位于 /usr/redis/redis-4.0.8/utils 下的 redis_init_script 。 复制到 /etc/init.d/ redisd 下并修改为如下： #!/bin/sh # # Simple Redis init.d script conceived to work on Linux systems # as it does use of the /proc filesystem. #端口号，这是默认的，如果你安装的时候不是默认端口号，则需要修改 REDISPORT=6379 #redis-server启动脚本的位置，你如果忘了可以用find或whereis找到 EXEC=/usr/redis/redis-4.0.8/src/redis-server #redis-cli客户端启动脚本的位置，你如果忘了可以用find或whereis找到 CLIEXEC=/usr/redis/redis-4.0.8/src/redis-cli PIDFILE=/var/run/redis_${REDISPORT}.pid #redis.conf配置文件的位置，需在 etc 下新建文件夹 redis, 并将 redis.conf 重命名为 6379.conf 并复制到 /etc/redis 中 CONF=/etc/redis/${REDISPORT}.conf case $1 in  start)  if [ -f $PIDFILE ]  then    echo $PIDFILE exists, process is already running or crashed  else    echo Starting Redis server...    $EXEC $CONF  fi  ;;  stop)  if [ ! -f $PIDFILE ]  then    echo $PIDFILE does not exist, process is not running  else    PID=$(cat $PIDFILE)    echo Stopping ...    $CLIEXEC -p $REDISPORT shutdown    while [ -x /proc/${PID} ]    do    echo Waiting for Redis to shutdown ...    sleep 1    done    echo Redis stopped  fi  ;;  *)  echo Please use start or stop as first argument  ;; esac 设置可执行权限： 设置可执行权限： chmod 755 /etc/init.d/redis 启动测试： $ /etc/init.d/redis start 可设置开机自启动: chkconfig redis on 停止： $ /etc/init.d/redis stop 集群安装 Nginx 安装 Mongodb 安装 Zookeeper 安装 MySQL 安装 ",
      "url"      : "http://zhangjinmiao.github.io/java/2019/03/23/java-development-environment.html",
      "keywords" : "Java, 软件安装, 环境搭建"
    } ,
  
    {
      "title"    : "Maven 介绍",
      "category" : "Maven",
      "content": "1. 依赖配置 groupId,必选，实际隶属项目 (基本坐标) artifactId,必选，其中的模块 (基本坐标) version 必选，版本号 (基本坐标) type 可选，依赖类型，默认 jar scope 可选，依赖范围，默认 compile optional 可选，标记依赖是否可选，默认 false exclusion 可选，排除传递依赖性，默认空 大部分依赖声明只包含基本坐标。 2. 依赖范围 maven 项目有三种 classpath（编译，测试，运行），scope 用来表示与 classpath 的关系，总共有 五 种  compile:编译，测试，运行 test:测试 provided:编译，测试 runtime:运行 system:编译，测试，同 provided，但必须指定 systemPath，慎用 依赖范围有效运行期： Maven 依赖 jar 包冲突解决 1. 判断 jar 是否正确的被引用  在项目启动时加上 VM 参数：-verbose:class 项目启动的时候会把所有加载的 jar 都打印出来 类似如下的信息：  classpath 加载的 jar .... 具体 load 的类 ...   我们可以通过上面的信息查找对应的 jar 是否正确的被依赖，具体类加载情况，同时可以看到版本号，确定是否由于依赖冲突造成的 jar 引用不正确；   通过 maven 自带的工具：‍‍mvn dependency:tree 具体后面可以加 -Dverbose 参数 ，详细参数可以去自己搜，这里不详细介绍。 比如分析如下 POM 运行： mvn dependency:tree -Dverbose   2. 冲突的解决 1）在 pom.xml 中引用的包中加入 exclusion，排除依赖 &lt;dependency&gt;  &lt;groupId&gt;com.alibaba&lt;/groupId&gt;  &lt;artifactId&gt;dubbo&lt;/artifactId&gt;  &lt;version&gt;2.5.3&lt;/version&gt;  &lt;exclusions&gt;   &lt;exclusion&gt;   &lt;artifactId&gt;spring&lt;/artifactId&gt;   &lt;groupId&gt;org.springframework&lt;/groupId&gt;  &lt;/exclusion&gt;  &lt;/exclusions&gt; &lt;/dependency&gt;  去除全部依赖 &lt;dependency&gt;  &lt;groupId&gt;com.alibaba&lt;/groupId&gt;  &lt;artifactId&gt;dubbo&lt;/artifactId&gt;  &lt;version&gt;2.5.3&lt;/version&gt;  &lt;exclusions&gt;   &lt;exclusion&gt;    &lt;artifactId&gt;*&lt;/artifactId&gt;    &lt;groupId&gt;*&lt;/groupId&gt;   &lt;/exclusion&gt;  &lt;/exclusions&gt;  &lt;/dependency&gt; 2）在 ide 中右击进行处理，处理完后在 pom.xml 中也会添加 exclusion 元素 3. 传递性依赖 Maven 传递依赖的关系 4. 可选依赖 有时候我们不想让依赖传递，那么可配置该依赖为可选依赖，将元素 optional 设置为 true 即可,例如： &lt;dependency&gt;  &lt;groupId&gt;commons-logging&lt;/groupId&gt;  &lt;artifactId&gt;commons-logging&lt;/artifactId&gt;  &lt;version&gt;1.2&lt;/version&gt;  &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; 那么依赖该项目的另一项目将不会得到此依赖的传递。 5. 排除依赖 当我们引入第三方 jar 包的时候，难免会引入传递性依赖，有些时候这是好事，然而有些时候我们不需要其中的一些传递性依赖。如当我们引入 spring 包时，我们不想引入传递性依赖 commons-logging，我们可以使用 exclusions元素声明排除依赖，exclusions可以包含一个或者多个exclusion子元素，因此可以排除一个或者多个传递性依赖。 &lt;dependency&gt;  &lt;groupId&gt;org.springframework&lt;/groupId&gt;  &lt;artifactId&gt;spring-core&lt;/artifactId&gt;  &lt;version&gt;4.3.8.RELEASE&lt;/version&gt;  &lt;exclusions&gt;  &lt;exclusion&gt;   &lt;groupId&gt;commons-logging&lt;/groupId&gt;   &lt;artifactId&gt;commons-logging&lt;/artifactId&gt;  &lt;/exclusion&gt;  &lt;/exclusions&gt; &lt;/dependency&gt; 6. 依赖归类 如果我们项目中用到很多关于 Spring Framework 的依赖，它们分别是org.springframework:spring-core:2.5.6, org.springframework:spring-beans:2.5.6,org.springframework:spring-context:2.5.6,它们都是来自同一项目的不同模块。因此，所有这些依赖的版本都是相同的，而且可以预见，如果将来需要升级 Spring Framework，这些依赖的版本会一起升级。因此，我们应该在一个唯一的地方定义版本，并且在 dependency 声明引用这一版本，这一在 Spring Framework 升级的时候只需要修改一处即可。  t&lt;properties&gt;  t t&lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;  t t&lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;  t t&lt;java.version&gt;1.8&lt;/java.version&gt;  t t&lt;springframework.version&gt;4.3.8.RELEASE&lt;/springframework.version&gt;  t&lt;/properties&gt;  t&lt;dependencies&gt;  t t&lt;dependency&gt;  t t t&lt;groupId&gt;org.springframework&lt;/groupId&gt;  t t t&lt;artifactId&gt;spring-core&lt;/artifactId&gt;  t t t&lt;version&gt;${springframework.version}&lt;/version&gt;  t t&lt;/dependency&gt;  t t&lt;dependency&gt;  t t t&lt;groupId&gt;org.springframework&lt;/groupId&gt;  t t t&lt;artifactId&gt;spring-beans&lt;/artifactId&gt;  t t t&lt;version&gt;${springframework.version}&lt;/version&gt;  t t&lt;/dependency&gt; &lt;/dependencies&gt; 7. 管理依赖 mvn dependency:list 表示依赖列表 mvn dependency:tree 表示依赖列表 mvn dependency:analyze 查找出在编译和测试中未使用但显示声明的依赖 Unused declared dependencies found: 8. 仓库 何为 Maven 仓库 在 Maven 世界中，任何一个依赖、插件或者项目构建的输出，都可以称为构件，任何一个构件都有一组坐标唯一标识。 以前传统的开放模式中，每个项目都有 lib 目录，各个项目 lib 目录下的内容存在大量的重复，这样不仅造成磁盘空间的浪费，而且也难 于管理。 Maven 在某个统一的位置存储所有项目的共享的构件，这个统一的位置，我们就称之为仓库。（仓库就是存放依赖和插件的地方） 仓库的布局 任何一个构件都有其唯一的坐标，根据这个坐标可以定义其在仓库中的唯一存储路径，这便是 Maven 的仓库布局方式。 该路径与坐标的大致对应关系为： groupId/artifactId/version/artifactId-version.packaging 仓库的分类 对于 Maven 来说，仓库只分为两类：本地仓库和远程仓库。 当 Maven 根据坐标寻找构件的时候，它首先会查看本地仓库，如果本地仓库存在此构件，则直接使用；如果本地仓库不存在此构件，Maven 就会去远程仓库查找，发现需要的构件，下载到本地仓库再使用。如果本地仓库和远程仓库都没有，Maven 就会报错。 私服是一种特殊的远程仓库，为了节省带宽和时间，应该在局域网内架设一个私有的仓库服务器，用其代理所有外部的远程仓库。内部的项目还能部署到私服上供其他项目使用。 本地仓库 一般来说，在 Maven 项目目录下，没有诸如 lib/ 这样用来存放依赖文件的目录。当 Maven 在执行编译或者测试时，如果需要使用依赖文件，它总是基于坐标使用本地仓库的依赖文件。 默认情况下，本地仓库地址为 C:  Users  用户名  .m2  repository  ，有时候，因为某些原因(C 盘空间不够，系统盘)，需要自定义本地仓库的目录地址，这时，可以编辑 Maven 目录下 settings.xml，设置 localRepository 元素的值为想要的仓库地址。例如：  u000b 一个构件只有在本地仓库中之后，才能由其他 Maven 项目使用。首先必须得以构件并安装到本地仓库中。在项目中执行 mvn clean install 命令，Install 插件的 install 目标将项目的构件输出文件安装到本地仓库。 远程仓库 当用户执行 maven 命令之后，Maven 会根据配置和需要从远程仓库下载构件到本地仓库。 这好比藏书，例如我要读，会先检查自己的书房是否藏了这本书，如果发现没有这本书，于是就跑去书店买一本回来，放在书房里。 本地仓库就好比书房，我需要读书的时候先从书房找，相应的，Maven 需要构件的时候，先从本地仓库找。当无法从本地仓库找到需要的构件的时候，就会从远程仓库下载构件到本地仓库。 私服 私服是一种特殊的远程仓库，它是架设在局域网内的仓库服务，私服代理广域网上的远程仓库，供局域网内的用户使用。当 Maven 需要下载构件的时候，它从私服请求，如果私服上不存在该构件，则从外部远程仓库下载，缓存在私服上之后，再为 Maven 的下载请求提供服务。 私服的好处：  节省自己的外网带宽 加速 Maven 构建 部署自己内部的第三方构件 提高稳定性，增强控制 降低中央仓库的负荷。 镜像 如果仓库 X可以提供仓库Y存储的所有内容，那么就可以认为X是Y的一个镜像。如果配置X的镜像，那么对于仓库Y的任何请求都会转至X仓库。  镜像常见的用法是结合私服，由于私服可以代理任何外部的公共仓库，因此，对于组织内部的 Maven 用户来说，使用一个私服地址就等于使用了所有需要的外部仓库。任何需要的构件都可以从私服获得，私服就是所有仓库的镜像。这时可以配置这样的一个镜像，见下图。  该例中&lt;mirrorOf&gt;的值为星号，表示该配置是所有 Maven 仓库的镜像，任何对于远程仓库的请求都会转至http://172.168.10.100:8081/nexus/content/groups/public/ 如果镜像仓库需要认证，则配置一个ID为nexus的&lt;server&gt;即可。 需要注意：由于镜像仓库完全屏蔽了被镜像的仓库，当镜像仓库不稳定或者停止服务的时候，Maven 将无法访问镜像仓库，因此将无法下载构件。 9. 生命周期 何为生命周期 Maven 的生命周期就是为了对所有的构建过程进行抽象和统一，这个生命周期包含了项目的清理、初始化、编译、测试、打包、集成测试、验证、部署和站点生成等几乎所有的构建步骤。 Maven 的三大生命周期 Maven 有三套相互独立的生命周期，请注意这里说的是”三套”，而且”相互独立”，这三套生命周期分别是：  Clean Lifecycle 在进行真正的构建之前进行一些清理工作。 Default Lifecycle 构建的核心部分，编译，测试，打包，部署等等。 Site Lifecycle 生成项目报告，站点，发布站点。 每个生命周期包含一些阶段(phase)，这些阶段是有顺序的，并且后面的阶段依赖于前面的阶段。以 clean 生命周期为例，它包含的阶段有 pre-clean，clean和post-clean。当用户调用pre-clean的时候，只有pre-clean阶段得以执行。当用户调用clean时候，pre-clean和clean阶段会得以顺序执行。 clean 生命周期 Clean 生命周期的目的是清理项目，它包含三个阶段： pre-clean 执行一些清理前需要准备完成的工作 clean 清理上一次构建生成的文件 post-clean 执行一些清理后需要完成的工作。 default 生命周期 Default 生命周期定义了真正构建时所需要执行的所有步骤，它是所有生命周期中最核心的部分，这里，只解释一些比较重要和常用的阶段： Validate 验证项目是否正确且所有必要的信息都可用。 generate-sources 为包含在编译范围内的代码生成源代码 process-sources 处理源代码，如过滤值。 generate-resources 生成所有需要在打包过程中的资源文件。 process-resources 复制并处理资源文件，至目标目录，准备打包。 compile编译项目的源代码。 process-classes 为编译生成的文件做后期工作。 generate-test-sources 为编译内容生成测试源代码 u000b process-test-sources 处理测试源代码。 generate-test-resources 生成测试需要的资源文件。 process-test-resources复制并处理资源文件，至目标测试目录。 test-compile 编译测试源代码。 process-test-classes 对测试编译生成的文件做后期处理。 test使用合适的单元测试框架运行测试。这些测试代码不会被打包或部署。 prepare-package 在真正打包之前，执行一些打包的必要操作。 package 接受编译好的代码，打包成可发布的格式，如 JAR 。 pre-integration-test 执行一些在集成测试运行之前需要的动作。 integration-test 如果有必要，处理包并发布至集成测试环境。 post-integration-test 执行一些在集成测试运行之后需要的动作。 Verify 执行所有的检查，验证包是否有效，质量是否合格。 install 将包安装至本地仓库，以让其它项目依赖。 deploy将最终的包复制到远程的仓库，以让其它开发人员与项目共享。 site 生命周期 Site 生命周期的目的是建立和发布项目站点，Maven 能够基于 POM 所包含的信息，自动生成一个友好的站点，方便团队交流和发布项目信息。该生命周期包含如下阶段： Pre-site 执行一些在生成项目站点发布到服务器上。 Site 生成项目站点文档。 Post-site 执行一些在生成项目站点之后需要完成的工作。 Site-deploy 将生命的项目 ",
      "url"      : "http://zhangjinmiao.github.io/maven/2019/03/31/Maven-%E4%BB%8B%E7%BB%8D.html",
      "keywords" : "maven, 包管理"
    } ,
  
    {
      "title"    : "Linux 开发中常用搜索技巧",
      "category" : "Linux",
      "content": " 本文转载自： Chris_Mo 在我们的实际开发中，一般应用都部署在 Linux 上，为了后期方便排查 bug 或者记录代码执行的流程。对于开发者而言，遇到问题经常需要去看 log 文件（或者使用 Kibana 这样的工具），这里介绍几个开发常用而又重要的日志查找技巧。 Linux 查看日志的几个常见命令 grep head cat tail less ack sed vi grep  grep (global search regular expression(RE) and print out the line,全面搜索正则表达式并把行打印出来)是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 常见使用方法之: grep 2017010500345878 --color info.log 这行命令在 info.log 中搜索含有”2017010500345878”关键词的段落并且使用其他颜色标记关键词。 优点：根据关键词快速方便定位并且打印出来段落的上下文。 head  head 命令是用来查看具体文件的前面几行的内容,该命令默认是前 10 行内容; 常见使用方法之: head -50 info.log 查看 info.log 文件的前 50 行。 优点：快速定位到文件的前多少行。 tail tail 命令是用来查看具体文件后面几行的内容，默认情况下，是查看该文件尾 10 行的内容;还可以使用 tail 来观察日志文件被更新的过程。使用 -f 选项，tail 会自动实时更新文件内容。 常见使用方法之: tail -f info.log cat cat 命令是 Linux 下的一个文本输出命令，通常是用于观看某个文件的内容的.常用有三大功能:  一次显示整个文件  从键盘创建一个文件  将几个文件合并为一个文件 这里我们只举例显示一个文件. 常见使用方法之: cat -n info.log less less(less) 命令可以对文件或其它输出进行分页显示 常见使用方法之: cat -n info.log ack ack 是一个基于 Perl 的类似于 grep 的命令行工具，但是搜索速度更快，能力比 grep 更强。 常见使用方法之: ack -w order 在当前目录递归搜索单词”eat”,不匹配类似于”orderService”或”paymentOrder”的字符串. sed sed 是一种在线编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（pattern space），接着用 sed 命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有 改变，除非你使用重定向存储输出。Sed 主要用来自动编辑一个或多个文件；简化对文件的反复操作；编写转换程序等。 常见使用方法之: sed -n '800,900' info.log 查看 info.log 文件 800 到 900 行之间的内容 vi 使用找一个字符串，在 vi 命令模式下键入“/”，后面跟要查找的字符串，再按回车。vi 将光标定位在该串下一次出现的地方上。键入 n 跳到该串的下一个出现处，键入 N 跳到该串的上一个出现处。 常见组合使用 使用[grep -n 异常 –color info.log ]查询到异常在文件中发生的行数,然后再看前后几十行日志的内容[sed -n ‘800,900’ info.log]. 总结 这些命令的功能都比较丰富,这里只是列出非常简单的一些用法,在一般的开发中都是频繁使用得到.具体的用法需要开自己总结使用自己喜欢使用的命令.(我这里的总结也会持续更新) ",
      "url"      : "http://zhangjinmiao.github.io/linux/2019/03/31/Linux-%E5%BC%80%E5%8F%91%E4%B8%AD%E5%B8%B8%E7%94%A8%E6%97%A5%E5%BF%97%E6%90%9C%E7%B4%A2%E6%8A%80%E5%B7%A7.html",
      "keywords" : "linux, grep,head,ack"
    } ,
  
    {
      "title"    : "Maven 使用技巧",
      "category" : "Maven",
      "content": "无法获取私服最新Jar包的问题 使用私服上面的jar包，有时候改了代码但版本没变，重新打包的时候获取不到最新jar，可按如下方式处理： 1. 配置 pom 检查快照依赖 找到 pom 的 repositories 配置节点，配置 Snapshots 节点，注意要找到 if_snapshots 这个 url,修改 snapshot 的 enabled 为 true，并设置 updatePolicy 为 alaways &lt;!-- 加载的是 第三方项目使用的jar包 --&gt; &lt;repositories&gt; &lt;repository&gt;  &lt;id&gt;snapshots&lt;/id&gt;  &lt;name&gt;snapshots&lt;/name&gt;  &lt;snapshots&gt;  &lt;enabled&gt;true&lt;/enabled&gt;  &lt;updatePolicy&gt;always&lt;/updatePolicy&gt;&lt;!--强制使用最新jar--&gt;  &lt;/snapshots&gt;  &lt;url&gt;http://nexus.zhenrongbao.com/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; 2. 编译时检查依赖 修改 maven 打包脚本，添加-U 参数，如： mvn clean install package -U -DskipTests=true -U 参数的含义是update-snapshots，保证可以更新快照包。 3. 版本号递增 如果jar包更新支持了某个新的功能，我们也可以递增版本号，避免这样的问题 ",
      "url"      : "http://zhangjinmiao.github.io/maven/2019/04/01/Maven-%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7.html",
      "keywords" : "maven"
    } ,
  
    {
      "title"    : "分布式、微服务、集群",
      "category" : "Distributed",
      "content": "分布式 一个在线购物网站有如下几个功能：商品管理、订单管理、用户管理、支付管理、购物车等等模块，每个模块部署到独立的云服务主机。分布式系统： 分布式锁 分布式锁是控制分布式系统之间同步访问共享资源的一种方式。 为什么使用 Java 高并发的情况下，单机部署处理方式一般使用ReentrantLcok或synchronized进行互斥控制；在分布式系统中，由于多线程、多进程并且程序分部在不同的机器上，使用单机处理的方式已失效，为了解决这个问题就需要一种跨 JVM 的互斥机制来控制共享资源的访问，这就是分布式锁要解决的问题。 三种实现 分布式锁具备的条件  在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行； 高可用的获取锁与释放锁； 高性能的获取锁与释放锁； 具备可重入特性； 具备锁失效机制，防止死锁； 具备阻塞锁特性，即没有获取到锁将继续等待获取锁； 具备非阻塞锁特性，即没有获取到锁将直接返回获取锁失败。 基于数据库的实现 优点 借助数据库，方案简单 缺点 依赖数据库需要一定的资源开销，性能问题需要考虑 基于 Redis 的实现 基于 ZooKeeper 的实现 集群 负载均衡集群 把各个业务功能模块复制部署多份，每个相同功能的模块，构成了一个组，并以单一系统的模式加以管理，组成一个集群组，集群组有自己的调度算法，确保每次访问只调用组中的一个模块。 负载策略 不同的公司，根据自身实力使用不同的负载均衡器：  财大气粗的用硬件 F5 不差钱的使用 DNS 负载均衡 技术牛逼的用 LVS 苦逼的创业型小公司只能使用 Nginx 当然不止这些。 ​ t t t t t t t t七层网络模型图 ​ t t t t t t t tTCP/IP 五层模型 不差钱的国企使用的 F5 工作在 4-7 层，一般互联网企业使用的 LVS 工作在传输层，使用最广泛的 Nginx 工作在应用层。 DNS 负载均衡的控制权在域名服务商手里。 高可用集群 集群的话，就不能够出现单点故障，，“双机热备”，“两地三中心”。 双击热备是高可用的一种体现形式，如上图所示，生产环境中我们存在两个负载均衡节点，主节点处于激活状态，另一个节点处于备用状态，当主节点意外宕机，可以通过 keepalived 检测并迅速切换到备用服务，保障业务正常运转。 两地三中心: 故障转移 服务器统一存储用户状态（基于 Redis 的 Session 共享） 微服务 在分布式系统中，微服务更加强调单一职责、轻量级通信（HTTP）、独立性并且进程隔离。 集群与分布式区别 单台服务器扛不住请求压力时，部署多台，这就是集群，集群主要的使用场景是为了分担请求的压力，也就是在几个服务器上部署相同的应用程序，来分担客户端请求，当压力进一步增大的时候，在数据存储的部分，如 MySQL 无法应对很多写的压力，因为在 mysql 做成集群之后，主要的写压力还是在 master 的机器上面，其他 slave 机器无法分担写压力，从而这个时候，也就引出来分布式。 分布式的使用场景是单台机器无法满足性能需求，必须融合多个节点，节点之间有交互，在写 MySQL 的时候，每个节点存储部分数据，这就是分布式的由来。 再存储一些非结构化数据，例如静态文件，图片，pdf，小视频，这些就是分布式文件系统的由来。 集群和分布式都是由多个节点组成，集群之间的通信协调基本不需要，但分布式各个节点的通信协调必不可少。 集群主要是为了应对请求压力的分担，从而有了 LB，负载均衡集群；为了应对可用性，从而有了 HA，高可用性集群；为了更强的性能，从而有了 HP，高性能集群；为了高并发大规模性能，从而有分布式系统集群。 参考：  三分钟读懂 TT 猫分布式、微服务和集群之路 ",
      "url"      : "http://zhangjinmiao.github.io/distributed/2019/04/03/%E5%88%86%E5%B8%83%E5%BC%8F-%E5%BE%AE%E6%9C%8D%E5%8A%A1-%E9%9B%86%E7%BE%A4.html",
      "keywords" : "分布式,微服务,集群"
    } ,
  
    {
      "title"    : "1. 如何阅读一本书",
      "category" : "Read",
      "content": "第一天总结 1、阅读 真正的阅读是通过独自死磕文字，让理解力逐渐变强而达到自我提升的过程。 2、阅读的四个境界  第一层：基础阅读   第二层：检视阅读：强调时间 1. 有系统的略读或粗读 2. 粗浅的阅读：不停顿，哪怕读到自己看不懂的地方也不要停下来查资料，直接忽略掉，把重点放在自己看得懂的地方。   第三层：分析阅读：吃透书   第四层：主题式阅读或比较阅读：涉及面广 3、好读者的炼成记 主动阅读：在阅读的过程中要提出问题，并自己尝试回答问题。 提出问题 一、整体来看这本书在说什么：寻找主题及丛属的关键议题 二、作者在细节上说了什么：了解作者的特殊观点 三、这本书说的有道理吗？是全部有道理还是部分有道理 四、这本书和你有什么关系  第二天总结 1、天使投资流程 不同项目的投资逻辑不一样，所以投资的第一步是对项目做好分类； 接触项目之后就需要仔细研读商业计划书，首先了解这个项目究竟是在做什么，其次找到该项目的商业模式，并推演该流程是否流畅，最后评价这个项目究竟解决了什么需求，并判断这个需求是否为真实需求； 接着投资人还需要把创业者约出来面谈，关于商业计划书中的核心点，需要确认双方的理解是否一致； 如果项目进行顺利并达成了投资意向，投资人还需要对这个公司进行更深入的调查。 等到投资环节已经完成，还需要不断地与创业者进行密切沟通，确定项目进展。 2、分类阅读规则 第一个规则：你一定要知道自己在读的是哪一类书，而且要越早知道越好，最好在你阅读之前就知道。 第二个规则：使用一个单一的句子，或最多几句话来叙述整本书的内容。 第三个规则：将书中重要的篇章列举出来，并说明他们是如何按照一定的顺序组成一个整体的架构的。 第四个规则：找出作者要问的问题，或者作者想要解决的问题。 第五个规则：找出重要的单字（word），并透过它们与作者达成共识。 第六个规则：将一本书中最重要的句子圈出来，找出其中的主旨。 第七个规则：从相关的关联中，设法架构出一本书的基础论述。 第八个规则：找出作者的解答。 第九个规则：在你说出我同意，我不同意或者我暂缓评论之前，你一定要肯定地说，我了解了。 第十个规则：当你不同意作者的观点时，要理性地表达自己的意见，不要无理地辩驳或争论。就像反对他一样，你也要有同意他的心理准备。 第十一条规则：尊重知识与个人观点的不同，在做任何评判前，都要找到理论依据。 小结  开始阅读之前要先对要阅读的书籍分类。大致分为:理论性作品和实用性作品。 每一本书都有自己构成的框架，我们要找到所阅读书籍中的中心思想、重要篇章、段落、句子甚至经常出现作者强调的词语或单字。 要能用自己的语言根据顺序表达出来，能构画出所阅读内容的框架。 找出作者想要解决的问题及方法。 要能根据自身的生活经验形容一些与主旨相关的内容。 找到论述中所包含的声明，区分不同的论述方法(归纳法和演绎法等)。 找出哪些是假设，哪些是经过证实的，哪些是有根据的，哪些是不需要证实的自明之理。 当你真正了解了这本书，你是否同意作者的关点加以评论。 如果你不同意作者的关点，要理性的表达自己的观点，尊重知识与个人观，并找到自己的依据。(你的知识是否错误，是否完整，是否符合逻辑，是否不够完整等)  第三天总结 1、如何阅读实用型书  分辨一本书是心灵鸡汤还是好的实用类书籍，方法很简单：检查作者是否提出了合理的可执行的，并且效果可检验的方法。任何一本实用型书籍都不能解决该书所关心的实际问题，实际的问题只能靠行动来解决。   关于诗这种艺术作品，像小说和戏剧一样，阅读的第一个规则是要一口气读完，不论你觉得自己懂不懂；第二个规则是重读一遍，并且大声读出来。 2、如何阅读想象文学  不要抗拒想象文学带给你的影响力。 不要去找共识、主旨或论述。 3、如何阅读小说 快速并且全心全意地读。 4、如何阅读史诗和戏剧  认真的阅读计划 运用更多的想象力，要假装看到演出的实景 5、如何阅读抒情诗  一口气读完，不论懂不懂 重读一遍，并且大声读出来  第四天总结 1、如何阅读历史书 读历史书时，抱以怀疑的态度，读历史书的目的： 明白发生过什么 分析这些历史对现在和未来的意义 2、如何阅读传记与自传、新闻和文摘 传记和自传 分类：  定案本：为一个人的 一生作详尽完整的学术性报告，只能是已故的人  授权本：由主人公信任的人来写  自传：自己来写 新闻 要看作者是谁，阅读时要擦亮双眼 文摘 好书：去读原著 一般的书：快速获取资讯 3、如何阅读科学与数学书 要比阅读其他书籍更主动，更全神贯注，最好拿笔做一些简单的笔记；理解基本术语，明白作者思路 4、如何阅读哲学书 阅读的过程中积极主动的思考，学习如何为自己的观点辩护，像哲学家一样论证你的观点。 5、如何阅读社会科学 使用主题阅读的方式，通过阅读大量相关的作品来思考这一问题 ",
      "url"      : "http://zhangjinmiao.github.io/read/2019/04/04/how-to-read-a-book.html",
      "keywords" : "读书, 生活"
    } ,
  
    {
      "title"    : "MySQL 平时积累技巧",
      "category" : "MySQL",
      "content": "mysql 中 long 时间的转换 在开发中，有时候为方便将日期时间以 long 类型(秒钟)存在数据库，这里要查询数据就需要进行转换。 在 mysql 中只要使用from_unixtime函数就可以了。 附 mysql 的两个转换函数：  在 mysql 数据库中，“2018-06-01 00：00：00”转化为列为长整型的函数： select unix_timestamp(2018-06-01 00:00:00)*1000, 这里要注意，mysql 数据库中的长整型，比 java 中的长整型少了秒后面的毫秒数，所以要乘以 1000，这样只有几毫秒之差   在 mysql 数据库中，“1527782400000”（java 中的 long 型数据）转化为日期： select from_unixtime(1527782400); 【注】：要将最后三位去掉。 mysql&gt; select unix_timestamp(2018-06-01 00:00:00); +---------------------------------------+ | unix_timestamp(2018-06-01 00:00:00) | +---------------------------------------+ |     1527782400 | +---------------------------------------+ 1 row in set (0.00 sec) ---------- mysql&gt; select from_unixtime(1527782400); +---------------------------+ | from_unixtime(1527782400) | +---------------------------+ | 2018-06-01 00:00:00  | +---------------------------+ 1 row in set (0.00 sec) too many connections 解决方法 show processlist; 查看连接数，可以发现有很多连接处于sleep状态，这些其实是暂时没有用的，所以可以 kill 掉 show variables like max_connections; 查看最大连接数，应该是与上面查询到的连接数相同，才会出现too many connections的情况 set GLOBAL max_connections=1000; 修改最大连接数，但是这不是一劳永逸的方法，应该要让它自动杀死那些 sleep 的进程。 show global variables like 'wait_timeout'; 这个数值指的是 mysql 在关闭一个非交互的连接之前要等待的秒数，默认是 28800s set global wait_timeout=300; 修改这个数值，这里可以随意，最好控制在几分钟内 set global interactive_timeout=500; 修改这个数值，表示 mysql 在关闭一个连接之前要等待的秒数，至此可以让 mysql 自动关闭那些没用的连接，但要注意的是，正在使用的连接到了时间也会被关闭，因此这个时间值要合适。 解决办法 批量 kill 之前没用的 sleep 连接  先把要 kill 的连接 id 都查询出来  select concat('KILL ',id,';') from information_schema.processlist where user='root';  复制中间的kill id;内容到 word 文档       替换掉符号“   ”和回车符（在 word 中查询 ^p 即可查询到回车符）     把修改过的内容复制回终端，最后按回车执行！ ",
      "url"      : "http://zhangjinmiao.github.io/mysql/2019/04/22/MySQL%E5%B9%B3%E6%97%B6%E7%A7%AF%E7%B4%AF%E6%8A%80%E5%B7%A7.html",
      "keywords" : "MySQL, 技巧"
    } ,
  
    {
      "title"    : "Spring Boot 系列学习",
      "category" : "springboot",
      "content": "基础原理 核心 运行原理 Web 应用系列 web 开发之 web 配置 web 开发之 Thymeleaf web 开发之 JSP 统一异常处理 应用监控系列 Spring Boot-Actuator 文档生成 API 文档生成 数据缓存系列 https://www.zybuluo.com/javazjm/note/932803 Spring Boot-EhCache Spring Boot-Redis Cache Spring Boot-Guava Cache 消息队列系列（MQ） Spring Boot-RabbitMQ Spring Boot-RocketMQ Spring Boot-Kafka 实用技术系列 Spring Boot-Quartz Spring Boot-Email Spring Boot-Shiro Spring Boot-异步方法 Spring Boot-Dubbo 整合 附录： 常用配置：  英文  中文  ",
      "url"      : "http://zhangjinmiao.github.io/springboot/2019/07/01/SpringBoot%E7%B3%BB%E5%88%97%E5%AD%A6%E4%B9%A0.html",
      "keywords" : "spring,springboot,微服务"
    } ,
  
    {
      "title"    : "JAVA 系列进阶指南",
      "category" : "Java",
      "content": "JAVA 基础  《Java 核心技术 36 讲》 JAVA 进阶 Spring  玩转 Spring 全家桶 MySQL  《MySQL 实战 45 讲》 MySQL 是怎样运行的：从根儿上理解 MySQL Redis  Redis 深度历险：核心原理与应用实践 Linux  《Linux 性能优化实战》 Java 程序员眼中的 Linux Nginx  《Nginx 核心知识 100 讲》 JAVA 高级 JVM  从 0 开始带你成为 JVM 实战高手 《深入拆解 Java 虚拟机》 性能优化  Java 性能调优实战 并发  Java 并发编程实战 数据结构  《数据结构与算法之美》 算法  十大经典排序算法 力扣 例子 Tomcat  深入拆解 Tomcat &amp; Jetty Kafka  Kafka 入门与实践 《Kafka 核心技术与实战》 源码阅读  参考知识星球-芋道源码&amp;&amp;架构 Spring RabbitMQ  基于 SpringBoot &amp; RabbitMQ 完成 DirectExchange 分布式消息消费 RabbitMQ 高级指南——AMQP 解析(上) RabbitMQ 高级指南——AMQP 解析(下) 资深 架构设计  《从 0 开始学架构》 《微服务架构实战 160 讲》 分布式 架构设计 实战(周末) 秒杀  《秒杀案例》 ——先看 《如何设计一个秒杀系统》 《Java 秒杀系统方案优化 高性能高并发实战-本地》进行中 架构  蚂蚁视频-本地 电商  《spring 团购案例-本地》 《十次方-本地》 优惠券 支付-交易  《龙果支付案例》或《XxPay 案例》 任务调度  《XXL-JOB》   《Elastic-Job》 spring boot  spring boot 原理分析 《尚硅谷 spring boot》、《千峰 spring boot》 每特教育 SpringBoot 2.0 [SpringBoot 微信点餐系统] [SpringBoot 博客企业前后端] 纯洁的微笑 gitchat springboot 例子 例子  整理一份 spring boot 脚手架项目 ♥️自建项目  参考 hope-boot,framework 等项目，在 GitHub 新建属于自己的项目，整合案例，搭建一套通用代码框架。  hope-boot  framework  jeecg-boot  pig 模块划分    模块  释义  进度     fast-admin  后台管理模块（controller）       fast-core  核心业务模块（按业务划分）       fast-framework  框架模块，提供基础配置，工具类等       fast-sso-server  单点登录-认证中心模块，使用 xxl-sso       fast-generator  代码生成模块，使用 renren-generator       fast-quartz  定时任务模块       fast-flyway  数据库版本管理工具模块                 项目结构 fast-boot |——fast-admin  t|—— |——fast-core  t|——modules  t t|—— |——fast-framework |——fast-sso-server |——fast-generator |——fast-quartz |——fast-flyway 工作相关  account-server ：功能包括 账户系统，优惠券系统，优化代码结构，完善项目。 guardian：离线任务系统，功能包括对账文件生成，监控报警，定时任务管理，去掉 service dao 层外部依赖。 spring cloud  Spring Cloud 底层原理 《尚硅谷 spring cloud》 [Spring Cloud 天气项目] 纯洁的微笑 方志朋 wangkang80 例子  整理一份 spring cloud 脚手架项目 面试  Java 工程师进阶知识完全扫盲♥️♥️ JavaGuide ♥️♥️ CS-Notes♥️ GO  Go语言从入门到实战 Go语言核心36讲 Python  系列学习+实战 Python 100 天 小柒哥 例子 我的例子 Spring Boot Spring Cloud 算法 JAVA Python ",
      "url"      : "http://zhangjinmiao.github.io/java/2019/07/20/todo.html",
      "keywords" : "系列进阶,面试"
    } ,
  
    {
      "title"    : "Java 8 教程",
      "category" : "Java8",
      "content": "本文是一个 Java 8 使用的系列文章，翻译自 mkyong 的 Java 8 Tutorials 教程。 目录： Lambda 表达式示例 forEach 示例 Streams filter 示例 Streams map() 示例 使用 Streams 进行集合分组 过滤一个来自流的空值 将数据流转换为列表 如何将 Array 转换为 Stream Stream 已经被操作或关闭 对 Map 排序 将 list 转为 map 对 map 过滤 Java 8 flatMap 示例 将 map 转为 list ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/07/27/Java-8-Tutorials.html",
      "keywords" : "Java 8, 系列"
    } ,
  
    {
      "title"    : "线上问题排查技巧",
      "category" : "Java",
      "content": "诡异问题 线程执行一个任务迟迟没有返回，应用假死。 接口响应缓慢，甚至请求超时。 CPU 高负载运行。 定位问题 1. 查看所有 java 进程 使用命令 jps -v 或 ps aux|grep java 找到应用的 pid。 2. 输出 dump 文件 使用命令 jstack 1523 &gt; 1523.log ,其中 1523 为进程号。如果应用程序简单，日志不大的话可直接查看，但是若应用程序复杂，导出来的日志较大的话，建议使用专门的分析工具。 分析工具：  在线工具   heaphero   IBM Memory Analyzer   Eclipse Memory Analysis(MAT)   下载地址：http://www.eclipse.org/mat/downloads.php  使用技巧：  1.双击报错解决办法： 右键mat显示包内容，进入Contents-&gt;MacOS下面，会有一个MemoryAnalyzer的命令。  打开终端，进入此路径找到MemoryAnalyzer，运行  ./MemoryAnalyzer -data dump文件所在文件夹路径  即可启动成功  2. 问题： 使用mac版的时候，默认配置的最大内存是1g，当hprof文件过大时，一打开就提示内存溢出：java.lang.OutOfMemoryError: Java heap space  解决办法： 找到MemoryAnalyzer的安装目录，如果不知道在哪，可以在打开着的MAT里，点击Help-&gt;About Eclipse Memory Analyzer-&gt;左下角Installation Details-&gt;Configuration选项卡，找到luncher，对应的值就是该程序的路径。进入该文件的父目录的父目录同一层有个Eclipse目录，进去有个配置文件MemoryAnalyzer.ini，修改里边的Xms为：-Xmx4g，即改为最大4g内存。重启MAT即可。   建议  尽量不要在线程中做大量耗时的网络操作，如查询数据库（可以的话在一开始就将数据从从 DB 中查出准备好）。   尽可能的减少多线程竞争锁。可以将数据分段，各个线程分别读取。   多利用 CAS+自旋 的方式更新数据，减少锁的使用。   应用中加上 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp 参数，在内存溢出时至少可以拿到内存日志。   线程池监控。如线程池大小、队列大小、最大线程数等数据，可提前做好预估。   JVM 监控，可以看到堆内存的涨幅趋势，GC 曲线等数据，也可以提前做好准备。  参考：  一次线上问题排查所引发的思考  ",
      "url"      : "http://zhangjinmiao.github.io/java/2019/07/28/online-troubleshoot.html",
      "keywords" : "问题排查, 内存溢出"
    } ,
  
    {
      "title"    : "Spring Boot 最佳实践",
      "category" : "springboot",
      "content": "使用自动配置 Spring Boot 的一个主要特性是它使用了自动配置。 这是 Spring Boot 中使代码简单工作的部分。 当在类路径上检测到特定的 jar 文件时，它将被激活。 使用它的最简单的方法是依赖于 Spring Boot Starters。 所以，如果你想和 Redis 互动，你可以从以下内容开始: &lt;dependency&gt;  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;  &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; 如果你想使用 MongoDB，你需要: &lt;dependency&gt;  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;  &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt; &lt;/dependency&gt; 等等… … 通过依赖这些启动器，您依赖于经过测试和验证的配置，这些配置将在一起工作得很好。 这有助于避免可怕的 Jar Hell。 可以使用以下注释属性从 Auto-configuration 中排除某些类: @EnableAutoConfiguration（exclude = {ClassNotToAutoconfigure.class}） 但只有在绝对必要的情况下才应该这样做。 关于自动配置的官方文档可以在这里找到。 使用 Spring Initializr 启动新的 Spring Boot 项目 Spring Initializr (Spring 初始化 https://start.Spring.io/ )为您提供了一个非常简单的方法来启动一个新的 Spring Boot 项目，并加载您可能需要的依赖项。 使用 Initializr 创建应用程序可以确保您正在选择经过测试和批准的依赖项，这些依赖项可以很好地与 Spring 自动配置一起工作。 你甚至可能发现一些你不知道存在的新的集成。 考虑为常见的组织问题创建您自己的自动配置 如果您在一个严重依赖 Spring Boot 的组织中工作，并且您有需要解决的共同问题，那么您可以创建自己的自动配置。 这个任务比较复杂，所以你需要考虑什么时候投资是值得的。 单个自动配置比多个定制配置更容易维护，所有配置都略有不同。 如果您要将库发布到开放源码，那么提供一个 Spring Boot 配置将极大地方便成千上万用户的采用。 正确地组织代码结构 虽然允许您有很多自由，但是有一些基本的规则值得遵循，然后列出您的源代码。  避免使用缺省包。 确保所有的东西(包括你的入口点)都包含在一个命名良好的包中。 这样，您将避免有关装配和组件扫描相关的意外情况 保持你的 Application.java 在顶级目录中 我建议将控制器和服务放在面向功能的模块中，但这是可选的。 一些非常好的开发人员建议把所有的控制器放在一起。 不论怎样，坚持一种风格！ 让你的“控制器”保持干净和专注 Controller 应该是很简单的。 您可以在这里阅读 Controller pattern explained as part of GRASP。 您希望控制器进行协调和委派，而不是执行实际的业务逻辑。 以下是一些关键的做法:  控制器应该是无状态的！ 控制器是默认的单例模式，给它们任何状态都会导致严重的问题 控制器不应该执行业务逻辑，而是依赖于委托 控制器应该处理应用程序的 HTTP 层， 这不应该传递给服务 控制器应该围绕一个用例 / 业务能力来设计 要深入了解这里的内容，可以开始讨论设计 REST APIs 的最佳实践。 不管你是否想使用 Spring Boot，这些都是值得学习的。 围绕业务能力建立你的服务 Services 是 Spring Boot 的另一个核心概念。 我发现最好是围绕业务功能 / 域 / 用例（随便你怎么称呼它）构建服务。 使用诸如 AccountService、 UserService、 PaymentService 等服务的应用程序比使用 DatabaseService、 ValidationService、 CalculationService 等服务的应用程序更容易处理。 您可以决定使用Controler和Service之间的一对一映射， 那将是理想的情况。但这并不意味着Service之间不能互相利用！ 让你的数据库成为一个细节——从核心逻辑中抽象出来 我曾经不确定如何最好地处理 Spring Boot 中的数据库交互。 在阅读了罗伯特 · 马丁(robertc.Martin)的Clean Architecture 之后，我更加清楚了。 您希望将数据库逻辑从服务中抽象出来。 理想情况下，您不希望服务知道它正在与哪个数据库通信。 使用一些抽象来封装对象的持久性。  Robert c. Martin 充满激情地主张将您的数据库变成一个“细节”。 这意味着不将应用程序耦合到特定的数据库。 过去很少有人会转换数据库。 我注意到，随着 Spring Boot 和现代 microservices 的开发，事情变得更快了。 保持您的业务逻辑与 Spring Boot 代码无关 考虑到“ Clear Architecture”的经验教训，您还应该保护您的业务逻辑。 这是非常诱人的组合各种春天启动代码在那里… 不要这样做。 如果您能够抵制诱惑，您将保持业务逻辑的可重用性。 部分服务成为库是很常见的。 如果不从代码中删除大量 Spring 注释，则更容易创建。 赞成构造函数注入 这个来自 Phil Webb (现任 Spring Boot 的领导者,@phillip Webb)。 使您的业务逻辑免受Spring Boot代码侵入的一种方法是依赖构造函数注入。 @autowired 注释不仅在构造函数上是可选的，而且您还可以轻松地在没有 Spring 的情况下实例化 bean。 熟悉并发模型 写过的最受欢迎的文章之一是 “Introduction to Concurrency in Spring Boot“。 我认为这是因为这个领域经常被误解和忽视。 随之而来的就是各种问题。 在 Spring 中，Controller 和 Service 是默认是单例的。 如果不小心的话，可能会导致并发性问题。 您通常还要处理有限的线程池。 熟悉这些概念。 如果您正在使用 Spring Boot 应用程序的新 WebFlux 样式，我已经解释了它的工作原理，在 “Spring’s WebFlux / Reactor Parallelism and Backpressure“。 加强配置管理的外部化 这一点超越了 Spring Boot，尽管这是人们开始创建多个类似服务时常见的问题..。 您可以手动配置 Spring 应用程序。 如果你正在处理几十个 Spring Boot 应用程序，你需要成熟你的 Spring 组态管理。 我推荐两种主要方法:  使用配置服务，比如 Spring Cloud Config。 将所有配置存储在环境变量中(可以基于 git 存储库提供) 这两个选项（第二个选项多一些）都要求您在 DevOps 领域有所建树，但这在微服务领域是意料之中的。 提供全局异常处理 您确实需要一种处理异常的一致方法。 Spring Boot 提供了两种主要的实现方式:  你应该使用 HandlerExceptionResolver用于定义全局异常处理策略 你也可以在控制器上添加 @exceptionhandler，如果你想在某些情况下更加具体，这可能会很有用 这与 Spring 非常相似，Baeldung 有一篇关于 用 Spring 处理 REST 错误 的详细文章，非常值得一读。 使用日志框架 您可能已经意识到了这一点，但是您应该使用 Logger 记录日志，而不是使用 System.out.println ()手动完成日志记录。 这在没有任何配置的 Spring Boot 中很容易实现。 只需获得该类的日志记录器实例: Logger logger = LoggerFactory.getLogger(MyClass.class); 这很重要，因为它允许您根据需要设置不同的日志级别。 测试你的代码 这不是特定于 Spring Boot 的，但是值得注意！ 测试你的代码。 如果您没有编写测试，那么您从一开始就在编写遗留代码。 如果其他人来到你的代码库，很快，改变任何东西都可能变得危险。 如果有多个服务相互依赖，那么风险就更大了。 因为有 Spring Boot 的最佳实践，所以您应该考虑在消费者驱动的契约中使用 Spring Cloud Contract。 它将使您与其他服务的集成更容易使用。 使用测试片使您的测试更容易和更集中 使用 Spring Boot 测试代码可能很棘手——您需要初始化数据层、连接众多服务、模拟事物… … 实际上并不需要那么困难！ 答案是-使用测试片。 使用测试片，您可以根据需要只连接应用程序的部分。 这可以为您节省很多时间，并确保您的测试不与您没有使用的东西耦合。 有一篇名为 Custom test slice with Spring Boot 1.4 的博客文章解释了这种技术。 总结 多亏了 Spring Boot，编写基于 Spring 的微服务变得比以往任何时候都容易。 我希望通过这些最佳实践，您的实现过程不仅会很快，而且从长远来看会更加健壮和成功。 祝你好运！  翻译自：  Spring Boot – Best Practices ",
      "url"      : "http://zhangjinmiao.github.io/springboot/2019/07/30/SpringBoot%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.html",
      "keywords" : "springboot, 翻译"
    } ,
  
    {
      "title"    : "Java 8 Lambda 表达式比较器使用",
      "category" : "Java8",
      "content": "引言 在这个例子中，我们将向您展示如何使用 java8 lambda 表达式编写一个 Comparator 来对 List 进行排序。  经典的比较器示例： Comparator&lt;Developer&gt; byName = new Comparator&lt;Developer&gt;() { @Override public int compare(Developer o1, Developer o2) {  return o1.getName().compareTo(o2.getName()); } };   使用 lambda： Comparator&lt;Developer&gt; byName =  (Developer o1, Developer o2)-&gt;o1.getName().compareTo(o2.getName());   1.没有 Lambda 的排序 先新建一个 Developer 类，然后比较 Developer 对象的年龄，通常我们使用 Collections.sort 并传递匿名 Comparator 类，如下所示: package com.jimzhang.lambda; import java.math.BigDecimal; import java.util.ArrayList; import java.util.Collections; import java.util.Comparator; import java.util.List; /** * 〈一句话功能简述〉&lt;br&gt; 〈排序〉 * * @author zhangjinmiao * @create 2019/8/4 10:19 */ public class TestSorting { public static void main(String[] args) {  List&lt;Developer&gt; listDevs = getDevelopers(); System.out.println(Before Sort);  for (Developer developer : listDevs) {  System.out.println(developer.toString());  } //sort by age  Collections.sort(listDevs, new Comparator&lt;Developer&gt;() {  @Override  public int compare(Developer o1, Developer o2) {  return o1.getAge() - o2.getAge();  }  }); System.out.println(After Sort);  for (Developer developer : listDevs) {  System.out.println(developer);  } } public static List&lt;Developer&gt; getDevelopers() {  List&lt;Developer&gt; developers = new ArrayList&lt;&gt;();  developers.add(new Developer(lisi, new BigDecimal(8000),23));  developers.add(new Developer(wangwu, new BigDecimal(9000),24));  developers.add(new Developer(maliu, new BigDecimal(10000),25));  developers.add(new Developer(zhangsan, new BigDecimal(7000),22));  return developers; } } 当排序要求改变时，您只需传入另一个新的匿名 Comparator 类: //sort by age  tCollections.sort(listDevs, new Comparator&lt;Developer&gt;() {  t t@Override  t tpublic int compare(Developer o1, Developer o2) {  t t treturn o1.getAge() - o2.getAge();  t t}  t});  t  t//sort by name t  tCollections.sort(listDevs, new Comparator&lt;Developer&gt;() {  t t@Override  t tpublic int compare(Developer o1, Developer o2) {  t t treturn o1.getName().compareTo(o2.getName());  t t}  t});  t t t t  t//sort by salary  tCollections.sort(listDevs, new Comparator&lt;Developer&gt;() {  t t@Override  t tpublic int compare(Developer o1, Developer o2) {  t t treturn o1.getSalary().compareTo(o2.getSalary());  t t}  t}); t t 这是可行的，但是，您是否觉得仅仅因为想要更改一行代码就创建一个类有点奇怪？ 2.使用 Lambda 排序 在 Java 8 中，List 接口直接支持排序方法，不再需要使用 Collections.sort。 //List.sort() since Java 8  tlistDevs.sort(new Comparator&lt;Developer&gt;() {  t t@Override  t tpublic int compare(Developer o1, Developer o2) {  t t treturn o2.getAge() - o1.getAge();  t t}  t}); t 3.更多例子 1. 按年龄排序 //sort by age  tCollections.sort(listDevs, new Comparator&lt;Developer&gt;() {  t t@Override  t tpublic int compare(Developer o1, Developer o2) {  t t treturn o1.getAge() - o2.getAge();  t t}  t});  t  t//lambda  tlistDevs.sort((Developer o1, Developer o2)-&gt;o1.getAge()-o2.getAge());  t  t//lambda, valid, parameter type is optional  tlistDevs.sort((o1, o2)-&gt;o1.getAge()-o2.getAge()); 2. 按名字排序  t//sort by name  tCollections.sort(listDevs, new Comparator&lt;Developer&gt;() {  t t@Override  t tpublic int compare(Developer o1, Developer o2) {  t t treturn o1.getName().compareTo(o2.getName());  t t}  t});  t t  t//lambda  tlistDevs.sort((Developer o1, Developer o2)-&gt;o1.getName().compareTo(o2.getName())); t t  t  t//lambda  tlistDevs.sort((o1, o2)-&gt;o1.getName().compareTo(o2.getName())); t t 3. 按薪水排序  t//sort by salary  tCollections.sort(listDevs, new Comparator&lt;Developer&gt;() {  t t@Override  t tpublic int compare(Developer o1, Developer o2) {  t t treturn o1.getSalary().compareTo(o2.getSalary());  t t}  t}); t t t t  t//lambda  tlistDevs.sort((Developer o1, Developer o2)-&gt;o1.getSalary().compareTo(o2.getSalary()));  t  t//lambda  tlistDevs.sort((o1, o2)-&gt;o1.getSalary().compareTo(o2.getSalary())); 4. 反向排序 薪水正序排序 Comparator&lt;Developer&gt; salaryComparator = (o1, o2)-&gt;o1.getSalary().compareTo(o2.getSalary());  tlistDevs.sort(salaryComparator); 输出： Developer{name='zhangsan', salary=7000, age=22} Developer{name='lisi', salary=8000, age=23} Developer{name='wangwu', salary=9000, age=24} Developer{name='maliu', salary=10000, age=25} 2.反向排序 Comparator&lt;Developer&gt; salaryComparator = (o1, o2)-&gt;o1.getSalary().compareTo(o2.getSalary());  tlistDevs.sort(salaryComparator.reversed()); 输出： Developer{name='maliu', salary=10000, age=25} Developer{name='wangwu', salary=9000, age=24} Developer{name='lisi', salary=8000, age=23} Developer{name='zhangsan', salary=7000, age=22} 源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/01/Java-8-Lambda-%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%AF%94%E8%BE%83%E5%99%A8%E4%BD%BF%E7%94%A8.html",
      "keywords" : "Java 8, 系列, lambda, sort"
    } ,
  
    {
      "title"    : "Java8 forEach 使用",
      "category" : "Java8",
      "content": "引言 在本文中，我们将向您展示如何使用新的 java 8 foreach 语句循环 List 和 Map。 1. forEach and Map  普通方式遍历 Map  tMap&lt;String, Integer&gt; items = new HashMap&lt;&gt;();  titems.put(A, 10);  titems.put(B, 20);  titems.put(C, 30);  titems.put(D, 40);  titems.put(E, 50);  titems.put(F, 60);  tfor (Map.Entry&lt;String, Integer&gt; entry : items.entrySet()) {  t tSystem.out.println(Item : + entry.getKey() + Count : + entry.getValue());  t}  在 java8 中，可以使用 forEach + lambda 表达式循环 Map。  tMap&lt;String, Integer&gt; items = new HashMap&lt;&gt;();  titems.put(A, 10);  titems.put(B, 20);  titems.put(C, 30);  titems.put(D, 40);  titems.put(E, 50);  titems.put(F, 60);  t  titems.forEach((k,v)-&gt;System.out.println(Item : + k + Count : + v));  t  titems.forEach((k,v)-&gt;{  t tSystem.out.println(Item : + k + Count : + v);  t tif(E.equals(k)){  t t tSystem.out.println(Hello E);  t t}  t}); 2. forEach and List  普通方式遍历 List List&lt;String&gt; items = new ArrayList&lt;&gt;();  titems.add(A);  titems.add(B);  titems.add(C);  titems.add(D);  titems.add(E);  tfor(String item : items){  t tSystem.out.println(item);  t} 2. 在 java8 中，可以使用 forEach + lambda 表达式或方法引用循环 List。  tList&lt;String&gt; items = new ArrayList&lt;&gt;();  titems.add(A);  titems.add(B);  titems.add(C);  titems.add(D);  titems.add(E);  t//lambda  t//Output : A,B,C,D,E  titems.forEach(item-&gt;System.out.println(item));  t t  t//Output : C  titems.forEach(item-&gt;{  t tif(C.equals(item)){  t t tSystem.out.println(item);  t t}  t});  t t  t//method reference  t//Output : A,B,C,D,E  titems.forEach(System.out::println);  t  t//Stream and filter  t//Output : B  titems.stream()  t t.filter(s-&gt;s.contains(B))  t t.forEach(System.out::println); 源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/02/Java8-forEach-%E4%BD%BF%E7%94%A8.html",
      "keywords" : "Java 8, 系列, forEach"
    } ,
  
    {
      "title"    : "Java8 Streams filter 使用",
      "category" : "Java8",
      "content": "引言 在本教程中，我们将向您展示几个 java8 示例，以演示 Streams filter ()、 collect ()、 findAny ()和 orElse ()的使用。 什么是流 Stream（流）是一个来自数据源的元素队列并支持聚合操作  元素 是特定类型的对象，形成一个队列。 Java 中的 Stream 并不会存储元素，而是按需计算。 数据源 流的来源。 可以是集合，数组，I/O channel， 产生器 generator 等。 聚合操作 类似 SQL 语句一样的操作， 比如 filter, map, reduce, find, match, sorted 等。 和以前的 Collection 操作不同， Stream 操作还有两个基础的特征：  Pipelining: 中间操作都会返回流对象本身。 这样多个操作可以串联成一个管道， 如同流式风格（fluent style）。 这样做可以对操作进行优化， 比如延迟执行(laziness)和短路( short-circuiting)。 内部迭代： 以前对集合遍历都是通过 Iterator 或者 For-Each 的方式, 显式的在集合外部进行迭代， 这叫做外部迭代。 Stream 提供了内部迭代的方式， 通过访问者模式(Visitor)实现。 生成流 在 Java 8 中, 集合接口有两个方法来生成流：  stream() − 为集合创建串行流。   parallelStream() − 为集合创建并行流。 1. Streams filter() and collect() 1.java 8 之前过滤 list 使用如下方式： List&lt;String&gt; lines = Arrays.asList(spring, node, php); List&lt;String&gt; result = getFilterOutput(lines, php);  for (String temp : result) {  System.out.println(temp); //output : spring, node  } } private static List&lt;String&gt; getFilterOutput(List&lt;String&gt; lines, String filter) {  List&lt;String&gt; result = new ArrayList&lt;&gt;();  for (String line : lines) {  if (!php.equals(line)) { // we dont like php  result.add(line);  }  }  return result; } 2.java 8 之后，使用 stream.filter() 过滤 list，使用 collect() 将流转为 list. List&lt;String&gt; lines = Arrays.asList(spring, node, php); List&lt;String&gt; result = lines.stream()  // convert list to stream  .filter(line -&gt; !php.equals(line)) // we dont like php  .collect(Collectors.toList());  // collect the output and convert streams to a List result.forEach(System.out::println);  //output : spring, node 2. Streams filter(), findAny() and orElse() java8 之前，获取 name 通过如下方式： public static void main(String[] args) {  List&lt;Developer&gt; persons = Arrays.asList(  new Developer(zhangsan, 20),  new Developer(lisi,21),  new Developer(wangwu,22)); Developer developer = getByNameBefore(persons,lisi);  System.out.println(developer == null ? 不存在 : developer.toString()); } private static Developer getByNameBefore(List&lt;Developer&gt; persons, String lisi) {  Developer result = null;  for (Developer developer : persons) {  if (developer.getName().equals(lisi)) {  result = developer;  }  } return result; } java8 之后： public static void main(String[] args) {  List&lt;Developer&gt; persons = Arrays.asList(  new Developer(zhangsan, 20),  new Developer(lisi,21),  new Developer(wangwu,22)); Developer developer = persons.stream().filter(x -&gt; lisi.equals(x.getName())).findAny()  .orElse(null);  System.out.println(developer); // 不存在的  Developer developer1 = persons.stream().filter(x -&gt; asan.equals(x.getName())).findAny()  .orElse(null);  System.out.println(developer1); } 多条件查询使用： public static void main(String[] args) {  List&lt;Developer&gt; persons = Arrays.asList(  new Developer(zhangsan, 20),  new Developer(lisi,21),  new Developer(wangwu,22)); Developer result1 = persons.stream()  .filter(p -&gt; lisi.equals(p.getName()) &amp;&amp; p.getAge() == 21).findAny().orElse(null); System.out.println(result1); Developer result2 = persons.stream().filter(p -&gt; {  if (lisi.equals(p.getName()) &amp;&amp; p.getAge() == 21) {  return true;  }  return false;  }).findAny().orElse(null); System.out.println(result2); } 3. Streams filter() and map() public static void main(String[] args) {  List&lt;Developer&gt; persons = Arrays.asList(  new Developer(zhangsan, 20),  new Developer(lisi,21),  new Developer(wangwu,22)); String name = persons.stream().filter(x -&gt; lisi.equals(x.getName()))  .map(Developer::getName)  //convert stream to String  .findAny().orElse(); System.out.println(name : + name); List&lt;String&gt; collect = persons.stream().filter(x -&gt; lisi.equals(x.getName()))  .map(Developer::getName).collect(Collectors.toList()); collect.forEach(System.out::println); } 源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/03/Java-8-Streams-filter-%E7%A4%BA%E4%BE%8B.html",
      "keywords" : "Java 8, 系列, forEach, streams filter"
    } ,
  
    {
      "title"    : "Java8 Streams map 使用",
      "category" : "Java8",
      "content": "引言 在 Java 8 中，stream (). Map ()允许您将一个对象转换为其他对象。查看下面例子： 1. 将 List 中的字符串转为大写 public static void main(String[] args) {  List&lt;String&gt; alpha = Arrays.asList(a, b, c, d); //Before Java8  List&lt;String&gt; alphaUpper = new ArrayList&lt;&gt;();  for (String s : alpha) {  alphaUpper.add(s.toUpperCase());  } System.out.println(alpha); //[a, b, c, d]  System.out.println(alphaUpper); //[A, B, C, D] // Java 8  List&lt;String&gt; collect = alpha.stream().map(String::toUpperCase).collect(Collectors.toList());  System.out.println(collect); //[A, B, C, D] // Extra, streams apply to any data type.  List&lt;Integer&gt; num = Arrays.asList(1,2,3,4,5);  List&lt;Integer&gt; collect1 = num.stream().map(n -&gt; n * 2).collect(Collectors.toList());  System.out.println(collect1); //[2, 4, 6, 8, 10] } 2. 将 List 中的对象转为字符串 public class Developer {  private String name; private BigDecimal salary; private Integer age; //... } public static void main(String[] args) {  List&lt;Developer&gt; persons = Arrays.asList(  new Developer(zhangsan, 20),  new Developer(lisi,21),  new Developer(wangwu,22)); //Before Java 8  List&lt;String&gt; result = new ArrayList&lt;&gt;();  for (Developer developer : persons) {  result.add(developer.getName());  } System.out.println(result); // [zhangsan, lisi, wangwu] //Java 8  List&lt;String&gt; collect = persons.stream().map(x -&gt; x.getName()).collect(Collectors.toList());  System.out.println(collect); // [zhangsan, lisi, wangwu] } 3. 将 List 中的对象转为另一个对象 public class Person { private String name; private int age; private String extra; //... } Java 8 之前： List&lt;Developer&gt; developers = Arrays.asList(  new Developer(zhangsan, 20),  new Developer(lisi,21),  new Developer(wangwu,22)); List&lt;Person&gt; result = new ArrayList&lt;&gt;(); for (Developer developer : developers) {  Person person = new Person();  person.setName(developer.getName());  person.setAge(developer.getAge());  if (lisi.equals(developer.getName())) {  person.setExtra(i am lisi);  }  result.add(person);  } System.out.println(JSONUtil.toJsonStr(result)); } java 8 List&lt;Developer&gt; developers = Arrays.asList(  new Developer(zhangsan, 20),  new Developer(lisi,21),  new Developer(wangwu,22)); List&lt;Person&gt; result = developers.stream().map(temp -&gt; {  Person person = new Person();  person.setName(temp.getName());  person.setAge(temp.getAge());  if (lisi.equals(temp.getName())) {  person.setExtra(i am lisi);  }  return person;  }).collect(Collectors.toList()); System.out.println(JSONUtil.toJsonStr(result));   源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/04/Java-8-Streams-map-%E4%BD%BF%E7%94%A8.html",
      "keywords" : "Java 8, 系列, streams map"
    } ,
  
    {
      "title"    : "Java8 Streams Collectors 使用",
      "category" : "Java8",
      "content": "引言 在本文中，我们将向您展示如何使用 java8 流的 Collectors 对列表进行分组、计数、求和和排序。 1. 分组、计数和排序  按列表分组并显示列表的总数。 List&lt;String&gt; items = Arrays.asList(apple, apple, banana,  apple, orange, banana, papaya); Map&lt;String, Long&gt; result = items.stream()  .collect(Collectors.groupingBy(Function.identity(), Collectors.counting())); System.out.println(result); 输出： {  tpapaya=1, orange=1, banana=2, apple=3 } 添加排序 List&lt;String&gt; items = Arrays.asList(apple, apple, banana,  apple, orange, banana, papaya); Map&lt;String, Long&gt; result = items.stream()  .collect(Collectors.groupingBy(Function.identity(), Collectors.counting())); Map&lt;String, Long&gt; finalMap = new LinkedHashMap&lt;&gt;(); result.entrySet().stream()  .sorted(Entry.&lt;String,Long&gt;comparingByValue().reversed()).forEachOrdered(e-&gt;finalMap.put(e.getKey(),e.getValue())); System.out.println(finalMap); 输出： {  tapple=3, banana=2, papaya=1, orange=1 } 2.列出对象 按用户定义的对象列表进行“分组”的示例。  按名称分组，并统计数量或求和。 public class Item { private String name;  private int qty;  private BigDecimal price; //constructors, getter/setters } List&lt;Item&gt; items = Arrays.asList(  new Item(apple, 10, new BigDecimal(9.99)),  new Item(banana, 20, new BigDecimal(19.99)),  new Item(orang, 10, new BigDecimal(29.99)),  new Item(watermelon, 10, new BigDecimal(29.99)),  new Item(papaya, 20, new BigDecimal(9.99)),  new Item(apple, 10, new BigDecimal(9.99)),  new Item(banana, 10, new BigDecimal(19.99)),  new Item(apple, 20, new BigDecimal(9.99))  ); Map&lt;String, Long&gt; couting = items.stream()  .collect(Collectors.groupingBy(Item::getName, Collectors.counting())); System.out.println(couting); System.out.println(======); Map&lt;String, Integer&gt; sum = items.stream()  .collect(Collectors.groupingBy(Item::getName, Collectors.summingInt(Item::getQty))); System.out.println(sum); 输出： {papaya=1, banana=2, apple=3, orang=1, watermelon=1} ====== {papaya=20, banana=30, apple=40, orang=10, watermelon=10} 按价格分组，Collectors.groupingBy and Collectors.mapping 的使用： List&lt;Item&gt; items = Arrays.asList(  new Item(apple, 10, new BigDecimal(9.99)),  new Item(banana, 20, new BigDecimal(19.99)),  new Item(orang, 10, new BigDecimal(29.99)),  new Item(watermelon, 10, new BigDecimal(29.99)),  new Item(papaya, 20, new BigDecimal(9.99)),  new Item(apple, 10, new BigDecimal(9.99)),  new Item(banana, 10, new BigDecimal(19.99)),  new Item(apple, 20, new BigDecimal(9.99))  ); System.out.println(=====&gt;group by price:);  // group by price  Map&lt;BigDecimal, List&lt;Item&gt;&gt; groupByPriceMap = items.stream()  .collect(Collectors.groupingBy(Item::getPrice)); System.out.println(groupByPriceMap); Map&lt;BigDecimal, Set&lt;String&gt;&gt; collect = items.stream().collect(Collectors  .groupingBy(Item::getPrice, Collectors.mapping(Item::getName, Collectors.toSet()))); System.out.println(=====&gt;group by + mapping to Set:);  System.out.println(collect); 输出： =====&gt;group by price: { 19.99=[  Item{name='banana', qty=20, price=19.99},  Item{name='banana', qty=10, price=19.99}  ], 29.99=[  Item{name='orang', qty=10, price=29.99},  Item{name='watermelon', qty=10, price=29.99}  ], 9.99=[  Item{name='apple', qty=10, price=9.99},  Item{name='papaya', qty=20, price=9.99},  Item{name='apple', qty=10, price=9.99},  Item{name='apple', qty=20, price=9.99}  ] } =====&gt;group by + mapping to Set: { 19.99=[banana], 29.99=[orang, watermelon], 9.99=[papaya, apple] } 源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/05/Java-8-Stream-Collectors-%E5%88%86%E7%BB%84-%E7%BB%9F%E8%AE%A1%E7%AD%89%E6%93%8D%E4%BD%9C.html",
      "keywords" : "Java 8, 系列, streams Collectors,group by, count, sum"
    } ,
  
    {
      "title"    : "Java8 Streams 过滤 null",
      "category" : "Java8",
      "content": "引言 本文展示如何过滤一个空值的流对象。  检查包含空值的流。 public static void main(String[] args) { Stream&lt;String&gt; language = Stream.of(java, python, node, null, ruby, null, php); List&lt;String&gt; result = language.collect(Collectors.toList()); result.forEach(System.out::println); } 输出： java python node null // &lt;--- NULL ruby null // &lt;--- NULL php 使用 Stream.filter (x-x! null) public static void main(String[] args) { Stream&lt;String&gt; language = Stream.of(java, python, node, null, ruby, null, php); //List&lt;String&gt; result = language.collect(Collectors.toList()); List&lt;String&gt; result = language.filter(x -&gt; x!=null).collect(Collectors.toList()); // 或使用 Objects: : nonNull 进行筛选  List&lt;String&gt; result = language.filter(Objects::nonNull).collect(Collectors.toList());  result.forEach(System.out::println);  } 输出： java python node ruby php 源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/06/Java-8-Stream-%E8%BF%87%E6%BB%A4%E7%A9%BA%E5%80%BC.html",
      "keywords" : "Java 8, 系列, streams, filter"
    } ,
  
    {
      "title"    : "Java8 将数据流转换为列表",
      "category" : "Java8",
      "content": "引言 示例演示如何通过 Collectors.toList 将数据流转换为 List。  这个在前面也多次出现过 public static void main(String[] args) { Stream&lt;String&gt; language = Stream.of(java, python, node); //Convert a Stream to List  List&lt;String&gt; list = language.collect(Collectors.toList());  list.forEach(System.out::println); } 输出： java python node 过滤数字 3 并将其转换为 List Stream&lt;Integer&gt; number = Stream.of(1, 2, 3, 4, 5);  List&lt;Integer&gt; collect = number.filter(x -&gt; x != 3).collect(Collectors.toList());  collect.forEach(System.out::println); 输出： 1 2 4 5 源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/07/Java-8-%E5%B0%86%E6%95%B0%E6%8D%AE%E6%B5%81%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%88%97%E8%A1%A8.html",
      "keywords" : "Java 8, 系列, streams convert"
    } ,
  
    {
      "title"    : "Java8 如何将 Array 转换为 Stream",
      "category" : "Java8",
      "content": "引言 在 java8 中，您可以使用 Arrays.Stream 或 Stream.of 将 Array 转换为 Stream。 1. 对象数组 对于对象数组，Arrays.stream 和 Stream.of 都返回相同的输出。 public static void main(String[] args) { ObjectArrays(); } private static void ObjectArrays() {  String[] array = {a, b, c, d, e};  //Arrays.stream  Stream&lt;String&gt; stream = Arrays.stream(array);  stream.forEach(x-&gt; System.out.println(x)); System.out.println(======); //Stream.of  Stream&lt;String&gt; stream1 = Stream.of(array);  stream1.forEach(x-&gt; System.out.println(x)); } 输出： a b c d e ====== a b c d e 查看 JDK 源码，对于对象数组，Stream.of 内部调用了 Arrays.stream 方法。 // Arrays public static &lt;T&gt; Stream&lt;T&gt; stream(T[] array) {  return stream(array, 0, array.length); } // Stream public static&lt;T&gt; Stream&lt;T&gt; of(T... values) {  return Arrays.stream(values); } 2. 基本数组 对于基本数组，Arrays.stream 和 Stream.of 将返回不同的输出。 public static void main(String[] args) { PrimitiveArrays(); } private static void PrimitiveArrays() {  int[] intArray = {1, 2, 3, 4, 5}; // 1. Arrays.stream -&gt; IntStream  IntStream stream = Arrays.stream(intArray);  stream.forEach(x-&gt;System.out.println(x)); System.out.println(======); // 2. Stream.of -&gt; Stream&lt;int[]&gt;  Stream&lt;int[]&gt; temp = Stream.of(intArray); // 不能直接输出，需要先转换为 IntStream  IntStream intStream = temp.flatMapToInt(x -&gt; Arrays.stream(x));  intStream.forEach(x-&gt; System.out.println(x)); } 输出： 1 2 3 4 5 ====== 1 2 3 4 5 查看源码， // Arrays public static IntStream stream(int[] array) {  return stream(array, 0, array.length); } // Stream public static&lt;T&gt; Stream&lt;T&gt; of(T t) {  return StreamSupport.stream(new Streams.StreamBuilderImpl&lt;&gt;(t), false); } Which one 对于对象数组，两者都调用相同的 Arrays.stream 方法 对于基本数组，我更喜欢 Arrays.stream，因为它返回固定的大小 IntStream，更容易操作。 所以，推荐使用 Arrays.stream，不需要考虑是对象数组还是基本数组，直接返回对应的流对象，操作方便。  源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/08/Java-8-%E5%A6%82%E4%BD%95%E5%B0%86-Array-%E8%BD%AC%E6%8D%A2%E4%B8%BA-Stream.html",
      "keywords" : "Java 8, 系列, array convert stream"
    } ,
  
    {
      "title"    : "Java8 Stream 已经被操作或关闭",
      "category" : "Java8",
      "content": "引言 在 java8 中，Stream 不能被重用，一旦它被使用或使用，流将被关闭。 1. 流关闭 查看下面的示例，它将抛出一个 IllegalStateException，表示“ stream is closed”。 public static void main(String[] args) {  String[] array = {a, b, c, d, e};  Stream&lt;String&gt; stream = Arrays.stream(array); // loop a stream  stream.forEach(x-&gt; System.out.println(x)); // 拒绝过滤，抛出 throws IllegalStateException  long count = stream.filter(x -&gt; b.equals(x)).count();  System.out.println(count); } 输出： a b c d e Exception in thread main java.lang.IllegalStateException: stream has already been operated upon or closed  tat java.util.stream.AbstractPipeline.&lt;init&gt;(AbstractPipeline.java:203)  tat java.util.stream.ReferencePipeline.&lt;init&gt;(ReferencePipeline.java:94)  tat java.util.stream.ReferencePipeline$StatelessOp.&lt;init&gt;(ReferencePipeline.java:618)  tat java.util.stream.ReferencePipeline$2.&lt;init&gt;(ReferencePipeline.java:163)  tat java.util.stream.ReferencePipeline.filter(ReferencePipeline.java:162)  tat com.jimzhang.stream.ClosedTest.main(ClosedTest.java:22) 2. 重用流 不管出于什么原因，你真的想重用一个数据流，试试下面的 Supplier 解决方案: public static void main(String[] args) { String[] array = {a, b, c, d, e};  Supplier&lt;Stream&lt;String&gt;&gt; streamSupplier = () -&gt; Stream.of(array); //get new stream  streamSupplier.get().forEach(x-&gt; System.out.println(x)); //get another new stream long count = streamSupplier.get().filter(x -&gt; b.equals(x)).count();  System.out.println(count); } 输出： a b c d e 1 每个 get ()将返回一个新的流。  源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/09/Java-8-Stream-%E5%B7%B2%E7%BB%8F%E8%A2%AB%E6%93%8D%E4%BD%9C%E6%88%96%E5%85%B3%E9%97%AD.html",
      "keywords" : "Java 8, 系列, stream closed,open"
    } ,
  
    {
      "title"    : "Java8 对 Map 排序",
      "category" : "Java8",
      "content": "引言 使用 keys 或 values 对 map 排序。 1. 快速开始 步骤： 将 map 转为流 对流排序 收集并返回一个新的 LinkedHashMap (保持顺序) Map result = map.entrySet().stream()  t.sorted(Map.Entry.comparingByKey())  t t t  t.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue,  t(oldValue, newValue) -&gt; oldValue, LinkedHashMap::new));  默认情况下，Collectors.toMap 将返回一个 HashMap。 2. 按 Keys 排序 public static void main(String[] args) {  Map&lt;String, Integer&gt; unsortMap = new HashMap&lt;&gt;();  unsortMap.put(z, 10);  unsortMap.put(b, 5);  unsortMap.put(a, 6);  unsortMap.put(c, 20);  unsortMap.put(d, 1);  unsortMap.put(e, 7);  unsortMap.put(y, 8);  unsortMap.put(n, 99);  unsortMap.put(g, 50);  unsortMap.put(m, 2);  unsortMap.put(f, 9); System.out.println(Original...);  System.out.println(unsortMap); // sort by keys, a,b,c..., and return a new LinkedHashMap  // toMap() will returns HashMap by default, we need LinkedHashMap to keep the order.  LinkedHashMap&lt;String, Integer&gt; result = unsortMap.entrySet().stream()  .sorted(Entry.comparingByKey()).collect(Collectors   .toMap(Entry::getKey, Entry::getValue, (oldValue, newValue) -&gt; oldValue,    LinkedHashMap::new)); // 不推荐，但是很有效。 Map&lt;String, Integer&gt; result2 = new LinkedHashMap&lt;&gt;();  unsortMap.entrySet().stream().sorted(Map.Entry.comparingByKey())  .forEachOrdered(x-&gt;result2.put(x.getKey(),x.getValue())); System.out.println(Sorted...);  System.out.println(result);  System.out.println(result2); } 输出： Original... {a=6, b=5, c=20, d=1, e=7, f=9, g=50, y=8, z=10, m=2, n=99} Sorted... {a=6, b=5, c=20, d=1, e=7, f=9, g=50, m=2, n=99, y=8, z=10} {a=6, b=5, c=20, d=1, e=7, f=9, g=50, m=2, n=99, y=8, z=10} 3. 按 Values 排序 public static void main(String[] args) {  Map&lt;String, Integer&gt; unsortMap = new HashMap&lt;&gt;();  unsortMap.put(z, 10);  unsortMap.put(b, 5);  unsortMap.put(a, 6);  unsortMap.put(c, 20);  unsortMap.put(d, 1);  unsortMap.put(e, 7);  unsortMap.put(y, 8);  unsortMap.put(n, 99);  unsortMap.put(g, 50);  unsortMap.put(m, 2);  unsortMap.put(f, 9); System.out.println(Original...);  System.out.println(unsortMap); //sort by values, and reserve it, 10,9,8,7,6...  LinkedHashMap&lt;String, Integer&gt; result = unsortMap.entrySet().stream()  .sorted(Entry.comparingByValue(Comparator.reverseOrder()))  .collect(Collectors.toMap(Entry::getKey, Entry::getValue, (oldValue, newValue) -&gt; oldValue,    LinkedHashMap::new)); // 替代方式  Map&lt;String, Integer&gt; result2 = new LinkedHashMap&lt;&gt;();  unsortMap.entrySet().stream()  .sorted(Map.Entry.comparingByValue(Comparator.reverseOrder()))  .forEachOrdered(x -&gt; result2.put(x.getKey(), x.getValue())); System.out.println(Sorted...);  System.out.println(result);  System.out.println(result2); } 输出： Original... {a=6, b=5, c=20, d=1, e=7, f=9, g=50, y=8, z=10, m=2, n=99} Sorted... {n=99, g=50, c=20, z=10, f=9, y=8, e=7, a=6, b=5, m=2, d=1} {n=99, g=50, c=20, z=10, f=9, y=8, e=7, a=6, b=5, m=2, d=1} 4. Map&lt;Object,Object&gt; Stream 不能直接对 Map&lt;Object,Object&gt; 进行排序，可以现将它转为 Map&lt;String,String&gt;，再操作： public static void main(String[] args) {  Properties properties = System.getProperties();  Set&lt;Entry&lt;Object, Object&gt;&gt; entries = properties.entrySet(); LinkedHashMap&lt;String, String&gt; collect = entries.stream()  .collect(Collectors.toMap(k -&gt; (String) k.getKey(), e -&gt; (String) e.getValue()))  .entrySet()  .stream().sorted(Entry.comparingByKey())  .collect(Collectors   .toMap(Entry::getKey, Entry::getValue, (oldValue, newValue) -&gt; oldValue,    LinkedHashMap::new)); collect.forEach((k,v)-&gt;System.out.println(k + : + v)); } 输出： awt.toolkit:sun.lwawt.macosx.LWCToolkit file.encoding:UTF-8 file.encoding.pkg:sun.io file.separator:/ ftp.nonProxyHosts:local|*.local|169.254/16|*.169.254/16 gopherProxySet:false http.nonProxyHosts:local|*.local|169.254/16|*.169.254/16 java.awt.graphicsenv:sun.awt.CGraphicsEnvironment java.awt.printerjob:sun.lwawt.macosx.CPrinterJob ..... sun.boot.library.path:/Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk/Contents/Home/jre/lib sun.cpu.endian:little sun.cpu.isalist: sun.io.unicode.encoding:UnicodeBig sun.java.command:com.jimzhang.map.sort.ObjectTest sun.java.launcher:SUN_STANDARD sun.jnu.encoding:UTF-8 sun.management.compiler:HotSpot 64-Bit Tiered Compilers sun.os.patch.level:unknown user.country:CN user.dir:/Users/zhangjinmiao/Documents/GitHub/java-8-demo user.home:/Users/zhangjinmiao user.language:zh user.name:zhangjinmiao user.timezone: 源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/10/Java-8-%E5%AF%B9-Map-%E6%8E%92%E5%BA%8F.html",
      "keywords" : "Java 8, 系列, sort a map"
    } ,
  
    {
      "title"    : "Java 8 将 list 转为 map",
      "category" : "Java8",
      "content": "引言 创建一个对象类 public class Hosting { private int Id;  private String name;  private long websites; public Hosting(int id, String name, long websites) {  Id = id;  this.name = name;  this.websites = websites;  } //getters, setters and toString() } 1.List to Map – Collectors.toMap() 创建 Hosting 对象的列表，并使用 Collectors.toMap 将其转换为 Map。 public static void testOne(){  List&lt;Hosting&gt; list = new ArrayList&lt;&gt;();  list.add(new Hosting(1, liquidweb.com, 80000));  list.add(new Hosting(2, linode.com, 90000));  list.add(new Hosting(3, digitalocean.com, 120000));  list.add(new Hosting(4, aws.amazon.com, 200000));  list.add(new Hosting(5, mkyong.com, 1)); // key = id, value - websites  Map&lt;Integer, String&gt; result1 = list.stream()  .collect(Collectors.toMap(Hosting::getId, Hosting::getName)); System.out.println(result1： + result1); // key = name, value - websites  Map&lt;String, Long&gt; result2 = list.stream()  .collect(Collectors.toMap(Hosting::getName, Hosting::getWebsites)); System.out.println(result2： + result2); // key = id, value = name 另一种写法  Map&lt;Integer, String&gt; result3 = list.stream()  .collect(Collectors.toMap(x -&gt; x.getId(), x -&gt; x.getName())); System.out.println(result3： + result3); } 2.List to Map – Duplicated Key 重复的 key 抛出异常。 private static void testTwo() {  List&lt;Hosting&gt; list = new ArrayList&lt;&gt;();  list.add(new Hosting(1, liquidweb.com, 80000));  list.add(new Hosting(2, linode.com, 90000));  list.add(new Hosting(3, digitalocean.com, 120000));  list.add(new Hosting(4, aws.amazon.com, 200000));  list.add(new Hosting(5, mkyong.com, 1)); list.add(new Hosting(6, linode.com, 100000)); // new line Map&lt;String, Long&gt; result1 = list.stream()  .collect(Collectors.toMap(Hosting::getName, Hosting::getWebsites)); System.out.println(result1： + result1); } 输出——下面的错误消息有点误导人，它应该显示“ linode”而不是键的值。 Exception in thread main java.lang.IllegalStateException: Duplicate key 90000  tat java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133)  tat java.util.HashMap.merge(HashMap.java:1245)  t//... 要解决上面重复的关键问题，传入第三个 mergeFunction 参数，如下所示: private static void testTwo() { List&lt;Hosting&gt; list = new ArrayList&lt;&gt;();  list.add(new Hosting(1, liquidweb.com, 80000));  list.add(new Hosting(2, linode.com, 90000));  list.add(new Hosting(3, digitalocean.com, 120000));  list.add(new Hosting(4, aws.amazon.com, 200000));  list.add(new Hosting(5, mkyong.com, 1)); list.add(new Hosting(6, linode.com, 100000)); // new line // Map&lt;String, Long&gt; result1 = list.stream() //  .collect(Collectors.toMap(Hosting::getName, Hosting::getWebsites)); Map&lt;String, Long&gt; result1 = list.stream().collect(  Collectors.toMap(Hosting::getName, Hosting::getWebsites, (oldValue, newValue) -&gt; oldValue)); System.out.println(result1： + result1); } 输出： result1：{liquidweb.com=80000, mkyong.com=1, digitalocean.com=120000, aws.amazon.com=200000, linode.com=90000} 使用新值： Map&lt;String, Long&gt; result1 = list.stream().collect(    Collectors.toMap(Hosting::getName, Hosting::getWebsites,     (oldValue, newValue) -&gt; newValue    )  ); 输出： result1：{liquidweb.com=80000, mkyong.com=1, digitalocean.com=120000, aws.amazon.com=200000, linode.com=100000} 3.List to Map – Sort &amp; Collect 先排序再收集。 private static void testThree() {  List&lt;Hosting&gt; list = new ArrayList&lt;&gt;();  list.add(new Hosting(1, liquidweb.com, 80000));  list.add(new Hosting(2, linode.com, 90000));  list.add(new Hosting(3, digitalocean.com, 120000));  list.add(new Hosting(4, aws.amazon.com, 200000));  list.add(new Hosting(5, mkyong.com, 1));  list.add(new Hosting(6, linode.com, 100000)); // use oldValue  Map&lt;String, Long&gt; result1 = list.stream().sorted(Comparator.comparingLong(Hosting::getWebsites).reversed())  .collect(Collectors.toMap(Hosting::getName,Hosting::getWebsites,(oldValue, newValue) -&gt; oldValue,   LinkedHashMap::new)); System.out.println(result1： + result1); } 输出： result1：{aws.amazon.com=200000, digitalocean.com=120000, linode.com=100000, liquidweb.com=80000, mkyong.com=1} 在上面的例子中，流是在收集之前排序的，所以“ linode. com 100000”变成了“ oldValue”。  源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/11/Java-8-%E5%B0%86-list-%E8%BD%AC%E4%B8%BA-map.html",
      "keywords" : "Java 8, 系列, list, map"
    } ,
  
    {
      "title"    : "Java 8 对 map 过滤",
      "category" : "Java8",
      "content": "引言 几个 Java 示例向您展示如何使用 java8 流 API 过滤 Map。 Java 8 之前： private static void beforeJava8() {  Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;();  map.put(1, linode.com);  map.put(2, heroku.com); String result = ;  for (Map.Entry&lt;Integer, String&gt; entry : map.entrySet()) {  if(something.equals(entry.getValue())){  result = entry.getValue();  }  } } 使用 java8，您可以将 Map.entrySet ()转换为流，然后使用 filter ()和 collect ()。 private static void java8() {  Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;();  map.put(1, linode.com);  map.put(2, heroku.com); //Map -&gt; Stream -&gt; Filter -&gt; String  String result = map.entrySet().stream().filter(x -&gt; something.equals(x.getValue()))  .map(x -&gt; x.getValue()).collect(Collectors.joining()); //Map -&gt; Stream -&gt; Filter -&gt; MAP  Map&lt;Integer, String&gt; collect = map.entrySet().stream().filter(x -&gt; x.getKey() == 2)  .collect(Collectors.toMap(x -&gt; x.getKey(), x -&gt; x.getValue())); // or like this  Map&lt;Integer, String&gt; collect1 = map.entrySet().stream().filter(x -&gt; x.getKey() == 3)  .collect(Collectors.toMap(Entry::getKey, Entry::getValue)); } 1. java8- 过滤一个 Map 通过值筛选 Map 并返回 String 的完整示例。 private static void one() {  Map&lt;Integer, String&gt; HOSTING = new HashMap&lt;&gt;();  HOSTING.put(1, linode.com);  HOSTING.put(2, heroku.com);  HOSTING.put(3, digitalocean.com);  HOSTING.put(4, aws.amazon.com); // Before Java 8  String result = ;  for (Map.Entry&lt;Integer, String&gt; entry : HOSTING.entrySet()) {  if (aws.amazon.com.equals(entry.getValue())) {  result = entry.getValue();  }  }  System.out.println(Before Java 8 : + result);  //Map -&gt; Stream -&gt; Filter -&gt; String  result = HOSTING.entrySet().stream()  .filter(map -&gt; aws.amazon.com.equals(map.getValue()))  .map(map -&gt; map.getValue())  .collect(Collectors.joining()); System.out.println(With Java 8 : + result);  // filter more values  result = HOSTING.entrySet().stream().filter(x -&gt; {  if(!x.getValue().contains(amazon) &amp;&amp; !x.getValue().contains(digital)) {  return true;  }  return false;  }).map(map -&gt; map.getValue()).collect(Collectors.joining(,)); System.out.println(With Java 8 : + result); } 输出： Before Java 8 : aws.amazon.com With Java 8 : aws.amazon.com With Java 8 : linode.com,heroku.com 2.java8- 过滤 Map # 2 这是另一个按键筛选 Map 的示例，但这次将返回一个 Map。 private static void two() {  Map&lt;Integer, String&gt; HOSTING = new HashMap&lt;&gt;();  HOSTING.put(1, linode.com);  HOSTING.put(2, heroku.com);  HOSTING.put(3, digitalocean.com);  HOSTING.put(4, aws.amazon.com); //Map -&gt; Stream -&gt; Filter -&gt; Map  Map&lt;Integer, String&gt; collect = HOSTING.entrySet().stream().filter(map -&gt; map.getKey() == 2)  .collect(Collectors.toMap(p -&gt; p.getKey(), p -&gt; p.getValue())); System.out.println(collect); Map&lt;Integer, String&gt; collect1 = HOSTING.entrySet().stream().filter(map -&gt; map.getKey() &lt;= 3)  .collect(Collectors.toMap(Entry::getKey, Entry::getValue)); System.out.println(collect1); } 输出： {2=heroku.com} {1=linode.com, 2=heroku.com, 3=digitalocean.com} 3.Java 8-Filter a Map # 3-Predicate 这一次，尝试新的 java8 Predicate。 // 提取公共校验为谓语 public static &lt;K, V&gt; Map&lt;K, V&gt; filterByValue(Map&lt;K, V&gt; map, Predicate&lt;V&gt; predicate) {  return map.entrySet().stream()  .filter(x-&gt;predicate.test(x.getValue()))  .collect(Collectors.toMap(Map.Entry::getKey,Map.Entry::getValue)); } private static void three() { Map&lt;Integer, String&gt; HOSTING = new HashMap&lt;&gt;();  HOSTING.put(1, linode.com);  HOSTING.put(2, heroku.com);  HOSTING.put(3, digitalocean.com);  HOSTING.put(4, aws.amazon.com);  HOSTING.put(5, aws2.amazon.com); // {1=linode.com}  Map&lt;Integer, String&gt; filteredMap = filterByValue(HOSTING, x-&gt;x.contains(linode));  System.out.println(filteredMap); // {1=linode.com, 4=aws.amazon.com, 5=aws2.amazon.com}  Map&lt;Integer, String&gt; filteredMap2 = filterByValue(HOSTING, x -&gt; (x.contains(aws) || x.contains(linode)));  System.out.println(filteredMap2); // {4=aws.amazon.com}  Map&lt;Integer, String&gt; filteredMap3 = filterByValue(HOSTING, x -&gt; (x.contains(aws) &amp;&amp; !x.contains(aws2)));  System.out.println(filteredMap3); // {1=linode.com, 2=heroku.com}  Map&lt;Integer, String&gt; filteredMap4 = filterByValue(HOSTING, x -&gt; (x.length() &lt;= 10));  System.out.println(filteredMap4); } 输出： {1=linode.com} {1=linode.com, 4=aws.amazon.com, 5=aws2.amazon.com} {4=aws.amazon.com} {1=linode.com, 2=heroku.com} 源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/12/Java-8-%E5%AF%B9-map-%E8%BF%87%E6%BB%A4.html",
      "keywords" : "Java 8, 系列, filter, map"
    } ,
  
    {
      "title"    : "Java 8 flatMap 示例",
      "category" : "Java8",
      "content": "引言 在 java8 中，Stream 可以保存不同的数据类型，例如: Stream&lt;String[]&gt; t Stream&lt;Set&lt;String&gt;&gt; t Stream&lt;List&lt;String&gt;&gt; t Stream&lt;List&lt;Object&gt;&gt; 但是，流操作(filter、 sum、 distinct…)和收集器不支持它，因此，我们需要 flatMap ()来执行以下转换: Stream&lt;String[]&gt; t t-&gt; flatMap -&gt; tStream&lt;String&gt; Stream&lt;Set&lt;String&gt;&gt; t-&gt; flatMap -&gt; tStream&lt;String&gt; Stream&lt;List&lt;String&gt;&gt; t-&gt; flatMap -&gt; tStream&lt;String&gt; Stream&lt;List&lt;Object&gt;&gt; t-&gt; flatMap -&gt; tStream&lt;Object&gt; Flatmap ()是如何工作的: { {1,2}, {3,4}, {5,6} } -&gt; flatMap -&gt; {1,2,3,4,5,6} 1.Stream + String [] + flatMap 下面的示例将打印一个空结果，因为 filter ()不知道如何过滤 String []流。 private static void one() {  //Stream&lt;String[]&gt;  Stream&lt;String[]&gt; temp = Arrays.stream(data); //filter a stream of string[], and return a string[]?  Stream&lt;String[]&gt; stream = temp.filter(x -&gt; a.equals(x.toString()));  stream.forEach(System.out::println); } 在上面的例子中，我们应该使用 flatMap ()将流字符串[]转换为流字符串。 private static void one() { //Stream&lt;String[]&gt;  Stream&lt;String[]&gt; temp = Arrays.stream(data); //filter a stream of string[], and return a string[]? // Stream&lt;String[]&gt; stream = temp.filter(x -&gt; a.equals(x.toString())); // stream.forEach(System.out::println);  // Stream&lt;String&gt;, GOOD!  Stream&lt;String&gt; stringStream = temp.flatMap(x -&gt; Arrays.stream(x));  Stream&lt;String&gt; stream1 = stringStream.filter(x -&gt; a.equals(x.toString()));  stream1.forEach(System.out::println); } 输出： a 2.Stream + Set + flatMap public class Student { private String name;  private Set&lt;String&gt; book; public void addBook(String book) {  if (this.book == null) {   this.book = new HashSet&lt;&gt;();  }  this.book.add(book);  }  //getters and setters }  private static void two() {  Student obj1 = new Student();  obj1.setName(mkyong);  obj1.addBook(Java 8 in Action);  obj1.addBook(Spring Boot in Action);  obj1.addBook(Effective Java (2nd Edition)); Student obj2 = new Student();  obj2.setName(zilap);  obj2.addBook(Learning Python, 5th Edition);  obj2.addBook(Effective Java (2nd Edition)); List&lt;Student&gt; list = new ArrayList&lt;&gt;();  list.add(obj1);  list.add(obj2); List&lt;String&gt; collect = list.stream().map(x -&gt; x.getBook()) //Stream&lt;Set&lt;String&gt;&gt;  .flatMap(x -&gt; x.stream()) //Stream&lt;String&gt;  .distinct()  .collect(Collectors.toList());  collect.forEach(x-&gt; System.out.println(x)); } 输出： Spring Boot in Action Effective Java (2nd Edition) Java 8 in Action Learning Python, 5th Edition 尝试注释 flatMap (x-x.stream ()) ，Collectors.toList ()将提示一个编译器错误，因为它不知道如何收集 Set 对象流。 3.Stream + Primitive + flatMapToInt 对于基本类型，可以使用 flatMapToInt。 private static void three() {  int[] intArray = {1, 2, 3, 4, 5, 6}; //1. Stream&lt;int[]&gt;  Stream&lt;int[]&gt; streamArray = Stream.of(intArray); //2. Stream&lt;int[]&gt; -&gt; flatMap -&gt; IntStream  IntStream intStream = streamArray.flatMapToInt(x -&gt; Arrays.stream(x)); intStream.forEach(x -&gt; System.out.println(x)); } 输出： 1 2 3 4 5 6 源码见：java-8-demo 系列文章详见：Java 8 教程 ",
      "url"      : "http://zhangjinmiao.github.io/java8/2019/08/13/Java-8-flatMap-%E7%A4%BA%E4%BE%8B.html",
      "keywords" : "Java 8, 系列, flatMap"
    } ,
  
    {
      "title"    : "Redis 分布式锁的正确实现方式（Java 版）",
      "category" : "Redis",
      "content": " 转载：https://wudashan.cn/2017/10/23/Redis-Distributed-Lock-Implement/ 前言 分布式锁一般有三种实现方式：  数据库乐观锁； 基于 Redis 的分布式锁； 基于 ZooKeeper 的分布式锁。 本篇博客将介绍第二种方式，基于 Redis 实现分布式锁。虽然网上已经有各种介绍 Redis 分布式锁实现的博客，然而他们的实现却有着各种各样的问题，为了避免误人子弟，本篇博客将详细介绍如何正确地实现 Redis 分布式锁。 可靠性 首先，为了确保分布式锁可用，我们至少要确保锁的实现同时满足以下四个条件：  互斥性。在任意时刻，只有一个客户端能持有锁。 不会发生死锁。即使有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证后续其他客户端能加锁。 具有容错性。只要大部分的 Redis 节点正常运行，客户端就可以加锁和解锁。 解铃还须系铃人。加锁和解锁必须是同一个客户端，客户端自己不能把别人加的锁给解了。 代码实现 组件依赖 首先我们要通过 Maven 引入 Jedis 开源组件，在 pom.xml 文件加入下面的代码： &lt;dependency&gt;  &lt;groupId&gt;redis.clients&lt;/groupId&gt;  &lt;artifactId&gt;jedis&lt;/artifactId&gt;  &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; 加锁代码 正确姿势 Talk is cheap, show me the code。先展示代码，再带大家慢慢解释为什么这样实现： public class RedisTool { private static final String LOCK_SUCCESS = OK;  private static final String SET_IF_NOT_EXIST = NX;  private static final String SET_WITH_EXPIRE_TIME = PX; /** * 尝试获取分布式锁 * @param jedis Redis客户端 * @param lockKey 锁 * @param requestId 请求标识 * @param expireTime 超期时间 * @return 是否获取成功 */  public static boolean tryGetDistributedLock(Jedis jedis, String lockKey, String requestId, int expireTime) {  String result = jedis.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime);  if (LOCK_SUCCESS.equals(result)) {   return true;  }  return false;  } } 可以看到，我们加锁就一行代码：jedis.set(String key, String value, String nxxx, String expx, int time)，这个 set()方法一共有五个形参：  第一个为 key，我们使用 key 来当锁，因为 key 是唯一的。   第二个为 value，我们传的是 requestId，很多童鞋可能不明白，有 key 作为锁不就够了吗，为什么还要用到 value？原因就是我们在上面讲到可靠性时，分布式锁要满足第四个条件解铃还须系铃人，通过给 value 赋值为 requestId，我们就知道这把锁是哪个请求加的了，在解锁的时候就可以有依据。requestId 可以使用 UUID.randomUUID().toString()方法生成。   第三个为 nxxx，这个参数我们填的是 NX，意思是 SET IF NOT EXIST，即当 key 不存在时，我们进行 set 操作；若 key 已经存在，则不做任何操作；   第四个为 expx，这个参数我们传的是 PX，意思是我们要给这个 key 加一个过期的设置，具体时间由第五个参数决定。   第五个为 time，与第四个参数相呼应，代表 key 的过期时间。 总的来说，执行上面的 set()方法就只会导致两种结果：  当前没有锁（key 不存在），那么就进行加锁操作，并对锁设置个有效期，同时 value 表示加锁的客户端。 已有锁存在，不做任何操作。 心细的童鞋就会发现了，我们的加锁代码满足我们可靠性里描述的三个条件。首先，set()加入了 NX 参数，可以保证如果已有 key 存在，则函数不会调用成功，也就是只有一个客户端能持有锁，满足互斥性。其次，由于我们对锁设置了过期时间，即使锁的持有者后续发生崩溃而没有解锁，锁也会因为到了过期时间而自动解锁（即 key 被删除），不会发生死锁。最后，因为我们将 value 赋值为 requestId，代表加锁的客户端请求标识，那么在客户端在解锁的时候就可以进行校验是否是同一个客户端。由于我们只考虑 Redis 单机部署的场景，所以容错性我们暂不考虑。 错误示例 1 比较常见的错误示例就是使用 jedis.setnx()和 jedis.expire()组合实现加锁，代码如下： public static void wrongGetLock1(Jedis jedis, String lockKey, String requestId, int expireTime) { Long result = jedis.setnx(lockKey, requestId);  if (result == 1) {  // 若在这里程序突然崩溃，则无法设置过期时间，将发生死锁  jedis.expire(lockKey, expireTime);  } } setnx()方法作用就是 SET IF NOT EXIST，expire()方法就是给锁加一个过期时间。乍一看好像和前面的 set()方法结果一样，然而由于这是两条 Redis 命令，不具有原子性，如果程序在执行完 setnx()之后突然崩溃，导致锁没有设置过期时间。那么将会发生死锁。网上之所以有人这样实现，是因为低版本的 jedis 并不支持多参数的 set()方法。 错误示例 2 这一种错误示例就比较难以发现问题，而且实现也比较复杂。 实现思路：使用 jedis.setnx()命令实现加锁，其中 key 是锁，value 是锁的过期时间。 执行过程:  通过 setnx()方法尝试加锁，如果当前锁不存在，返回加锁成功。 如果锁已经存在则获取锁的过期时间，和当前时间比较，如果锁已经过期，则设置新的过期时间，返回加锁成功。 代码如下： public static boolean wrongGetLock2(Jedis jedis, String lockKey, int expireTime) { long expires = System.currentTimeMillis() + expireTime;  String expiresStr = String.valueOf(expires); // 如果当前锁不存在，返回加锁成功  if (jedis.setnx(lockKey, expiresStr) == 1) {  return true;  } // 如果锁存在，获取锁的过期时间  String currentValueStr = jedis.get(lockKey);  if (currentValueStr != null &amp;&amp; Long.parseLong(currentValueStr) &lt; System.currentTimeMillis()) {  // 锁已过期，获取上一个锁的过期时间，并设置现在锁的过期时间  String oldValueStr = jedis.getSet(lockKey, expiresStr);  if (oldValueStr != null &amp;&amp; oldValueStr.equals(currentValueStr)) {   // 考虑多线程并发的情况，只有一个线程的设置值和当前值相同，它才有权利加锁   return true;  }  }   // 其他情况，一律返回加锁失败  return false; } 那么这段代码问题在哪里？  由于是客户端自己生成过期时间，所以需要强制要求分布式下每个客户端的时间必须同步。 当锁过期的时候，如果多个客户端同时执行 jedis.getSet()方法，那么虽然最终只有一个客户端可以加锁，但是这个客户端的锁的过期时间可能被其他客户端覆盖。 锁不具备拥有者标识，即任何客户端都可以解锁。 解锁代码 正确姿势 还是先展示代码，再带大家慢慢解释为什么这样实现： public class RedisTool { private static final Long RELEASE_SUCCESS = 1L; /** * 释放分布式锁 * @param jedis Redis客户端 * @param lockKey 锁 * @param requestId 请求标识 * @return 是否释放成功 */  public static boolean releaseDistributedLock(Jedis jedis, String lockKey, String requestId) {  String script = if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end;  Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId));  if (RELEASE_SUCCESS.equals(result)) {   return true;  }  return false;  } } 可以看到，我们解锁只需要两行代码就搞定了！第一行代码，我们写了一个简单的 Lua 脚本代码，上一次见到这个编程语言还是在《黑客与画家》里，没想到这次居然用上了。第二行代码，我们将 Lua 代码传到 jedis.eval()方法里，并使参数 KEYS[1]赋值为 lockKey，ARGV[1]赋值为 requestId。 eval()方法是将 Lua 代码交给 Redis 服务端执行。 那么这段 Lua 代码的功能是什么呢？ 其实很简单，首先获取锁对应的 value 值，检查是否与 requestId 相等，如果相等则删除锁（解锁）。那么为什么要使用 Lua 语言来实现呢？因为要确保上述操作是原子性的。关于非原子性会带来什么问题，可以阅读【解锁代码-错误示例 2】 。那么为什么执行 eval()方法可以确保原子性，源于 Redis 的特性，下面是官网对 eval 命令的部分解释： 简单来说，就是在 eval 命令执行 Lua 代码的时候，Lua 代码将被当成一个命令去执行，并且直到 eval 命令执行完成，Redis 才会执行其他命令。 错误示例 1 最常见的解锁代码就是直接使用 jedis.del()方法删除锁，这种不先判断锁的拥有者而直接解锁的方式，会导致任何客户端都可以随时进行解锁，即使这把锁不是它的。 public static void wrongReleaseLock1(Jedis jedis, String lockKey) {  jedis.del(lockKey); } 错误示例 2 这种解锁代码乍一看也是没问题，甚至我之前也差点这样实现，与正确姿势差不多，唯一区别的是分成两条命令去执行，代码如下： public static void wrongReleaseLock2(Jedis jedis, String lockKey, String requestId) {   // 判断加锁与解锁是不是同一个客户端  if (requestId.equals(jedis.get(lockKey))) {  // 若在此时，这把锁突然不是这个客户端的，则会误解锁  jedis.del(lockKey);  } } 如代码注释，问题在于如果调用 jedis.del()方法的时候，这把锁已经不属于当前客户端的时候会解除他人加的锁。那么是否真的有这种场景？答案是肯定的，比如客户端 A 加锁，一段时间之后客户端 A 解锁，在执行 jedis.del()之前，锁突然过期了，此时客户端 B 尝试加锁成功，然后客户端 A 再执行 del()方法，则将客户端 B 的锁给解除了。 总结 本文主要介绍了如何使用 Java 代码正确实现 Redis 分布式锁，对于加锁和解锁也分别给出了两个比较经典的错误示例。其实想要通过 Redis 实现分布式锁并不难，只要保证能满足可靠性里的四个条件。互联网虽然给我们带来了方便，只要有问题就可以 google，然而网上的答案一定是对的吗？其实不然，所以我们更应该时刻保持着质疑精神，多想多验证。 如果你的项目中 Redis 是多机部署的，那么可以尝试使用 Redisson 实现分布式锁，这是 Redis 官方提供的 Java 组件，链接在参考阅读章节已经给出。 实用工具 结合 redisson 实现了一套分布式锁的通用 SDK，项目中可直接使用。 参考阅读 [1] Distributed locks with Redis [2] EVAL command [3] Redisson ",
      "url"      : "http://zhangjinmiao.github.io/redis/2019/10/09/redis-distributed-lock.html",
      "keywords" : "redis"
    } ,
  
    {
      "title"    : "开源数据同步神器——canal",
      "category" : "Canal",
      "content": " 本文转自：IT 米粉 前言 如今大型的 IT 系统中，都会使用分布式的方式，同时会有非常多的中间件，如 redis、消息队列、大数据存储等，但是实际核心的数据存储依然是存储在数据库，作为使用最广泛的数据库，如何将 mysql 的数据与中间件的数据进行同步，既能确保数据的一致性、及时性，也能做到代码无侵入的方式呢？如果有这样的一个需求，数据修改后，需要及时的将 mysql 中的数据更新到 elasticsearch,我们会怎么进行实现呢？ 数据同步方案选择 针对上文的需求，经过思考，初步有如下的一些方案：  代码实现  针对代码中进行数据库的增删改操作时，同时进行 elasticsearch 的增删改操作。   mybatis 实现  通过 mybatis plugin 进行实现，截取 sql 语句进行分析， 针对 insert、update、delete 的语句进行处理。显然，这些操作如果都是单条数据的操作，是很容易处理的。但是，实际开发中，总是会有一些批量的更新或者删除操作，这时候，就很难进行处理了。   Aop 实现  不管是通过哪种 Aop 方式，根据制定的规则，如规范方法名，注解等进行切面处理，但依然还是会出现无法处理批量操作数据的问题。   logstash  logstash 类似的同步组件提供的文件和数据同步的功能，可以进行数据的同步，只需要简单的配置就能将 mysql 数据同步到 elasticsearch，但是 logstash 的原理是每秒进行一次增量数据查询，将结果同步到 elasticsearch ，实时性要求特别高的，可能无法满足要求。且此方案的性能不是很好，造成资源的浪费。   那么是否有什么更好的方式进行处理吗？mysql binlog 同步，实时性强，对于应用无任何侵入性，且性能更好，不会造成资源浪费，那么就有了我今天的主角——canal canal 介绍 canal 是阿里巴巴的一个开源项目，基于 java 实现，整体已经在很多大型的互联网项目生产环境中使用，包括阿里、美团等都有广泛的应用，是一个非常成熟的数据库同步方案，基础的使用只需要进行简单的配置即可。 canal 是通过模拟成为 mysql 的 slave 的方式，监听 mysql 的 binlog 日志来获取数据，binlog 设置为 row 模式以后，不仅能获取到执行的每一个增删改的脚本，同时还能获取到修改前和修改后的数据，基于这个特性，canal 就能高性能的获取到 mysql 数据数据的变更。 使用 canal 的介绍在官网有非常详细的说明，如果想了解更多，大家可以移步官网（https://github.com/alibaba/canal）了解。我这里补充下使用中不太容易理解部分。 canal 的部署主要分为 server 端和 client 端。 server 端部署好以后，可以直接监听 mysql binlog,因为 server 端是把自己模拟成了 mysql slave，所以，只能接受数据，没有进行任何逻辑的处理，具体的逻辑处理，需要 client 端进行处理。 client 端一般是需要大家进行简单的开发。https://github.com/alibaba/canal/wiki/ClientAPI 有一个简单的示例，很容易理解。 canal Adapter 为了便于大家的使用，官方做了一个独立的组件　Adapter，Adapter　是可以将　canal server　端获取的数据转换成几个常用的中间件数据源，现在支持　kafka、rocketmq、hbase、elasticsearch，针对这几个中间件的支持，直接配置即可，无需开发。上文中，如果需要将　mysql　的数据同步到　elasticsearch，直接运行 canal Adapter，修改相关的配置即可。 ###　常见问题 －　无法接收到数据，程序也没有报错？ 一定要确保　mysql　的　binlog　模式为　row　模式，canal　原理是解析　Binlog　文件，并且直接从文件中获取数据的。 －　Adapter 使用无法同步数据？ 按照官方文档，检查配置项，如 sql 的大小写，字段的大小写可能都会有影响，如果还无法搞定，可以自己获取代码调试下，Adapter 的代码还是比较容易看懂的。 canal Adapter elasticsearch 改造 因为有了 canal 和 canal Adapter 这个神器，同步到 elasticsearch、hbase 等问题都解决了，但是自己的开发的过程中发现，Adapter 使用还是有些问题，因为先使用的是 elasticsearch 同步功能，所以对 elasticsearch 进行了一些改造： ###　elasticsearch　初始化 一个全新的　elasticsearch　无法使用，因为没有创建　elasticsearch index　和　mapping,增加了对应的功能。 elasticsearch　配置文件　mapping　节点增加两个参数： enablefieldmap: true fieldmap:  id: text  name: text  c_time: text enablefieldmap 是否需要自动生成 fieldmap，默认为 false,如果需要启动的时候就生成这设置为 true,并且设置 fieldmap,类似 elasticsearch mapping 中每个字段的类型。 esconfig bug 处理 代码中获取 binlog 的日志处理时，必须要获取数据库名，但是当获取 binlog 为 type query 时，是无法获取数据库名的，此处有 bug，导致出现 “Outer adapter write failed” ,且未输出错误日志，修复此 bug. 后续计划 增加 rabbit MQ 的支持 增加 redis 的支持 源码 源码地址：https://github.com/itmifen/canal ",
      "url"      : "http://zhangjinmiao.github.io/canal/2019/10/29/canal.html",
      "keywords" : "读写分离, canal, 中间件, 数据库"
    } ,
  
    {
      "title"    : "Mac 下 IDEA 快捷键",
      "category" : "Tools",
      "content": "IntelliJ IDEA For Mac 快捷键 alt+f7 查找变量方法使用的地方 F3 添加书签 Ctrl + O 快捷覆写方法 Alt + F3 添加书签标识 command + F3 显示书签 command + Shift + A 查找动作 Alt + F1 快捷选择 command+alt+f7 这个是查找选中的字符在工程中出现的地方，可以不是方法变量类等，这个和上面的有区别的 command＋F7 可以查询当前元素在当前文件中的引用，然后按 F3 可以选择，功能基本同上选中文本，按 command+shift+F7 ，高亮显示所有该文本，按 Esc 高亮消失。选中文本，按 Alt+F3 ，逐个往下查找相同文本，并高亮显示。shift+f3 就是往上找 ctrl+enter 出现生成 get,set 方法的界面 shift+enter 换到下一行 command+R 替换 command+shift+R 可以在整个工程或着某个目录下面替换变量 command+control+R 运行当前工程 command+Y 查看选中当前源码 command+D 复制一行 command+delete 删除一行 control+shift+J 把多行连接成一行，会去掉空格的行 command+J 可以生成一些自动代码，比如 for 循环 command+B 找变量的来源 同 F4 查找变量来源 control+shift+B 找变量所属的类 command+G 查找变量并且定位 command+shift+F 可以在整个工程或着某个目录下面查找变量 相当于 eclipse 里的 ctrl+H alt+shift+C 最近修改的文件 command+E 最近打开的文件 alt+enter 导入包，自动修改 command+alt+L 格式化代码 command+alt+I 自动缩进，不用多次使用 tab 或着 backspace 键，也是比较方便的 command+shift+enter 代码补全，这个会判断可能用到的，这个代码补全和代码提示是不一样的 command+P 方法参数提示 command+alt+T 把选中的代码放在 TRY{} IF{} ELSE{} 里 command+X 剪切 command+shift+V 可以复制多个文本 command+shift+U 大小写转换 command+/ 注释一行或着多行 // command+alt+/ 注释/…/ command+alt+左右箭头 返回上次编辑的位置 command+左右箭头 返回最左边最右边 shift+f6 重命名 command+shift+上下箭头 把代码上移或着下移 command+[或] 可以跳到大括号的开头结尾 command+f12 可以显示当前文件的结构 command+alt+B 可以导航到一个抽象方法的实现代码 command+, 呼出偏好设置 Mac 键盘符号和修饰键说明 ⌘ Command ⇧ Shift ⌥ Option ⌃ Control ↩︎ Return/Enter ⌫ Delete ⌦ 向前删除键（Fn+Delete） ↑ 上箭头 ↓ 下箭头 ← 左箭头 → 右箭头 ⇞ Page Up（Fn+↑） ⇟ Page Down（Fn+↓） Home Fn + ← End Fn + → ⇥ 右制表符（Tab 键） ⇤ 左制表符（Shift+Tab） ⎋ Escape (Esc) Editing（编辑） ⌃Space 基本的代码补全（补全任何类、方法、变量） ⌃⇧Space 智能代码补全（过滤器方法列表和变量的预期类型） ⌘⇧↩ 自动结束代码，行末自动添加分号 ⌘P 显示方法的参数信息 ⌃J, Mid. button click 快速查看文档 ⇧F1 查看外部文档（在某些代码上会触发打开浏览器显示相关文档） ⌘+鼠标放在代码上 显示代码简要信息 ⌘F1 在错误或警告处显示具体描述信息 ⌘N, ⌃↩, ⌃N 生成代码（getter、setter、构造函数、hashCode/equals,toString） ⌃O 覆盖方法（重写父类方法） ⌃I 实现方法（实现接口中的方法） ⌘⌥T 包围代码（使用 if..else, try..catch, for, synchronized 等包围选中的代码） ⌘/ 注释/取消注释与行注释 ⌘⌥/ 注释/取消注释与块注释 ⌥↑ 连续选中代码块 ⌥↓ 减少当前选中的代码块 ⌃⇧Q 显示上下文信息 ⌥↩ 显示意向动作和快速修复代码 ⌘⌥L 格式化代码 ⌃⌥O 优化 import ⌃⌥I 自动缩进线 ⇥ / ⇧⇥ 缩进代码 / 反缩进代码 ⌘X 剪切当前行或选定的块到剪贴板 ⌘C 复制当前行或选定的块到剪贴板 ⌘V 从剪贴板粘贴 ⌘⇧V 从最近的缓冲区粘贴 ⌘D 复制当前行或选定的块 ⌘⌫ 删除当前行或选定的块的行 ⌃⇧J 智能的将代码拼接成一行 ⌘↩ 智能的拆分拼接的行 ⇧↩ 开始新的一行 ⌘⇧U 大小写切换 ⌘⇧] / ⌘⇧[ 选择直到代码块结束/开始 ⌥⌦ 删除到单词的末尾（⌦键为 Fn+Delete） ⌥⌫ 删除到单词的开头 ⌘+ / ⌘- 展开 / 折叠代码块 ⌘⇧+ 展开所以代码块 ⌘⇧- 折叠所有代码块 ⌘W 关闭活动的编辑器选项卡 Search/Replace（查询/替换） Double ⇧ 查询任何东西 ⌘F 文件内查找 ⌘G 查找模式下，向下查找 ⌘⇧G 查找模式下，向上查找 ⌘R 文件内替换 ⌘⇧F 全局查找（根据路径） ⌘⇧R 全局替换（根据路径） ⌘⇧S 查询结构（Ultimate Edition 版专用，需要在 Keymap 中设置） ⌘⇧M 替换结构（Ultimate Edition 版专用，需要在 Keymap 中设置） Usage Search（使用查询） ⌥F7 / ⌘F7 在文件中查找用法 / 在类中查找用法 ⌘⇧F7 在文件中突出显示的用法 ⌘⌥F7 显示用法 Compile and Run（编译和运行） ⌘F9 编译 Project ⌘⇧F9 编译选择的文件、包或模块 ⌃⌥R 弹出 Run 的可选择菜单 ⌃⌥D 弹出 Debug 的可选择菜单 ⌃R 运行 ⌃D 调试 ⌃⇧R, ⌃⇧D 从编辑器运行上下文环境配置 Debugging（调试） F8 进入下一步，如果当前行断点是一个方法，则不进入当前方法体内 F7 进入下一步，如果当前行断点是一个方法，则进入当前方法体内，如果该方法体还有方法，则不会进入该内嵌的方法中 ⇧F7 智能步入，断点所在行上有多个方法调用，会弹出进入哪个方法 ⇧F8 跳出 ⌥F9 运行到光标处，如果光标前有其他断点会进入到该断点 ⌥F8 计算表达式（可以更改变量值使其生效） ⌘⌥R 恢复程序运行，如果该断点下面代码还有断点则停在下一个断点上 ⌘F8 切换断点（若光标当前行有断点则取消断点，没有则加上断点） ⌘⇧F8 查看断点信息 Navigation（导航） ⌘O 查找类文件 ⌘⇧O 查找所有类型文件、打开文件、打开目录，打开目录需要在输入的内容前面或后面加一个反斜杠/ ⌘⌥O 前往指定的变量 / 方法 ⌃← / ⌃→ 左右切换打开的编辑 tab 页 F12 返回到前一个工具窗口 ⎋ 从工具窗口进入代码文件窗口 ⇧⎋ 隐藏当前或最后一个活动的窗口，且光标进入代码文件窗口 ⌘⇧F4 关闭活动 run/messages/find/… tab ⌘L 在当前文件跳转到某一行的指定处 ⌘E 显示最近打开的文件记录列表 ⌘⌥← / ⌘⌥→ 退回 / 前进到上一个操作的地方 ⌘⇧⌫ 跳转到最后一个编辑的地方 ⌥F1 显示当前文件选择目标弹出层，弹出层中有很多目标可以进行选择(如在代码编辑窗口可以选择显示该文件的 Finder) ⌘B / ⌘ 鼠标点击 进入光标所在的方法/变量的接口或是定义处 ⌘⌥B 跳转到实现处，在某个调用的方法名上使用会跳到具体的实现处，可以跳过接口 ⌥ Space, ⌘Y 快速打开光标所在方法、类的定义 ⌃⇧B 跳转到类型声明处 ⌘U 前往当前光标所在方法的父类的方法 / 接口定义 ⌃↓ / ⌃↑ 当前光标跳转到当前文件的前一个/后一个方法名位置 ⌘] / ⌘[ 移动光标到当前所在代码的花括号开始/结束位置 ⌘F12 弹出当前文件结构层，可以在弹出的层上直接输入进行筛选（可用于搜索类中的方法） ⌃H 显示当前类的层次结构 ⌘⇧H 显示方法层次结构 ⌃⌥H 显示调用层次结构 F2 / ⇧F2 跳转到下一个/上一个突出错误或警告的位置 F4 / ⌘↓ 编辑/查看代码源 ⌥ Home 显示到当前文件的导航条 F3 选中文件/文件夹/代码行，添加/取消书签 ⌥F3 选中文件/文件夹/代码行，使用助记符添加/取消书签 ⌃0…⌃9 定位到对应数值的书签位置 ⌘F3 显示所有书签 Refactoring（重构） F5 复制文件到指定目录 F6 移动文件到指定目录 ⌘⌫ 在文件上为安全删除文件，弹出确认框 ⇧F6 重命名文件 ⌘F6 更改签名 ⌘⌥N 一致性 ⌘⌥M 将选中的代码提取为方法 ⌘⌥V 提取变量 ⌘⌥F 提取字段 ⌘⌥C 提取常量 ⌘⌥P 提取参数 VCS/Local History（版本控制/本地历史记录） ⌘K 提交代码到版本控制器 ⌘T 从版本控制器更新代码 ⌥⇧C 查看最近的变更记录 ⌃C 快速弹出版本控制器操作面板 Live Templates（动态代码模板） ⌘⌥J 弹出模板选择窗口，将选定的代码使用动态模板包住 ⌘J 插入自定义动态代码模板 General（通用） ⌘1…⌘9 打开相应编号的工具窗口 ⌘S 保存所有 ⌘⌥Y 同步、刷新 ⌃⌘F 切换全屏模式 ⌘⇧F12 切换最大化编辑器 ⌥⇧F 添加到收藏夹 ⌥⇧I 检查当前文件与当前的配置文件 §⌃, ⌃` 快速切换当前的 scheme（切换主题、代码样式等） ⌘, 打开 IDEA 系统设置 ⌘; 打开项目结构对话框 ⇧⌘A 查找动作（可设置相关选项） ⌃⇥ 编辑窗口标签和工具窗口之间切换（如果在切换的过程加按上 delete，则是关闭对应选中的窗口） Other（一些官方文档上没有体现的快捷键） ⌘⇧8 竖编辑模式 ",
      "url"      : "http://zhangjinmiao.github.io/tools/2019/12/15/Idea-shortcut-use-mac.html",
      "keywords" : "Mac,IDEA"
    } ,
  
    {
      "title"    : "分布式配置中心——Apollo",
      "category" : "Java",
      "content": "1.概述 官方介绍  Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。 服务端基于 Spring Boot 和 Spring Cloud 开发，打包后可以直接运行，不需要额外安装 Tomcat 等应用容器。 Java 客户端不依赖任何框架，能够运行于所有 Java 运行时环境，同时对 Spring/Spring Boot 环境也有额外支持。 .Net 客户端不依赖任何框架，能够运行于所有 .Net 运行时环境。 核心功能  统一管理不同环境、不同集群的配置 配置修改实时生效（热发布） 版本发布管理 灰度发布 权限管理、发布审核、操作审计 客户端配置信息监控 提供 Java 和 .Net 原生客户端 提供开放平台 API 部署简单 详细的功能说明，请访问 《Apollo 功能列表》 。 2.Apollo架构设计之服务器端 官方架构图 模块介绍 架构图二 针对官方给出的架构图做了下拆分，便于理解。 领域模型 权限模型 实时推送设计 Release Message 设计 3.Apollo架构设计之客户端 客户端架构图 客户端实现总结 4.Apollo架构设计之高可用和监控 HA 高可用设计 HA 图例 监控 CAT：实时监控告警服务中间件 5.分布式部署指南 参考：分布式部署指南 先决条件 部署案例 部署图例 配置注意点 6.Java客户端使用 参考：Java客户端使用指南 Apollo Client和Spring集成~XML方式 参考：lab03 Apollo Client和Spring集成~代码方式 参考：lab04 Apollo Client和Spring Boot集成 参考：lab05 Apollo开放平台接入实操 参考：lab06 相关文档资料：  官方gitHub  Apollo配置中心介绍  本地快速部署Quick Start  应用接入指南  Java客户端使用指南  Apollo使用场景和示例代码  Apollo实践案例  杨波——携程 Apollo 配置中心架构深度剖析  Apollo源码解析 ",
      "url"      : "http://zhangjinmiao.github.io/java/2020/04/22/apollo.html",
      "keywords" : "apollo, 分布式配置中心"
    } ,
  
    {
      "title"    : "Docker 学习",
      "category" : "Docker",
      "content": "Docker 介绍 什么是 Docker Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 OverlayFS 类的 Union FS 等技术，对进程进行封装隔离，属于 操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 containerd。 Docker 架构： runc 是一个 Linux 命令行工具，用于根据 OCI容器运行时规范 创建和运行容器。 containerd 是一个守护程序，它管理容器生命周期，提供了在一个节点上执行容器和管理镜像的最小功能集。 Docker 和传统虚拟化方式的比较： 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程； 容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟，比传统虚拟机更为轻便。 为什么要用 Docker 1：更高效的利用系统资源 2：更快速的启动时间 3：一致的运行环境 4：持续交付和部署 5：更轻松的迁移 6：更轻松的维护和扩展 对比传统虚拟机总结    特性  启动  硬盘使用  性能  系统支持量     容器  秒级  一般为 MB  接近原生  单机支持上千个容器    虚拟机  分钟级  一般为 GB  若于  一般几十个   参考：  Docker 简介 Docker 安装 Docker 命令 基础命令    命令  命令描述     info  显示 Docker 详细的系统信息    version  显示 docker 客户端和服务端版本信息    inspect  查看容器或镜像的配置信息, 默认为 json 数据    events  实时打印服务端执行的事件   镜像命令    命令  命令描述     images  查看本地镜像（列出本地所有镜像）    inspect  查看镜像详情    search  查找镜像    tag  修改镜像 tag    history  显示镜像每层的变更内容    rmi  删除本地镜像    pull  获取镜像    push  推送镜像到仓库    login  登录第三方仓库    logout  退出第三方仓库    save  打包本地镜像, 使用压缩包来完成迁移    load  导入镜像压缩包    commit  将容器保存为镜像    build  使用 Dockerfile 构建镜像    import  导入本地容器快照文件为镜像   容器命令    命令  命令描述     create  根据镜像生成一个新的容器    start  启动一个新的容器    run  创建、启动容器并执行相应的命令    rename  重命名容器名    ps  查看运行中的容器    top  显示容器的运行进程    stop  关闭容器    kill  强制关闭容器    restart  重启容器    pause  暂停容器    unpause  恢复暂停的容器    exec  在已运行的容器中执行命令    attach  进入运行中的容器, 显示该容器的控制台界面。    logs  打印容器的控制台输出内容    port  容器端口映射列表    rm  删除已停止的容器    diff  展示容器相对于构建它的镜像内容所做的改变    export  导出容器到本地快照文件    cp  在容器和宿主机之间复制文件    wait  阻塞当前命令直到对应的容器被关闭, 容器关闭后打印结束代码   资源命令    命令  命令描述     stats  显示容器硬件资源使用情况    update  更新容器的硬件资源限制    system  管理系统资源   参考： 官方命令 Docker 常用指令详解 Docker 镜像 网络 存储 7.Docker Compose Docker Compose 的文档： https://docs.docker.com/compose/overview/ 相关文档资料：  官网  敖小剑 Docker 学习笔记  Docker–从入门到实践 ",
      "url"      : "http://zhangjinmiao.github.io/docker/2020/06/16/docker.html",
      "keywords" : "docker, 云原生, 容器化"
    } ,
  
    {
      "title"    : "微服务常见面试",
      "category" : "microservice",
      "content": "1.微服务篇 1.1.SpringCloud常见组件有哪些？ 问题说明：这个题目主要考察对SpringCloud的组件基本了解 难易程度：简单 参考话术： SpringCloud包含的组件很多，有很多功能是重复的。其中最常用组件包括： •注册中心组件：Eureka、Nacos等 •负载均衡组件：Ribbon •远程调用组件：OpenFeign •网关组件：Zuul、Gateway •服务保护组件：Hystrix、Sentinel •服务配置管理组件：SpringCloudConfig、Nacos 1.2.Nacos的服务注册表结构是怎样的？ 问题说明：考察对Nacos数据分级结构的了解，以及Nacos源码的掌握情况 难易程度：一般 参考话术： Nacos采用了数据的分级存储模型，最外层是Namespace，用来隔离环境。然后是Group，用来对服务分组。接下来就是服务（Service）了，一个服务包含多个实例，但是可能处于不同机房，因此Service下有多个集群（Cluster），Cluster下是不同的实例（Instance）。 对应到Java代码中，Nacos采用了一个多层的Map来表示。结构为Map&lt;String, Map&lt;String, Service»，其中最外层Map的key就是namespaceId，值是一个Map。内层Map的key是group拼接serviceName，值是Service对象。Service对象内部又是一个Map，key是集群名称，值是Cluster对象。而Cluster对象内部维护了Instance的集合。 如图： 1.3.Nacos如何支撑阿里内部数十万服务注册压力？ 问题说明：考察对Nacos源码的掌握情况 难易程度：难 参考话术： Nacos内部接收到注册的请求时，不会立即写数据，而是将服务注册的任务放入一个阻塞队列就立即响应给客户端。然后利用线程池读取阻塞队列中的任务，异步来完成实例更新，从而提高并发写能力。 1.4.Nacos如何避免并发读写冲突问题？ 问题说明：考察对Nacos源码的掌握情况 难易程度：难 参考话术： Nacos在更新实例列表时，会采用CopyOnWrite技术，首先将旧的实例列表拷贝一份，然后更新拷贝的实例列表，再用更新后的实例列表来覆盖旧的实例列表。 这样在更新的过程中，就不会对读实例列表的请求产生影响，也不会出现脏读问题了。 1.5.Nacos与Eureka的区别有哪些？ 问题说明：考察对Nacos、Eureka的底层实现的掌握情况 难易程度：难 参考话术： Nacos与Eureka有相同点，也有不同之处，可以从以下几点来描述：  接口方式：Nacos与Eureka都对外暴露了Rest风格的API接口，用来实现服务注册、发现等功能 实例类型：Nacos的实例有永久和临时实例之分；而Eureka只支持临时实例 健康检测：Nacos对临时实例采用心跳模式检测，对永久实例采用主动请求来检测；Eureka只支持心跳模式 服务发现：Nacos支持定时拉取和订阅推送两种模式；Eureka只支持定时拉取模式 1.6.Sentinel的限流与Gateway的限流有什么差别？ 问题说明：考察对限流算法的掌握情况 难易程度：难 参考话术： 限流算法常见的有三种实现：滑动时间窗口、令牌桶算法、漏桶算法。Gateway则采用了基于Redis实现的令牌桶算法。 而Sentinel内部却比较复杂：  默认限流模式是基于滑动时间窗口算法 排队等待的限流模式则基于漏桶算法 而热点参数限流则是基于令牌桶算法 1.7.Sentinel的线程隔离与Hystix的线程隔离有什么差别? 问题说明：考察对线程隔离方案的掌握情况 难易程度：一般 参考话术： Hystix默认是基于线程池实现的线程隔离，每一个被隔离的业务都要创建一个独立的线程池，线程过多会带来额外的CPU开销，性能一般，但是隔离性更强。 Sentinel是基于信号量（计数器）实现的线程隔离，不用创建线程池，性能较好，但是隔离性一般。 2.MQ篇 2.1.你们为什么选择了RabbitMQ而不是其它的MQ？ 如图： 话术： kafka是以吞吐量高而闻名，不过其数据稳定性一般，而且无法保证消息有序性。我们公司的日志收集也有使用，业务模块中则使用的RabbitMQ。 阿里巴巴的RocketMQ基于Kafka的原理，弥补了Kafka的缺点，继承了其高吞吐的优势，其客户端目前以Java为主。但是我们担心阿里巴巴开源产品的稳定性，所以就没有使用。 RabbitMQ基于面向并发的语言Erlang开发，吞吐量不如Kafka，但是对我们公司来讲够用了。而且消息可靠性较好，并且消息延迟极低，集群搭建比较方便。支持多种协议，并且有各种语言的客户端，比较灵活。Spring对RabbitMQ的支持也比较好，使用起来比较方便，比较符合我们公司的需求。 综合考虑我们公司的并发需求以及稳定性需求，我们选择了RabbitMQ。 2.2.RabbitMQ如何确保消息的不丢失？ 话术： RabbitMQ针对消息传递过程中可能发生问题的各个地方，给出了针对性的解决方案：  生产者发送消息时可能因为网络问题导致消息没有到达交换机：  RabbitMQ提供了publisher confirm机制    生产者发送消息后，可以编写ConfirmCallback函数   消息成功到达交换机后，RabbitMQ会调用ConfirmCallback通知消息的发送者，返回ACK   消息如果未到达交换机，RabbitMQ也会调用ConfirmCallback通知消息的发送者，返回NACK   消息超时未发送成功也会抛出异常     消息到达交换机后，如果未能到达队列，也会导致消息丢失：  RabbitMQ提供了publisher return机制    生产者可以定义ReturnCallback函数   消息到达交换机，未到达队列，RabbitMQ会调用ReturnCallback通知发送者，告知失败原因     消息到达队列后，MQ宕机也可能导致丢失消息：  RabbitMQ提供了持久化功能，集群的主从备份功能    消息持久化，RabbitMQ会将交换机、队列、消息持久化到磁盘，宕机重启可以恢复消息   镜像集群，仲裁队列，都可以提供主从备份功能，主节点宕机，从节点会自动切换为主，数据依然在     消息投递给消费者后，如果消费者处理不当，也可能导致消息丢失  SpringAMQP基于RabbitMQ提供了消费者确认机制、消费者重试机制，消费者失败处理策略：    消费者的确认机制：     消费者处理消息成功，未出现异常时，Spring返回ACK给RabbitMQ，消息才被移除   消费者处理消息失败，抛出异常，宕机，Spring返回NACK或者不返回结果，消息不被异常       消费者重试机制：     默认情况下，消费者处理失败时，消息会再次回到MQ队列，然后投递给其它消费者。Spring提供的消费者重试机制，则是在处理失败后不返回NACK，而是直接在消费者本地重试。多次重试都失败后，则按照消费者失败处理策略来处理消息。避免了消息频繁入队带来的额外压力。       消费者失败策略：     当消费者多次本地重试失败时，消息默认会丢弃。   Spring提供了Republish策略，在多次重试都失败，耗尽重试次数后，将消息重新投递给指定的异常交换机，并且会携带上异常栈信息，帮助定位问题。         2.3.RabbitMQ如何避免消息堆积？ 话术： 消息堆积问题产生的原因往往是因为消息发送的速度超过了消费者消息处理的速度。因此解决方案无外乎以下三点：  提高消费者处理速度 增加更多消费者 增加队列消息存储上限 1）提高消费者处理速度 消费者处理速度是由业务代码决定的，所以我们能做的事情包括：  尽可能优化业务代码，提高业务性能 接收到消息后，开启线程池，并发处理多个消息 优点：成本低，改改代码即可 缺点：开启线程池会带来额外的性能开销，对于高频、低时延的任务不合适。推荐任务执行周期较长的业务。 2）增加更多消费者 一个队列绑定多个消费者，共同争抢任务，自然可以提供消息处理的速度。 优点：能用钱解决的问题都不是问题。实现简单粗暴 缺点：问题是没有钱。成本太高 3）增加队列消息存储上限 在RabbitMQ的1.8版本后，加入了新的队列模式：Lazy Queue 这种队列不会将消息保存在内存中，而是在收到消息后直接写入磁盘中，理论上没有存储上限。可以解决消息堆积问题。 优点：磁盘存储更安全；存储无上限；避免内存存储带来的Page Out问题，性能更稳定； 缺点：磁盘存储受到IO性能的限制，消息时效性不如内存模式，但影响不大。 2.4.RabbitMQ如何保证消息的有序性？ 话术： 其实RabbitMQ是队列存储，天然具备先进先出的特点，只要消息的发送是有序的，那么理论上接收也是有序的。不过当一个队列绑定了多个消费者时，可能出现消息轮询投递给消费者的情况，而消费者的处理顺序就无法保证了。 因此，要保证消息的有序性，需要做的下面几点：  保证消息发送的有序性 保证一组有序的消息都发送到同一个队列 保证一个队列只包含一个消费者 2.5.如何防止MQ消息被重复消费？ 话术： 消息重复消费的原因多种多样，不可避免。所以只能从消费者端入手，只要能保证消息处理的幂等性就可以确保消息不被重复消费。 而幂等性的保证又有很多方案：  给每一条消息都添加一个唯一id，在本地记录消息表及消息状态，处理消息时基于数据库表的id唯一性做判断 同样是记录消息表，利用消息状态字段实现基于乐观锁的判断，保证幂等 基于业务本身的幂等性。比如根据id的删除、查询业务天生幂等；新增、修改等业务可以考虑基于数据库id唯一性、或者乐观锁机制确保幂等。本质与消息表方案类似。 2.6.如何保证RabbitMQ的高可用？ 话术： 要实现RabbitMQ的高可用无外乎下面两点：  做好交换机、队列、消息的持久化 搭建RabbitMQ的镜像集群，做好主从备份。当然也可以使用仲裁队列代替镜像集群。 2.7.使用MQ可以解决那些问题？ 话术： RabbitMQ能解决的问题很多，例如：  解耦合：将几个业务关联的微服务调用修改为基于MQ的异步通知，可以解除微服务之间的业务耦合。同时还提高了业务性能。 流量削峰：将突发的业务请求放入MQ中，作为缓冲区。后端的业务根据自己的处理能力从MQ中获取消息，逐个处理任务。流量曲线变的平滑很多 延迟队列：基于RabbitMQ的死信队列或者DelayExchange插件，可以实现消息发送后，延迟接收的效果。 3.Redis篇 3.1.Redis与Memcache的区别？  redis支持更丰富的数据类型（支持更复杂的应用场景）：Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，String。 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中。 集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前是原生支持 cluster 模式的. Redis使用单线程：Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的多路 IO 复用模型。  3.2.Redis的单线程问题 面试官：Redis采用单线程，如何保证高并发？ 面试话术： Redis快的主要原因是：  完全基于内存 数据结构简单，对数据操作也简单 使用多路 I/O 复用模型，充分利用CPU资源 面试官：这样做的好处是什么？ 面试话术： 单线程优势有下面几点：  代码更清晰，处理逻辑更简单 不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为锁而导致的性能消耗 不存在多进程或者多线程导致的CPU切换，充分利用CPU资源 3.2.Redis的持久化方案由哪些？ 相关资料： 1）RDB 持久化 RDB持久化可以使用save或bgsave，为了不阻塞主进程业务，一般都使用bgsave，流程：  Redis 进程会 fork 出一个子进程（与父进程内存数据一致）。 父进程继续处理客户端请求命令 由子进程将内存中的所有数据写入到一个临时的 RDB 文件中。 完成写入操作之后，旧的 RDB 文件会被新的 RDB 文件替换掉。 下面是一些和 RDB 持久化相关的配置：  save 60 10000：如果在 60 秒内有 10000 个 key 发生改变，那就执行 RDB 持久化。 stop-writes-on-bgsave-error yes：如果 Redis 执行 RDB 持久化失败（常见于操作系统内存不足），那么 Redis 将不再接受 client 写入数据的请求。 rdbcompression yes：当生成 RDB 文件时，同时进行压缩。 dbfilename dump.rdb：将 RDB 文件命名为 dump.rdb。 dir /var/lib/redis：将 RDB 文件保存在/var/lib/redis目录下。 　　当然在实践中，我们通常会将stop-writes-on-bgsave-error设置为false，同时让监控系统在 Redis 执行 RDB 持久化失败时发送告警，以便人工介入解决，而不是粗暴地拒绝 client 的写入请求。 RDB持久化的优点：  RDB持久化文件小，Redis数据恢复时速度快 子进程不影响父进程，父进程可以持续处理客户端命令 子进程fork时采用copy-on-write方式，大多数情况下，没有太多的内存消耗，效率比较好。 RDB 持久化的缺点：  子进程fork时采用copy-on-write方式，如果Redis此时写操作较多，可能导致额外的内存占用，甚至内存溢出 RDB文件压缩会减小文件体积，但通过时会对CPU有额外的消耗 如果业务场景很看重数据的持久性 (durability)，那么不应该采用 RDB 持久化。譬如说，如果 Redis 每 5 分钟执行一次 RDB 持久化，要是 Redis 意外奔溃了，那么最多会丢失 5 分钟的数据。 2）AOF 持久化 　　可以使用appendonly yes配置项来开启 AOF 持久化。Redis 执行 AOF 持久化时，会将接收到的写命令追加到 AOF 文件的末尾，因此 Redis 只要对 AOF 文件中的命令进行回放，就可以将数据库还原到原先的状态。 　　与 RDB 持久化相比，AOF 持久化的一个明显优势就是，它可以提高数据的持久性 (durability)。因为在 AOF 模式下，Redis 每次接收到 client 的写命令，就会将命令write()到 AOF 文件末尾。 　　然而，在 Linux 中，将数据write()到文件后，数据并不会立即刷新到磁盘，而会先暂存在 OS 的文件系统缓冲区。在合适的时机，OS 才会将缓冲区的数据刷新到磁盘（如果需要将文件内容刷新到磁盘，可以调用fsync()或fdatasync()）。 　　通过appendfsync配置项，可以控制 Redis 将命令同步到磁盘的频率：  always：每次 Redis 将命令write()到 AOF 文件时，都会调用fsync()，将命令刷新到磁盘。这可以保证最好的数据持久性，但却会给系统带来极大的开销。 no：Redis 只将命令write()到 AOF 文件。这会让 OS 决定何时将命令刷新到磁盘。 everysec：除了将命令write()到 AOF 文件，Redis 还会每秒执行一次fsync()。在实践中，推荐使用这种设置，一定程度上可以保证数据持久性，又不会明显降低 Redis 性能。 　　然而，AOF 持久化并不是没有缺点的：Redis 会不断将接收到的写命令追加到 AOF 文件中，导致 AOF 文件越来越大。过大的 AOF 文件会消耗磁盘空间，并且导致 Redis 重启时更加缓慢。为了解决这个问题，在适当情况下，Redis 会对 AOF 文件进行重写，去除文件中冗余的命令，以减小 AOF 文件的体积。在重写 AOF 文件期间， Redis 会启动一个子进程，由子进程负责对 AOF 文件进行重写。 　　可以通过下面两个配置项，控制 Redis 重写 AOF 文件的频率：  auto-aof-rewrite-min-size 64mb auto-aof-rewrite-percentage 100 　　上面两个配置的作用：当 AOF 文件的体积大于 64MB，并且 AOF 文件的体积比上一次重写之后的体积大了至少一倍，那么 Redis 就会执行 AOF 重写。 优点：  持久化频率高，数据可靠性高 没有额外的内存或CPU消耗 缺点：  文件体积大 文件大导致服务数据恢复时效率较低 面试话术： Redis 提供了两种数据持久化的方式，一种是 RDB，另一种是 AOF。默认情况下，Redis 使用的是 RDB 持久化。 RDB持久化文件体积较小，但是保存数据的频率一般较低，可靠性差，容易丢失数据。另外RDB写数据时会采用Fork函数拷贝主进程，可能有额外的内存消耗，文件压缩也会有额外的CPU消耗。 ROF持久化可以做到每秒钟持久化一次，可靠性高。但是持久化文件体积较大，导致数据恢复时读取文件时间较长，效率略低 3.3.Redis的集群方式有哪些？ 面试话术： Redis集群可以分为主从集群和分片集群两类。 主从集群一般一主多从，主库用来写数据，从库用来读数据。结合哨兵，可以再主库宕机时从新选主，目的是保证Redis的高可用。 分片集群是数据分片，我们会让多个Redis节点组成集群，并将16383个插槽分到不同的节点上。存储数据时利用对key做hash运算，得到插槽值后存储到对应的节点即可。因为存储数据面向的是插槽而非节点本身，因此可以做到集群动态伸缩。目的是让Redis能存储更多数据。 1）主从集群 主从集群，也是读写分离集群。一般都是一主多从方式。 Redis 的复制（replication）功能允许用户根据一个 Redis 服务器来创建任意多个该服务器的复制品，其中被复制的服务器为主服务器（master），而通过复制创建出来的服务器复制品则为从服务器（slave）。 只要主从服务器之间的网络连接正常，主从服务器两者会具有相同的数据，主服务器就会一直将发生在自己身上的数据更新同步 给从服务器，从而一直保证主从服务器的数据相同。  写数据时只能通过主节点完成 读数据可以从任何节点完成 如果配置了哨兵节点，当master宕机时，哨兵会从salve节点选出一个新的主。 主从集群分两种：  带有哨兵的集群： 2）分片集群 主从集群中，每个节点都要保存所有信息，容易形成木桶效应。并且当数据量较大时，单个机器无法满足需求。此时我们就要使用分片集群了。 集群特征：  每个节点都保存不同数据  所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽.   节点的fail是通过集群中超过半数的节点检测失效时才生效.   客户端与redis节点直连,不需要中间proxy层连接集群中任何一个可用节点都可以访问到数据 redis-cluster把所有的物理节点映射到[0-16383]slot（插槽）上，实现动态伸缩 为了保证Redis中每个节点的高可用，我们还可以给每个节点创建replication（slave节点），如图： 出现故障时，主从可以及时切换： 3.4.Redis的常用数据类型有哪些？ 支持多种类型的数据结构，主要区别是value存储的数据格式不同：  string：最基本的数据类型，二进制安全的字符串，最大512M。 list：按照添加顺序保持顺序的字符串列表。 set：无序的字符串集合，不存在重复的元素。 sorted set：已排序的字符串集合。 hash：key-value对格式 3.5.聊一下Redis事务机制 相关资料： 参考：http://redisdoc.com/topic/transaction.html Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的。Redis会将一个事务中的所有命令序列化，然后按顺序执行。但是Redis事务不支持回滚操作，命令运行出错后，正确的命令会继续执行。  MULTI: 用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个待执行命令队列中 EXEC：按顺序执行命令队列内的所有命令。返回所有命令的返回值。事务执行过程中，Redis不会执行其它事务的命令。 DISCARD：清空命令队列，并放弃执行事务， 并且客户端会从事务状态中退出 WATCH：Redis的乐观锁机制，利用compare-and-set（CAS）原理，可以监控一个或多个键，一旦其中有一个键被修改，之后的事务就不会执行 使用事务时可能会遇上以下两种错误：  执行 EXEC 之前，入队的命令可能会出错。比如说，命令可能会产生语法错误（参数数量错误，参数名错误，等等），或者其他更严重的错误，比如内存不足（如果服务器使用 maxmemory 设置了最大内存限制的话）。  Redis 2.6.5 开始，服务器会对命令入队失败的情况进行记录，并在客户端调用 EXEC 命令时，拒绝执行并自动放弃这个事务。   命令可能在 EXEC 调用之后失败。举个例子，事务中的命令可能处理了错误类型的键，比如将列表命令用在了字符串键上面，诸如此类。  即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行，不会回滚。   为什么 Redis 不支持回滚（roll back）？ 以下是这种做法的优点：  Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。 鉴于没有任何机制能避免程序员自己造成的错误， 并且这类错误通常不会在生产环境中出现， 所以 Redis 选择了更简单、更快速的无回滚方式来处理事务。 面试话术： Redis事务其实是把一系列Redis命令放入队列，然后批量执行，执行过程中不会有其它事务来打断。不过与关系型数据库的事务不同，Redis事务不支持回滚操作，事务中某个命令执行失败，其它命令依然会执行。 为了弥补不能回滚的问题，Redis会在事务入队时就检查命令，如果命令异常则会放弃整个事务。 因此，只要程序员编程是正确的，理论上说Redis会正确执行所有事务，无需回滚。 面试官：如果事务执行一半的时候Redis宕机怎么办？ Redis有持久化机制，因为可靠性问题，我们一般使用AOF持久化。事务的所有命令也会写入AOF文件，但是如果在执行EXEC命令之前，Redis已经宕机，则AOF文件中事务不完整。使用 redis-check-aof 程序可以移除 AOF 文件中不完整事务的信息，确保服务器可以顺利启动。 3.6.Redis的Key过期策略 参考资料： 为什么需要内存回收？  1、在Redis中，set指令可以指定key的过期时间，当过期时间到达以后，key就失效了； 2、Redis是基于内存操作的，所有的数据都是保存在内存中，一台机器的内存是有限且很宝贵的。 基于以上两点，为了保证Redis能继续提供可靠的服务，Redis需要一种机制清理掉不常用的、无效的、多余的数据，失效后的数据需要及时清理，这就需要内存回收了。 Redis的内存回收主要分为过期删除策略和内存淘汰策略两部分。 过期删除策略 删除达到过期时间的key。  1）定时删除 对于每一个设置了过期时间的key都会创建一个定时器，一旦到达过期时间就立即删除。该策略可以立即清除过期的数据，对内存较友好，但是缺点是占用了大量的CPU资源去处理过期的数据，会影响Redis的吞吐量和响应时间。  2）惰性删除 当访问一个key时，才判断该key是否过期，过期则删除。该策略能最大限度地节省CPU资源，但是对内存却十分不友好。有一种极端的情况是可能出现大量的过期key没有被再次访问，因此不会被清除，导致占用了大量的内存。  在计算机科学中，懒惰删除（英文：lazy deletion）指的是从一个散列表（也称哈希表）中删除元素的一种方法。在这个方法中，删除仅仅是指标记一个元素被删除，而不是整个清除它。被删除的位点在插入时被当作空元素，在搜索之时被当作已占据。 3）定期删除 每隔一段时间，扫描Redis中过期key字典，并清除部分过期的key。该策略是前两者的一个折中方案，还可以通过调整定时扫描的时间间隔和每次扫描的限定耗时，在不同情况下使得CPU和内存资源达到最优的平衡效果。 在Redis中，同时使用了定期删除和惰性删除。不过Redis定期删除采用的是随机抽取的方式删除部分Key，因此不能保证过期key 100%的删除。 Redis结合了定期删除和惰性删除，基本上能很好的处理过期数据的清理，但是实际上还是有点问题的，如果过期key较多，定期删除漏掉了一部分，而且也没有及时去查，即没有走惰性删除，那么就会有大量的过期key堆积在内存中，导致redis内存耗尽，当内存耗尽之后，有新的key到来会发生什么事呢？是直接抛弃还是其他措施呢？有什么办法可以接受更多的key？ 内存淘汰策略 Redis的内存淘汰策略，是指内存达到maxmemory极限时，使用某种算法来决定清理掉哪些数据，以保证新数据的存入。 Redis的内存淘汰机制包括：  noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间（server.db[i].dict）中，移除最近最少使用的 key（这个是最常用的）。 allkeys-random：当内存不足以容纳新写入数据时，在键空间（server.db[i].dict）中，随机移除某个 key。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（server.db[i].expires）中，移除最近最少使用的 key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（server.db[i].expires）中，随机移除某个 key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间（server.db[i].expires）中，有更早过期时间的 key 优先移除。 在配置文件中，通过maxmemory-policy可以配置要使用哪一个淘汰机制。 什么时候会进行淘汰？ Redis会在每一次处理命令的时候（processCommand函数调用freeMemoryIfNeeded）判断当前redis是否达到了内存的最大限制，如果达到限制，则使用对应的算法去处理需要删除的key。 在淘汰key时，Redis默认最常用的是LRU算法（Latest Recently Used）。Redis通过在每一个redisObject保存lru属性来保存key最近的访问时间，在实现LRU算法时直接读取key的lru属性。 具体实现时，Redis遍历每一个db，从每一个db中随机抽取一批样本key，默认是3个key，再从这3个key中，删除最近最少使用的key。 面试话术： Redis过期策略包含定期删除和惰性删除两部分。定期删除是在Redis内部有一个定时任务，会定期删除一些过期的key。惰性删除是当用户查询某个Key时，会检查这个Key是否已经过期，如果没过期则返回用户，如果过期则删除。 但是这两个策略都无法保证过期key一定删除，漏网之鱼越来越多，还可能导致内存溢出。当发生内存不足问题时，Redis还会做内存回收。内存回收采用LRU策略，就是最近最少使用。其原理就是记录每个Key的最近使用时间，内存回收时，随机抽取一些Key，比较其使用时间，把最老的几个删除。 Redis的逻辑是：最近使用过的，很可能再次被使用 3.7.Redis在项目中的哪些地方有用到? （1）共享session 在分布式系统下，服务会部署在不同的tomcat，因此多个tomcat的session无法共享，以前存储在session中的数据无法实现共享，可以用redis代替session，解决分布式系统间数据共享问题。 （2）数据缓存 Redis采用内存存储，读写效率较高。我们可以把数据库的访问频率高的热点数据存储到redis中，这样用户请求时优先从redis中读取，减少数据库压力，提高并发能力。 （3）异步队列 Reids在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得Redis能作为一个很好的消息队列平台来使用。而且Redis中还有pub/sub这样的专用结构，用于1对N的消息通信模式。 （4）分布式锁 Redis中的乐观锁机制，可以帮助我们实现分布式锁的效果，用于解决分布式系统下的多线程安全问题 3.8.Redis的缓存击穿、缓存雪崩、缓存穿透 1）缓存穿透 参考资料：  什么是缓存穿透  正常情况下，我们去查询数据都是存在。那么请求去查询一条压根儿数据库中根本就不存在的数据，也就是缓存和数据库都查询不到这条数据，但是请求每次都会打到数据库上面去。这种查询不存在数据的现象我们称为缓存穿透。   穿透带来的问题  试想一下，如果有黑客会对你的系统进行攻击，拿一个不存在的id 去查询数据，会产生大量的请求到数据库去查询。可能会导致你的数据库由于压力过大而宕掉。   解决办法  缓存空值：之所以会发生穿透，就是因为缓存中没有存储这些空数据的key。从而导致每次查询都到数据库去了。那么我们就可以为这些key对应的值设置为null 丢到缓存里面去。后面再出现查询这个key 的请求的时候，直接返回null 。这样，就不用在到数据库中去走一圈了，但是别忘了设置过期时间。  BloomFilter（布隆过滤）：将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。在缓存之前在加一层 BloomFilter ，在查询的时候先去 BloomFilter 去查询 key 是否存在，如果不存在就直接返回，存在再走查缓存 -&gt; 查 DB。   话术： 缓存穿透有两种解决方案：其一是把不存在的key设置null值到缓存中。其二是使用布隆过滤器，在查询缓存前先通过布隆过滤器判断key是否存在，存在再去查询缓存。 设置null值可能被恶意针对，攻击者使用大量不存在的不重复key ，那么方案一就会缓存大量不存在key数据。此时我们还可以对Key规定格式模板，然后对不存在的key做正则规范匹配，如果完全不符合就不用存null值到redis，而是直接返回错误。 2）缓存击穿 相关资料：  什么是缓存击穿？ key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题。 当这个key在失效的瞬间，redis查询失败，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞。  解决方案：  使用互斥锁(mutex key)：mutex，就是互斥。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用Redis的SETNX去set一个互斥key，当操作返回成功时，再进行load db的操作并回设缓存；否则，就重试整个get缓存的方法。SETNX，是「SET if Not eXists」的缩写，也就是只有不存在的时候才设置，可以利用它来实现互斥的效果。  软过期：也就是逻辑过期，不使用redis提供的过期时间，而是业务层在数据中存储过期时间信息。查询时由业务程序判断是否过期，如果数据即将过期时，将缓存的时效延长，程序可以派遣一个线程去数据库中获取最新的数据，其他线程这时看到延长了的过期时间，就会继续使用旧数据，等派遣的线程获取最新数据后再更新缓存。   推荐使用互斥锁，因为软过期会有业务逻辑侵入和额外的判断。 面试话术： 缓存击穿主要担心的是某个Key过期，更新缓存时引起对数据库的突发高并发访问。因此我们可以在更新缓存时采用互斥锁控制，只允许一个线程去更新缓存，其它线程等待并重新读取缓存。例如Redis的setnx命令就能实现互斥效果。 3）缓存雪崩 相关资料： 缓存雪崩，是指在某一个时间段，缓存集中过期失效。对这批数据的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰。 解决方案：  数据分类分批处理：采取不同分类数据，缓存不同周期 相同分类数据：采用固定时长加随机数方式设置缓存 热点数据缓存时间长一些，冷门数据缓存时间短一些 避免redis节点宕机引起雪崩，搭建主从集群，保证高可用 面试话术： 解决缓存雪崩问题的关键是让缓存Key的过期时间分散。因此我们可以把数据按照业务分类，然后设置不同过期时间。相同业务类型的key，设置固定时长加随机数。尽可能保证每个Key的过期时间都不相同。 另外，Redis宕机也可能导致缓存雪崩，因此我们还要搭建Redis主从集群及哨兵监控，保证Redis的高可用。 3.9.缓存冷热数据分离 背景资料： Redis使用的是内存存储，当需要海量数据存储时，成本非常高。 经过调研发现，当前主流DDR3内存和主流SATA SSD的单位成本价格差距大概在20倍左右，为了优化redis机器综合成本，我们考虑实现基于热度统计 的数据分级存储及数据在RAM/FLASH之间的动态交换，从而大幅度降低成本，达到性能与成本的高平衡。 基本思路：基于key访问次数(LFU)的热度统计算法识别出热点数据，并将热点数据保留在redis中，对于无访问/访问次数少的数据则转存到SSD上，如果SSD上的key再次变热，则重新将其加载到redis内存中。 目前流行的高性能磁盘存储，并且遵循Redis协议的方案包括：  SSDB：http://ssdb.io/zh_cn/ RocksDB：https://rocksdb.org.cn/ 因此，我们就需要在应用程序与缓存服务之间引入代理，实现Redis和SSD之间的切换，如图： 这样的代理方案阿里云提供的就有。当然也有一些开源方案，例如：https://github.com/JingchengLi/swapdb 3.10.Redis实现分布式锁 分布式锁要满足的条件：  多进程互斥：同一时刻，只有一个进程可以获取锁 保证锁可以释放：任务结束或出现异常，锁一定要释放，避免死锁 阻塞锁（可选）：获取锁失败时可否重试 重入锁（可选）：获取锁的代码递归调用时，依然可以获取锁 1）最基本的分布式锁： 利用Redis的setnx命令，这个命令的特征时如果多次执行，只有第一次执行会成功，可以实现互斥的效果。但是为了保证服务宕机时也可以释放锁，需要利用expire命令给锁设置一个有效期 setnx lock thread-01 # 尝试获取锁 expire lock 10 # 设置有效期 面试官问题1：如果expire之前服务宕机怎么办？ 要保证setnx和expire命令的原子性。redis的set命令可以满足： set key value [NX] [EX time] 需要添加nx和ex的选项：  NX：与setnx一致，第一次执行成功 EX：设置过期时间 面试官问题2：释放锁的时候，如果自己的锁已经过期了，此时会出现安全漏洞，如何解决？ 在锁中存储当前进程和线程标识，释放锁时对锁的标识判断，如果是自己的则删除，不是则放弃操作。 但是这两步操作要保证原子性，需要通过Lua脚本来实现。 if redis.call(get,KEYS[1]) == ARGV[1] then  redis.call(del,KEYS[1]) end 2）可重入分布式锁 如果有重入的需求，则除了在锁中记录进程标识，还要记录重试次数，流程如下： 下面我们假设锁的key为“lock”，hashKey是当前线程的id：“threadId”，锁自动释放时间假设为20 获取锁的步骤：  1、判断lock是否存在 EXISTS lock  存在，说明有人获取锁了，下面判断是不是自己的锁    判断当前线程id作为hashKey是否存在：HEXISTS lock threadId     不存在，说明锁已经有了，且不是自己获取的，锁获取失败，end   存在，说明是自己获取的锁，重入次数+1：HINCRBY lock threadId 1，去到步骤3         2、不存在，说明可以获取锁，HSET key threadId 1  3、设置锁自动释放时间，EXPIRE lock 20   释放锁的步骤：  1、判断当前线程id作为hashKey是否存在：HEXISTS lock threadId  不存在，说明锁已经失效，不用管了  存在，说明锁还在，重入次数减1：HINCRBY lock threadId -1，获取新的重入次数   2、判断重入次数是否为0：  为0，说明锁全部释放，删除key：DEL lock  大于0，说明锁还在使用，重置有效时间：EXPIRE lock 20   对应的Lua脚本如下： 首先是获取锁： local key = KEYS[1]; -- 锁的key local threadId = ARGV[1]; -- 线程唯一标识 local releaseTime = ARGV[2]; -- 锁的自动释放时间 if(redis.call('exists', key) == 0) then -- 判断是否存在  tredis.call('hset', key, threadId, '1'); -- 不存在, 获取锁  tredis.call('expire', key, releaseTime); -- 设置有效期  treturn 1; -- 返回结果 end; if(redis.call('hexists', key, threadId) == 1) then -- 锁已经存在，判断threadId是否是自己 t  tredis.call('hincrby', key, threadId, '1'); -- 不存在, 获取锁，重入次数+1  tredis.call('expire', key, releaseTime); -- 设置有效期  treturn 1; -- 返回结果 end; return 0; -- 代码走到这里,说明获取锁的不是自己，获取锁失败 然后是释放锁： local key = KEYS[1]; -- 锁的key local threadId = ARGV[1]; -- 线程唯一标识 local releaseTime = ARGV[2]; -- 锁的自动释放时间 if (redis.call('HEXISTS', key, threadId) == 0) then -- 判断当前锁是否还是被自己持有  return nil; -- 如果已经不是自己，则直接返回 end; local count = redis.call('HINCRBY', key, threadId, -1); -- 是自己的锁，则重入次数-1 if (count &gt; 0) then -- 判断是否重入次数是否已经为0  redis.call('EXPIRE', key, releaseTime); -- 大于0说明不能释放锁，重置有效期然后返回  return nil; else  redis.call('DEL', key); -- 等于0说明可以释放锁，直接删除  return nil; end; 3）高可用的锁 面试官问题：redis分布式锁依赖与redis，如果redis宕机则锁失效。如何解决？ 此时大多数同学会回答说：搭建主从集群，做数据备份。 这样就进入了陷阱，因为面试官的下一个问题就来了： 面试官问题：如果搭建主从集群做数据备份时，进程A获取锁，master还没有把数据备份到slave，master宕机，slave升级为master，此时原来锁失效，其它进程也可以获取锁，出现安全问题。如何解决？ 关于这个问题，Redis官网给出了解决方案，使用RedLock思路可以解决：  在Redis的分布式环境中，我们假设有N个Redis master。这些节点完全互相独立，不存在主从复制或者其他集群协调机制。之前我们已经描述了在Redis单实例下怎么安全地获取和释放锁。我们确保将在每（N)个实例上使用此方法获取和释放锁。在这个样例中，我们假设有5个Redis master节点，这是一个比较合理的设置，所以我们需要在5台机器上面或者5台虚拟机上面运行这些实例，这样保证他们不会同时都宕掉。 为了取到锁，客户端应该执行以下操作:  获取当前Unix时间，以毫秒为单位。  依次尝试从N个实例，使用相同的key和随机值获取锁。在步骤2，当向Redis设置锁时,客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间。例如你的锁自动失效时间为10秒，则超时时间应该在5-50毫秒之间。这样可以避免服务器端Redis已经挂掉的情况下，客户端还在死死地等待响应结果。如果服务器端没有在规定时间内响应，客户端应该尽快尝试另外一个Redis实例。  客户端使用当前时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间。当且仅当从大多数（这里是3个节点）的Redis节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功。  如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果）。  如果因为某些原因，获取锁失败（没有在至少N/2+1个Redis实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁（即便某些Redis实例根本就没有加锁成功）。 3.11.如何实现数据库与缓存数据一致？ 面试话术： 实现方案有下面几种：  本地缓存同步：当前微服务的数据库数据与缓存数据同步，可以直接在数据库修改时加入对Redis的修改逻辑，保证一致。 跨服务缓存同步：服务A调用了服务B，并对查询结果缓存。服务B数据库修改，可以通过MQ通知服务A，服务A修改Redis缓存数据 通用方案：使用Canal框架，伪装成MySQL的salve节点，监听MySQL的binLog变化，然后修改Redis缓存数据 ",
      "url"      : "http://zhangjinmiao.github.io/microservice/2022/08/16/microservice-qa.html",
      "keywords" : "微服务, 面试, spring cloud, mq, redis"
    } ,
  
    {
      "title"    : "JVM 面试问题集锦",
      "category" : "Interview",
      "content": " 基础篇要点：算法、数据结构、基础设计模式 1. 二分查找 要求  能够用自己语言描述二分查找算法 能够手写二分查找代码 能够解答一些变化后的考法 算法描述  前提：有已排序数组 A（假设已经做好）   定义左边界 L、右边界 R，确定搜索范围，循环执行二分查找（3、4两步）   获取中间索引 M = Floor((L+R) /2)   中间索引的值 A[M] 与待搜索的值 T 进行比较 ① A[M] == T 表示找到，返回中间索引 ② A[M] &gt; T，中间值右侧的其它元素都大于 T，无需比较，中间索引左边去找，M - 1 设置为右边界，重新查找 ③ A[M] &lt; T，中间值左侧的其它元素都小于 T，无需比较，中间索引右边去找， M + 1 设置为左边界，重新查找   当 L &gt; R 时，表示没有找到，应结束循环  更形象的描述请参考：binary_search.html 算法实现 public static int binarySearch(int[] a, int t) {  int l = 0, r = a.length - 1, m;  while (l &lt;= r) {  m = (l + r) / 2;  if (a[m] == t) {   return m;  } else if (a[m] &gt; t) {   r = m - 1;  } else {   l = m + 1;  }  }  return -1; } 测试代码 public static void main(String[] args) {  int[] array = {1, 5, 8, 11, 19, 22, 31, 35, 40, 45, 48, 49, 50};  int target = 47;  int idx = binarySearch(array, target);  System.out.println(idx); } 解决整数溢出问题 当 l 和 r 都较大时，l + r 有可能超过整数范围，造成运算错误，解决方法有两种： int m = l + (r - l) / 2; 还有一种是： int m = (l + r) &gt;&gt;&gt; 1; 其它考法  有一个有序表为 1,5,8,11,19,22,31,35,40,45,48,49,50 当二分查找值为 48 的结点时，查找成功需要比较的次数   使用二分法在序列 1,4,6,7,15,33,39,50,64,78,75,81,89,96 中查找元素 81 时，需要经过（ ）次比较   在拥有128个元素的数组中二分查找一个数，需要比较的次数最多不超过多少次 对于前两个题目，记得一个简要判断口诀：奇数二分取中间，偶数二分取中间靠左。对于后一道题目，需要知道公式： [n = log_2N = log_{10}N/log_{10}2] 其中 n 为查找次数，N 为元素个数 2. 冒泡排序 要求  能够用自己语言描述冒泡排序算法 能够手写冒泡排序代码 了解一些冒泡排序的优化手段 算法描述  依次比较数组中相邻两个元素大小，若 a[j] &gt; a[j+1]，则交换两个元素，两两都比较一遍称为一轮冒泡，结果是让最大的元素排至最后 重复以上步骤，直到整个数组有序 更形象的描述请参考：bubble_sort.html 算法实现 public static void bubble(int[] a) {  for (int j = 0; j &lt; a.length - 1; j++) {  // 一轮冒泡  boolean swapped = false; // 是否发生了交换  for (int i = 0; i &lt; a.length - 1 - j; i++) {   System.out.println(比较次数 + i);   if (a[i] &gt; a[i + 1]) {    Utils.swap(a, i, i + 1);    swapped = true;   }  }  System.out.println(第 + j + 轮冒泡     + Arrays.toString(a));  if (!swapped) {   break;  }  } } 优化点1：每经过一轮冒泡，内层循环就可以减少一次 优化点2：如果某一轮冒泡没有发生交换，则表示所有数据有序，可以结束外层循环 进一步优化 public static void bubble_v2(int[] a) {  int n = a.length - 1;  while (true) {  int last = 0; // 表示最后一次交换索引位置  for (int i = 0; i &lt; n; i++) {   System.out.println(比较次数 + i);   if (a[i] &gt; a[i + 1]) {    Utils.swap(a, i, i + 1);    last = i;   }  }  n = last;  System.out.println(第轮冒泡     + Arrays.toString(a));  if (n == 0) {   break;  }  } } 每轮冒泡时，最后一次交换索引可以作为下一轮冒泡的比较次数，如果这个值为零，表示整个数组有序，直接退出外层循环即可 3. 选择排序 要求  能够用自己语言描述选择排序算法 能够比较选择排序与冒泡排序 理解非稳定排序与稳定排序 算法描述  将数组分为两个子集，排序的和未排序的，每一轮从未排序的子集中选出最小的元素，放入排序子集   重复以上步骤，直到整个数组有序  更形象的描述请参考：selection_sort.html 算法实现 public static void selection(int[] a) {  for (int i = 0; i &lt; a.length - 1; i++) {  // i 代表每轮选择最小元素要交换到的目标索引  int s = i; // 代表最小元素的索引  for (int j = s + 1; j &lt; a.length; j++) {   if (a[s] &gt; a[j]) { // j 元素比 s 元素还要小, 更新 s    s = j;   }  }  if (s != i) {   swap(a, s, i);  }  System.out.println(Arrays.toString(a));  } } 优化点：为减少交换次数，每一轮可以先找最小的索引，在每轮最后再交换元素 与冒泡排序比较  二者平均时间复杂度都是 $O(n^2)$   选择排序一般要快于冒泡，因为其交换次数少   但如果集合有序度高，冒泡优于选择   冒泡属于稳定排序算法，而选择属于不稳定排序  稳定排序指，按对象中不同字段进行多次排序，不会打乱同值元素的顺序  不稳定排序则反之   稳定排序与不稳定排序 System.out.println(=================不稳定================); Card[] cards = getStaticCards(); System.out.println(Arrays.toString(cards)); selection(cards, Comparator.comparingInt((Card a) -&gt; a.sharpOrder).reversed()); System.out.println(Arrays.toString(cards)); selection(cards, Comparator.comparingInt((Card a) -&gt; a.numberOrder).reversed()); System.out.println(Arrays.toString(cards)); System.out.println(=================稳定=================); cards = getStaticCards(); System.out.println(Arrays.toString(cards)); bubble(cards, Comparator.comparingInt((Card a) -&gt; a.sharpOrder).reversed()); System.out.println(Arrays.toString(cards)); bubble(cards, Comparator.comparingInt((Card a) -&gt; a.numberOrder).reversed()); System.out.println(Arrays.toString(cards)); 都是先按照花色排序（♠♥♣♦），再按照数字排序（AKQJ…）  不稳定排序算法按数字排序时，会打乱原本同值的花色顺序 [[♠7], [♠2], [♠4], [♠5], [♥2], [♥5]] [[♠7], [♠5], [♥5], [♠4], [♥2], [♠2]]   原来 ♠2 在前 ♥2 在后，按数字再排后，他俩的位置变了   稳定排序算法按数字排序时，会保留原本同值的花色顺序，如下所示 ♠2 与 ♥2 的相对位置不变 [[♠7], [♠2], [♠4], [♠5], [♥2], [♥5]] [[♠7], [♠5], [♥5], [♠4], [♠2], [♥2]]   4. 插入排序 要求  能够用自己语言描述插入排序算法 能够比较插入排序与选择排序 算法描述  将数组分为两个区域，排序区域和未排序区域，每一轮从未排序区域中取出第一个元素，插入到排序区域（需保证顺序）   重复以上步骤，直到整个数组有序  更形象的描述请参考：insertion_sort.html 算法实现 // 修改了代码与希尔排序一致 public static void insert(int[] a) {  // i 代表待插入元素的索引  for (int i = 1; i &lt; a.length; i++) {  int t = a[i]; // 代表待插入的元素值  int j = i;  System.out.println(j);  while (j &gt;= 1) {   if (t &lt; a[j - 1]) { // j-1 是上一个元素索引，如果 &gt; t，后移    a[j] = a[j - 1];    j--;   } else { // 如果 j-1 已经 &lt;= t, 则 j 就是插入位置    break;   }  }  a[j] = t;  System.out.println(Arrays.toString(a) + + j);  } } 与选择排序比较  二者平均时间复杂度都是 $O(n^2)$   大部分情况下，插入都略优于选择   有序集合插入的时间复杂度为 $O(n)$   插入属于稳定排序算法，而选择属于不稳定排序 提示  插入排序通常被同学们所轻视，其实它的地位非常重要。小数据量排序，都会优先选择插入排序 5. 希尔排序 要求  能够用自己语言描述希尔排序算法 算法描述  首先选取一个间隙序列，如 (n/2，n/4 … 1)，n 为数组长度   每一轮将间隙相等的元素视为一组，对组内元素进行插入排序，目的有二 ① 少量元素插入排序速度很快 ② 让组内值较大的元素更快地移动到后方   当间隙逐渐减少，直至为 1 时，即可完成排序  更形象的描述请参考：shell_sort.html 算法实现 private static void shell(int[] a) {  int n = a.length;  for (int gap = n / 2; gap &gt; 0; gap /= 2) {  // i 代表待插入元素的索引  for (int i = gap; i &lt; n; i++) {   int t = a[i]; // 代表待插入的元素值   int j = i;   while (j &gt;= gap) {    // 每次与上一个间隙为 gap 的元素进行插入排序    if (t &lt; a[j - gap]) { // j-gap 是上一个元素索引，如果 &gt; t，后移    a[j] = a[j - gap];    j -= gap;    } else { // 如果 j-1 已经 &lt;= t, 则 j 就是插入位置    break;    }   }   a[j] = t;   System.out.println(Arrays.toString(a) + gap: + gap);  }  } } 参考资料  https://en.wikipedia.org/wiki/Shellsort 6. 快速排序 要求  能够用自己语言描述快速排序算法 掌握手写单边循环、双边循环代码之一 能够说明快排特点 了解洛穆托与霍尔两种分区方案的性能比较 算法描述  每一轮排序选择一个基准点（pivot）进行分区  让小于基准点的元素的进入一个分区，大于基准点的元素的进入另一个分区  当分区完成时，基准点元素的位置就是其最终位置   在子分区内重复以上过程，直至子分区元素个数少于等于 1，这体现的是分而治之的思想 （divide-and-conquer） 从以上描述可以看出，一个关键在于分区算法，常见的有洛穆托分区方案、双边循环分区方案、霍尔分区方案 更形象的描述请参考：quick_sort.html 单边循环快排（lomuto 洛穆托分区方案）  选择最右元素作为基准点元素   j 指针负责找到比基准点小的元素，一旦找到则与 i 进行交换   i 指针维护小于基准点元素的边界，也是每次交换的目标索引   最后基准点与 i 交换，i 即为分区位置 public static void quick(int[] a, int l, int h) {  if (l &gt;= h) {  return;  }  int p = partition(a, l, h); // p 索引值  quick(a, l, p - 1); // 左边分区的范围确定  quick(a, p + 1, h); // 左边分区的范围确定 } private static int partition(int[] a, int l, int h) {  int pv = a[h]; // 基准点元素  int i = l;  for (int j = l; j &lt; h; j++) {  if (a[j] &lt; pv) {   if (i != j) {    swap(a, i, j);   }   i++;  }  }  if (i != h) {  swap(a, h, i);  }  System.out.println(Arrays.toString(a) + i= + i);  // 返回值代表了基准点元素所在的正确索引，用它确定下一轮分区的边界  return i; } 双边循环快排（不完全等价于 hoare 霍尔分区方案）  选择最左元素作为基准点元素 j 指针负责从右向左找比基准点小的元素，i 指针负责从左向右找比基准点大的元素，一旦找到二者交换，直至 i，j 相交 最后基准点与 i（此时 i 与 j 相等）交换，i 即为分区位置 要点  基准点在左边，并且要先 j 后 i while( i &lt; j &amp;&amp; a[j] &gt; pv ) j– while ( i &lt; j &amp;&amp; a[i] &lt;= pv ) i++ private static void quick(int[] a, int l, int h) {  if (l &gt;= h) {  return;  }  int p = partition(a, l, h);  quick(a, l, p - 1);  quick(a, p + 1, h); } private static int partition(int[] a, int l, int h) {  int pv = a[l];  int i = l;  int j = h;  while (i &lt; j) {  // j 从右找小的  while (i &lt; j &amp;&amp; a[j] &gt; pv) {   j--;  }  // i 从左找大的  while (i &lt; j &amp;&amp; a[i] &lt;= pv) {   i++;  }  swap(a, i, j);  }  swap(a, l, j);  System.out.println(Arrays.toString(a) + j= + j);  return j; } 快排特点  平均时间复杂度是 $O(nlog_2⁡n )$，最坏时间复杂度 $O(n^2)$   数据量较大时，优势非常明显   属于不稳定排序 洛穆托分区方案 vs 霍尔分区方案  霍尔的移动次数平均来讲比洛穆托少3倍 https://qastack.cn/cs/11458/quicksort-partitioning-hoare-vs-lomuto 补充代码说明  day01.sort.QuickSort3 演示了空穴法改进的双边快排，比较次数更少  day01.sort.QuickSortHoare 演示了霍尔分区的实现  day01.sort.LomutoVsHoare 对四种分区实现的移动次数比较 7. ArrayList 要求  掌握 ArrayList 扩容规则 扩容规则  ArrayList() 会使用长度为零的数组   ArrayList(int initialCapacity) 会使用指定容量的数组   public ArrayList(Collection&lt;? extends E&gt; c) 会使用 c 的大小作为数组容量   add(Object o) 首次扩容为 10，再次扩容为上次容量的 1.5 倍   addAll(Collection c) 没有元素时，扩容为 Math.max(10, 实际元素个数)，有元素时为 Math.max(原容量 1.5 倍, 实际元素个数) 其中第 4 点必须知道，其它几点视个人情况而定 提示  测试代码见 day01.list.TestArrayList ，这里不再列出 要注意的是，示例中用反射方式来更直观地反映 ArrayList 的扩容特征，但从 JDK 9 由于模块化的影响，对反射做了较多限制，需要在运行测试代码时添加 VM 参数 --add-opens java.base/java.util=ALL-UNNAMED 方能运行通过，后面的例子都有相同问题 代码说明  day01.list.TestArrayList#arrayListGrowRule 演示了 add(Object) 方法的扩容规则，输入参数 n 代表打印多少次扩容后的数组长度 8. Iterator 要求  掌握什么是 Fail-Fast、什么是 Fail-Safe Fail-Fast 与 Fail-Safe  ArrayList 是 fail-fast 的典型代表，遍历的同时不能修改，尽快失败   CopyOnWriteArrayList 是 fail-safe 的典型代表，遍历的同时可以修改，原理是读写分离 提示  测试代码见 day01.list.FailFastVsFailSafe，这里不再列出 9. LinkedList 要求  能够说清楚 LinkedList 对比 ArrayList 的区别，并重视纠正部分错误的认知 LinkedList  基于双向链表，无需连续内存 随机访问慢（要沿着链表遍历） 头尾插入删除性能高 占用内存多 ArrayList  基于数组，需要连续内存 随机访问快（指根据下标访问） 尾部插入、删除性能可以，其它部分插入、删除都会移动数据，因此性能会低 可以利用 cpu 缓存，局部性原理 代码说明  day01.list.ArrayListVsLinkedList#randomAccess 对比随机访问性能  day01.list.ArrayListVsLinkedList#addMiddle 对比向中间插入性能  day01.list.ArrayListVsLinkedList#addFirst 对比头部插入性能  day01.list.ArrayListVsLinkedList#addLast 对比尾部插入性能  day01.list.ArrayListVsLinkedList#linkedListSize 打印一个 LinkedList 占用内存  day01.list.ArrayListVsLinkedList#arrayListSize 打印一个 ArrayList 占用内存 10. HashMap 要求  掌握 HashMap 的基本数据结构 掌握树化 理解索引计算方法、二次 hash 的意义、容量对索引计算的影响 掌握 put 流程、扩容、扩容因子 理解并发使用 HashMap 可能导致的问题 理解 key 的设计 1）基本数据结构  1.7 数组 + 链表       1.8 数组 + （链表   红黑树）     更形象的演示，见资料中的 hash-demo.jar，运行需要 jdk14 以上环境，进入 jar 包目录，执行下面命令 java -jar --add-exports java.base/jdk.internal.misc=ALL-UNNAMED hash-demo.jar 2）树化与退化 树化意义  红黑树用来避免 DoS 攻击，防止链表超长时性能下降，树化应当是偶然情况，是保底策略 hash 表的查找，更新的时间复杂度是 $O(1)$，而红黑树的查找，更新的时间复杂度是 $O(log_2⁡n )$，TreeNode 占用空间也比普通 Node 的大，如非必要，尽量还是使用链表 hash 值如果足够随机，则在 hash 表内按泊松分布，在负载因子 0.75 的情况下，长度超过 8 的链表出现概率是 0.00000006，树化阈值选择 8 就是为了让树化几率足够小 树化规则  当链表长度超过树化阈值 8 时，先尝试扩容来减少链表长度，如果数组容量已经 &gt;=64，才会进行树化 退化规则  情况1：在扩容时如果拆分树时，树元素个数 &lt;= 6 则会退化链表 情况2：remove 树节点时，若 root、root.left、root.right、root.left.left 有一个为 null ，也会退化为链表 3）索引计算 索引计算方法  首先，计算对象的 hashCode() 再进行调用 HashMap 的 hash() 方法进行二次哈希  二次 hash() 是为了综合高位数据，让哈希分布更为均匀   最后 &amp; (capacity – 1) 得到索引 数组容量为何是 2 的 n 次幂  计算索引时效率更高：如果是 2 的 n 次幂可以使用位与运算代替取模 扩容时重新计算索引效率更高： hash &amp; oldCap == 0 的元素留在原来位置 ，否则新位置 = 旧位置 + oldCap 注意  二次 hash 是为了配合 容量是 2 的 n 次幂 这一设计前提，如果 hash 表的容量不是 2 的 n 次幂，则不必二次 hash 容量是 2 的 n 次幂 这一设计计算索引效率更好，但 hash 的分散性就不好，需要二次 hash 来作为补偿，没有采用这一设计的典型例子是 Hashtable 4）put 与扩容 put 流程  HashMap 是懒惰创建数组的，首次使用才创建数组 计算索引（桶下标） 如果桶下标还没人占用，创建 Node 占位返回 如果桶下标已经有人占用  已经是 TreeNode 走红黑树的添加或更新逻辑  是普通 Node，走链表的添加或更新逻辑，如果链表长度超过树化阈值，走树化逻辑   返回前检查容量是否超过阈值，一旦超过进行扩容 1.7 与 1.8 的区别  链表插入节点时，1.7 是头插法，1.8 是尾插法   1.7 是大于等于阈值且没有空位时才扩容，而 1.8 是大于阈值就扩容   1.8 在扩容计算 Node 索引时，会优化 扩容（加载）因子为何默认是 0.75f  在空间占用与查询时间之间取得较好的权衡 大于这个值，空间节省了，但链表就会比较长影响性能 小于这个值，冲突减少了，但扩容就会更频繁，空间占用也更多 5）并发问题 扩容死链（1.7 会存在） 1.7 源码如下： void transfer(Entry[] newTable, boolean rehash) {  int newCapacity = newTable.length;  for (Entry&lt;K,V&gt; e : table) {  while(null != e) {   Entry&lt;K,V&gt; next = e.next;   if (rehash) {    e.hash = null == e.key ? 0 : hash(e.key);   }   int i = indexFor(e.hash, newCapacity);   e.next = newTable[i];   newTable[i] = e;   e = next;  }  } } e 和 next 都是局部变量，用来指向当前节点和下一个节点 线程1（绿色）的临时变量 e 和 next 刚引用了这俩节点，还未来得及移动节点，发生了线程切换，由线程2（蓝色）完成扩容和迁移  线程2 扩容完成，由于头插法，链表顺序颠倒。但线程1 的临时变量 e 和 next 还引用了这俩节点，还要再来一遍迁移  第一次循环  循环接着线程切换前运行，注意此时 e 指向的是节点 a，next 指向的是节点 b  e 头插 a 节点，注意图中画了两份 a 节点，但事实上只有一个（为了不让箭头特别乱画了两份）  当循环结束是 e 会指向 next 也就是 b 节点   第二次循环  next 指向了节点 a  e 头插节点 b  当循环结束时，e 指向 next 也就是节点 a   第三次循环  next 指向了 null  e 头插节点 a，a 的 next 指向了 b（之前 a.next 一直是 null），b 的 next 指向 a，死链已成  当循环结束时，e 指向 next 也就是 null，因此第四次循环时会正常退出   数据错乱（1.7，1.8 都会存在）  代码参考 day01.map.HashMapMissData，具体调试步骤参考视频 补充代码说明  day01.map.HashMapDistribution 演示 map 中链表长度符合泊松分布  day01.map.DistributionAffectedByCapacity 演示容量及 hashCode 取值对分布的影响   day01.map.DistributionAffectedByCapacity#hashtableGrowRule 演示了 Hashtable 的扩容规律  day01.sort.Utils#randomArray 如果 hashCode 足够随机，容量是否是 2 的 n 次幂影响不大  day01.sort.Utils#lowSameArray 如果 hashCode 低位一样的多，容量是 2 的 n 次幂会导致分布不均匀  day01.sort.Utils#evenArray 如果 hashCode 偶数的多，容量是 2 的 n 次幂会导致分布不均匀  由此得出对于容量是 2 的 n 次幂的设计来讲，二次 hash 非常重要    day01.map.HashMapVsHashtable 演示了对于同样数量的单词字符串放入 HashMap 和 Hashtable 分布上的区别 6）key 的设计 key 的设计要求  HashMap 的 key 可以为 null，但 Map 的其他实现则不然 作为 key 的对象，必须实现 hashCode 和 equals，并且 key 的内容不能修改（不可变） key 的 hashCode 应该有良好的散列性 如果 key 可变，例如修改了 age 会导致再次查询时查询不到 public class HashMapMutableKey {  public static void main(String[] args) {  HashMap&lt;Student, Object&gt; map = new HashMap&lt;&gt;();  Student stu = new Student(张三, 18);  map.put(stu, new Object());  System.out.println(map.get(stu));  stu.age = 19;  System.out.println(map.get(stu));  } static class Student {  String name;  int age;  public Student(String name, int age) {   this.name = name;   this.age = age;  }  public String getName() {   return name;  }  public void setName(String name) {   this.name = name;  }  public int getAge() {   return age;  }  public void setAge(int age) {   this.age = age;  }  @Override  public boolean equals(Object o) {   if (this == o) return true;   if (o == null || getClass() != o.getClass()) return false;   Student student = (Student) o;   return age == student.age &amp;&amp; Objects.equals(name, student.name);  }  @Override  public int hashCode() {   return Objects.hash(name, age);  }  } } String 对象的 hashCode() 设计  目标是达到较为均匀的散列效果，每个字符串的 hashCode 足够独特 字符串中的每个字符都可以表现为一个数字，称为 $S_i$，其中 i 的范围是 0 ~ n - 1 散列公式为： $S_0∗31^{(n-1)}+ S_1∗31^{(n-2)}+ … S_i ∗ 31^{(n-1-i)}+ …S_{(n-1)}∗31^0$ 31 代入公式有较好的散列特性，并且 31 * h 可以被优化为  即 $32 ∗h -h $  即 $2^5 ∗h -h$  即 $h≪5 -h$   11. 单例模式 要求  掌握五种单例模式的实现方式 理解为何 DCL 实现时要使用 volatile 修饰静态变量 了解 jdk 中用到单例的场景 饿汉式 public class Singleton1 implements Serializable {  private Singleton1() {  if (INSTANCE != null) {   throw new RuntimeException(单例对象不能重复创建);  }  System.out.println(private Singleton1());  } private static final Singleton1 INSTANCE = new Singleton1(); public static Singleton1 getInstance() {  return INSTANCE;  } public static void otherMethod() {  System.out.println(otherMethod());  } public Object readResolve() {  return INSTANCE;  } } 构造方法抛出异常是防止反射破坏单例 readResolve() 是防止反序列化破坏单例 枚举饿汉式 public enum Singleton2 {  INSTANCE; private Singleton2() {  System.out.println(private Singleton2());  } @Override  public String toString() {  return getClass().getName() + @ + Integer.toHexString(hashCode());  } public static Singleton2 getInstance() {  return INSTANCE;  } public static void otherMethod() {  System.out.println(otherMethod());  } } 枚举饿汉式能天然防止反射、反序列化破坏单例 懒汉式 public class Singleton3 implements Serializable {  private Singleton3() {  System.out.println(private Singleton3());  } private static Singleton3 INSTANCE = null; // Singleton3.class  public static synchronized Singleton3 getInstance() {  if (INSTANCE == null) {   INSTANCE = new Singleton3();  }  return INSTANCE;  } public static void otherMethod() {  System.out.println(otherMethod());  } } 其实只有首次创建单例对象时才需要同步，但该代码实际上每次调用都会同步 因此有了下面的双检锁改进 双检锁懒汉式 public class Singleton4 implements Serializable {  private Singleton4() {  System.out.println(private Singleton4());  } private static volatile Singleton4 INSTANCE = null; // 可见性，有序性 public static Singleton4 getInstance() {  if (INSTANCE == null) {   synchronized (Singleton4.class) {    if (INSTANCE == null) {    INSTANCE = new Singleton4();    }   }  }  return INSTANCE;  } public static void otherMethod() {  System.out.println(otherMethod());  } } 为何必须加 volatile：  INSTANCE = new Singleton4() 不是原子的，分成 3 步：创建对象、调用构造、给静态变量赋值，其中后两步可能被指令重排序优化，变成先赋值、再调用构造 如果线程1 先执行了赋值，线程2 执行到第一个 INSTANCE == null 时发现 INSTANCE 已经不为 null，此时就会返回一个未完全构造的对象 内部类懒汉式 public class Singleton5 implements Serializable {  private Singleton5() {  System.out.println(private Singleton5());  } private static class Holder {  static Singleton5 INSTANCE = new Singleton5();  } public static Singleton5 getInstance() {  return Holder.INSTANCE;  } public static void otherMethod() {  System.out.println(otherMethod());  } } 避免了双检锁的缺点 JDK 中单例的体现  Runtime 体现了饿汉式单例 Console 体现了双检锁懒汉式单例 Collections 中的 EmptyNavigableSet 内部类懒汉式单例 ReverseComparator.REVERSE_ORDER 内部类懒汉式单例 Comparators.NaturalOrderComparator.INSTANCE 枚举饿汉式单例 ",
      "url"      : "http://zhangjinmiao.github.io/interview/2022/08/19/java-base.html",
      "keywords" : "Interview,面试,jvm,gc"
    } ,
  
    {
      "title"    : "多线程与并发面试问题集锦",
      "category" : "Interview",
      "content": "1. 线程状态 要求  掌握 Java 线程六种状态 掌握 Java 线程状态转换 能理解五种状态与六种状态两种说法的区别 六种状态及转换 分别是  新建  当一个线程对象被创建，但还未调用 start 方法时处于新建状态  此时未与操作系统底层线程关联   可运行  调用了 start 方法，就会由新建进入可运行  此时与底层线程关联，由操作系统调度执行   终结  线程内代码已经执行完毕，由可运行进入终结  此时会取消与底层线程关联   阻塞  当获取锁失败后，由可运行进入 Monitor 的阻塞队列阻塞，此时不占用 cpu 时间  当持锁线程释放锁时，会按照一定规则唤醒阻塞队列中的阻塞线程，唤醒后的线程进入可运行状态   等待  当获取锁成功后，但由于条件不满足，调用了 wait() 方法，此时从可运行状态释放锁进入 Monitor 等待集合等待，同样不占用 cpu 时间  当其它持锁线程调用 notify() 或 notifyAll() 方法，会按照一定规则唤醒等待集合中的等待线程，恢复为可运行状态   有时限等待  当获取锁成功后，但由于条件不满足，调用了 wait(long) 方法，此时从可运行状态释放锁进入 Monitor 等待集合进行有时限等待，同样不占用 cpu 时间  当其它持锁线程调用 notify() 或 notifyAll() 方法，会按照一定规则唤醒等待集合中的有时限等待线程，恢复为可运行状态，并重新去竞争锁  如果等待超时，也会从有时限等待状态恢复为可运行状态，并重新去竞争锁  还有一种情况是调用 sleep(long) 方法也会从可运行状态进入有时限等待状态，但与 Monitor 无关，不需要主动唤醒，超时时间到自然恢复为可运行状态   其它情况（只需了解）  可以用 interrupt() 方法打断等待、有时限等待的线程，让它们恢复为可运行状态  park，unpark 等方法也可以让线程等待和唤醒 五种状态 五种状态的说法来自于操作系统层面的划分  运行态：分到 cpu 时间，能真正执行线程内代码的 就绪态：有资格分到 cpu 时间，但还未轮到它的 阻塞态：没资格分到 cpu 时间的  涵盖了 java 状态中提到的阻塞、等待、有时限等待  多出了阻塞 I/O，指线程在调用阻塞 I/O 时，实际活由 I/O 设备完成，此时线程无事可做，只能干等   新建与终结态：与 java 中同名状态类似，不再啰嗦 2. 线程池 要求  掌握线程池的 7 大核心参数 七大参数  corePoolSize 核心线程数目 - 池中会保留的最多线程数 maximumPoolSize 最大线程数目 - 核心线程+救急线程的最大数目 keepAliveTime 生存时间 - 救急线程的生存时间，生存时间内没有新任务，此线程资源会释放 unit 时间单位 - 救急线程的生存时间单位，如秒、毫秒等 workQueue - 当没有空闲核心线程时，新来任务会加入到此队列排队，队列满会创建救急线程执行任务 threadFactory 线程工厂 - 可以定制线程对象的创建，例如设置线程名字、是否是守护线程等 handler 拒绝策略 - 当所有线程都在繁忙，workQueue 也放满时，会触发拒绝策略  抛异常 java.util.concurrent.ThreadPoolExecutor.AbortPolicy  由调用者执行任务 java.util.concurrent.ThreadPoolExecutor.CallerRunsPolicy  丢弃任务 java.util.concurrent.ThreadPoolExecutor.DiscardPolicy  丢弃最早排队任务 java.util.concurrent.ThreadPoolExecutor.DiscardOldestPolicy   代码说明 day02.TestThreadPoolExecutor 以较为形象的方式演示了线程池的核心组成 3. wait vs sleep 要求  能够说出二者区别 一个共同点，三个不同点 共同点  wait() ，wait(long) 和 sleep(long) 的效果都是让当前线程暂时放弃 CPU 的使用权，进入阻塞状态 不同点  方法归属不同  sleep(long) 是 Thread 的静态方法  而 wait()，wait(long) 都是 Object 的成员方法，每个对象都有   醒来时机不同  执行 sleep(long) 和 wait(long) 的线程都会在等待相应毫秒后醒来  wait(long) 和 wait() 还可以被 notify 唤醒，wait() 如果不唤醒就一直等下去  它们都可以被打断唤醒   锁特性不同（重点）  wait 方法的调用必须先获取 wait 对象的锁，而 sleep 则无此限制  wait 方法执行后会释放对象锁，允许其它线程获得该对象锁（我放弃 cpu，但你们还可以用）  而 sleep 如果在 synchronized 代码块中执行，并不会释放对象锁（我放弃 cpu，你们也用不了）   4. lock vs synchronized 要求  掌握 lock 与 synchronized 的区别 理解 ReentrantLock 的公平、非公平锁 理解 ReentrantLock 中的条件变量 三个层面 不同点  语法层面  synchronized 是关键字，源码在 jvm 中，用 c++ 语言实现  Lock 是接口，源码由 jdk 提供，用 java 语言实现  使用 synchronized 时，退出同步代码块锁会自动释放，而使用 Lock 时，需要手动调用 unlock 方法释放锁   功能层面  二者均属于悲观锁、都具备基本的互斥、同步、锁重入功能  Lock 提供了许多 synchronized 不具备的功能，例如获取等待状态、公平锁、可打断、可超时、多条件变量  Lock 有适合不同场景的实现，如 ReentrantLock， ReentrantReadWriteLock   性能层面  在没有竞争时，synchronized 做了很多优化，如偏向锁、轻量级锁，性能不赖  在竞争激烈时，Lock 的实现通常会提供更好的性能   公平锁  公平锁的公平体现  已经处在阻塞队列中的线程（不考虑超时）始终都是公平的，先进先出  公平锁是指未处于阻塞队列中的线程来争抢锁，如果队列不为空，则老实到队尾等待  非公平锁是指未处于阻塞队列中的线程来争抢锁，与队列头唤醒的线程去竞争，谁抢到算谁的   公平锁会降低吞吐量，一般不用 条件变量  ReentrantLock 中的条件变量功能类似于普通 synchronized 的 wait，notify，用在当线程获得锁后，发现条件不满足时，临时等待的链表结构 与 synchronized 的等待集合不同之处在于，ReentrantLock 中的条件变量可以有多个，可以实现更精细的等待、唤醒控制 代码说明  day02.TestReentrantLock 用较为形象的方式演示 ReentrantLock 的内部结构 5. volatile 要求  掌握线程安全要考虑的三个问题 掌握 volatile 能解决哪些问题 原子性  起因：多线程下，不同线程的指令发生了交错导致的共享变量的读写混乱 解决：用悲观锁或乐观锁解决，volatile 并不能解决原子性 可见性  起因：由于编译器优化、或缓存优化、或 CPU 指令重排序优化导致的对共享变量所做的修改另外的线程看不到 解决：用 volatile 修饰共享变量，能够防止编译器等优化发生，让一个线程对共享变量的修改对另一个线程可见 有序性  起因：由于编译器优化、或缓存优化、或 CPU 指令重排序优化导致指令的实际执行顺序与编写顺序不一致 解决：用 volatile 修饰共享变量会在读、写共享变量时加入不同的屏障，阻止其他读写操作越过屏障，从而达到阻止重排序的效果 注意：  volatile 变量写加的屏障是阻止上方其它写操作越过屏障排到 volatile 变量写之下  volatile 变量读加的屏障是阻止下方其它读操作越过屏障排到 volatile 变量读之上  volatile 读写加入的屏障只能防止同一线程内的指令重排   代码说明  day02.threadsafe.AddAndSubtract 演示原子性  day02.threadsafe.ForeverLoop 演示可见性   注意：本例经实践检验是编译器优化导致的可见性问题    day02.threadsafe.Reordering 演示有序性   需要打成 jar 包后测试    请同时参考视频讲解 6. 悲观锁 vs 乐观锁 要求  掌握悲观锁和乐观锁的区别 对比悲观锁与乐观锁  悲观锁的代表是 synchronized 和 Lock 锁  其核心思想是【线程只有占有了锁，才能去操作共享变量，每次只有一个线程占锁成功，获取锁失败的线程，都得停下来等待】  线程从运行到阻塞、再从阻塞到唤醒，涉及线程上下文切换，如果频繁发生，影响性能  实际上，线程在获取 synchronized 和 Lock 锁时，如果锁已被占用，都会做几次重试操作，减少阻塞的机会   乐观锁的代表是 AtomicInteger，使用 cas 来保证原子性  其核心思想是【无需加锁，每次只有一个线程能成功修改共享变量，其它失败的线程不需要停止，不断重试直至成功】  由于线程一直运行，不需要阻塞，因此不涉及线程上下文切换  它需要多核 cpu 支持，且线程数不应超过 cpu 核数   代码说明  day02.SyncVsCas 演示了分别使用乐观锁和悲观锁解决原子赋值  请同时参考视频讲解 7. Hashtable vs ConcurrentHashMap 要求  掌握 Hashtable 与 ConcurrentHashMap 的区别 掌握 ConcurrentHashMap 在不同版本的实现区别 更形象的演示，见资料中的 hash-demo.jar，运行需要 jdk14 以上环境，进入 jar 包目录，执行下面命令 java -jar --add-exports java.base/jdk.internal.misc=ALL-UNNAMED hash-demo.jar Hashtable 对比 ConcurrentHashMap  Hashtable 与 ConcurrentHashMap 都是线程安全的 Map 集合 Hashtable 并发度低，整个 Hashtable 对应一把锁，同一时刻，只能有一个线程操作它 ConcurrentHashMap 并发度高，整个 ConcurrentHashMap 对应多把锁，只要线程访问的是不同锁，那么不会冲突 ConcurrentHashMap 1.7  数据结构：Segment(大数组) + HashEntry(小数组) + 链表，每个 Segment 对应一把锁，如果多个线程访问不同的 Segment，则不会冲突 并发度：Segment 数组大小即并发度，决定了同一时刻最多能有多少个线程并发访问。Segment 数组不能扩容，意味着并发度在 ConcurrentHashMap 创建时就固定了 索引计算  假设大数组长度是 $2^m$，key 在大数组内的索引是 key 的二次 hash 值的高 m 位  假设小数组长度是 $2^n$，key 在小数组内的索引是 key 的二次 hash 值的低 n 位   扩容：每个小数组的扩容相对独立，小数组在超过扩容因子时会触发扩容，每次扩容翻倍 Segment[0] 原型：首次创建其它小数组时，会以此原型为依据，数组长度，扩容因子都会以原型为准 ConcurrentHashMap 1.8  数据结构：Node 数组 + 链表或红黑树，数组的每个头节点作为锁，如果多个线程访问的头节点不同，则不会冲突。首次生成头节点时如果发生竞争，利用 cas 而非 syncronized，进一步提升性能 并发度：Node 数组有多大，并发度就有多大，与 1.7 不同，Node 数组可以扩容 扩容条件：Node 数组满 3/4 时就会扩容 扩容单位：以链表为单位从后向前迁移链表，迁移完成的将旧数组头节点替换为 ForwardingNode 扩容时并发 get  根据是否为 ForwardingNode 来决定是在新数组查找还是在旧数组查找，不会阻塞  如果链表长度超过 1，则需要对节点进行复制（创建新节点），怕的是节点迁移后 next 指针改变  如果链表最后几个元素扩容后索引不变，则节点无需复制   扩容时并发 put  如果 put 的线程与扩容线程操作的链表是同一个，put 线程会阻塞  如果 put 的线程操作的链表还未迁移完成，即头节点不是 ForwardingNode，则可以并发执行  如果 put 的线程操作的链表已经迁移完成，即头结点是 ForwardingNode，则可以协助扩容   与 1.7 相比是懒惰初始化 capacity 代表预估的元素个数，capacity / factory 来计算出初始数组大小，需要贴近 $2^n$ loadFactor 只在计算初始数组大小时被使用，之后扩容固定为 3/4 超过树化阈值时的扩容问题，如果容量已经是 64，直接树化，否则在原来容量基础上做 3 轮扩容 8. ThreadLocal 要求  掌握 ThreadLocal 的作用与原理 掌握 ThreadLocal 的内存释放时机 作用  ThreadLocal 可以实现【资源对象】的线程隔离，让每个线程各用各的【资源对象】，避免争用引发的线程安全问题 ThreadLocal 同时实现了线程内的资源共享 原理 每个线程内有一个 ThreadLocalMap 类型的成员变量，用来存储资源对象  调用 set 方法，就是以 ThreadLocal 自己作为 key，资源对象作为 value，放入当前线程的 ThreadLocalMap 集合中 调用 get 方法，就是以 ThreadLocal 自己作为 key，到当前线程中查找关联的资源值 调用 remove 方法，就是以 ThreadLocal 自己作为 key，移除当前线程关联的资源值 ThreadLocalMap 的一些特点  key 的 hash 值统一分配 初始容量 16，扩容因子 2/3，扩容容量翻倍 key 索引冲突后用开放寻址法解决冲突 弱引用 key ThreadLocalMap 中的 key 被设计为弱引用，原因如下  Thread 可能需要长时间运行（如线程池中的线程），如果 key 不再使用，需要在内存不足（GC）时释放其占用的内存 内存释放时机  被动 GC 释放 key  仅是让 key 的内存释放，关联 value 的内存并不会释放   懒惰被动释放 value  get key 时，发现是 null key，则释放其 value 内存  set key 时，会使用启发式扫描，清除临近的 null key 的 value 内存，启发次数与元素个数，是否发现 null key 有关   主动 remove 释放 key，value  会同时释放 key，value 的内存，也会清除临近的 null key 的 value 内存  推荐使用它，因为一般使用 ThreadLocal 时都把它作为静态变量（即强引用），因此无法被动依靠 GC 回收   ",
      "url"      : "http://zhangjinmiao.github.io/interview/2022/08/20/concurrent.html",
      "keywords" : "Interview,面试,并发,线程"
    } ,
  
    {
      "title"    : "Spring 面试问题集锦",
      "category" : "Interview",
      "content": "1. Spring refresh 流程 要求  掌握 refresh 的 12 个步骤 Spring refresh 概述 refresh 是 AbstractApplicationContext 中的一个方法，负责初始化 ApplicationContext 容器，容器必须调用 refresh 才能正常工作。它的内部主要会调用 12 个方法，我们把它们称为 refresh 的 12 个步骤：  prepareRefresh   obtainFreshBeanFactory   prepareBeanFactory   postProcessBeanFactory   invokeBeanFactoryPostProcessors   registerBeanPostProcessors   initMessageSource   initApplicationEventMulticaster   onRefresh   registerListeners   finishBeanFactoryInitialization   finishRefresh  功能分类   1 为准备环境    2 3 4 5 6 为准备 BeanFactory    7 8 9 10 12 为准备 ApplicationContext    11 为初始化 BeanFactory 中非延迟单例 bean   1. prepareRefresh  这一步创建和准备了 Environment 对象，它作为 ApplicationContext 的一个成员变量 Environment 对象的作用之一是为后续 @Value，值注入时提供键值 Environment 分成三个主要部分  systemProperties - 保存 java 环境键值  systemEnvironment - 保存系统环境键值  自定义 PropertySource - 保存自定义键值，例如来自于 *.properties 文件的键值   2. obtainFreshBeanFactory  这一步获取（或创建） BeanFactory，它也是作为 ApplicationContext 的一个成员变量 BeanFactory 的作用是负责 bean 的创建、依赖注入和初始化，bean 的各项特征由 BeanDefinition 定义  BeanDefinition 作为 bean 的设计蓝图，规定了 bean 的特征，如单例多例、依赖关系、初始销毁方法等  BeanDefinition 的来源有多种多样，可以是通过 xml 获得、配置类获得、组件扫描获得，也可以是编程添加   所有的 BeanDefinition 会存入 BeanFactory 中的 beanDefinitionMap 集合  3. prepareBeanFactory  这一步会进一步完善 BeanFactory，为它的各项成员变量赋值 beanExpressionResolver 用来解析 SpEL，常见实现为 StandardBeanExpressionResolver propertyEditorRegistrars 会注册类型转换器  它在这里使用了 ResourceEditorRegistrar 实现类  并应用 ApplicationContext 提供的 Environment 完成 ${ } 解析   registerResolvableDependency 来注册 beanFactory 以及 ApplicationContext，让它们也能用于依赖注入 beanPostProcessors 是 bean 后处理器集合，会工作在 bean 的生命周期各个阶段，此处会添加两个：  ApplicationContextAwareProcessor 用来解析 Aware 接口  ApplicationListenerDetector 用来识别容器中 ApplicationListener 类型的 bean   4. postProcessBeanFactory  这一步是空实现，留给子类扩展。  一般 Web 环境的 ApplicationContext 都要利用它注册新的 Scope，完善 Web 下的 BeanFactory   这里体现的是模板方法设计模式 5. invokeBeanFactoryPostProcessors  这一步会调用 beanFactory 后处理器 beanFactory 后处理器，充当 beanFactory 的扩展点，可以用来补充或修改 BeanDefinition 常见的 beanFactory 后处理器有  ConfigurationClassPostProcessor – 解析 @Configuration、@Bean、@Import、@PropertySource 等  PropertySourcesPlaceHolderConfigurer – 替换 BeanDefinition 中的 ${ }  MapperScannerConfigurer – 补充 Mapper 接口对应的 BeanDefinition   6. registerBeanPostProcessors  这一步是继续从 beanFactory 中找出 bean 后处理器，添加至 beanPostProcessors 集合中 bean 后处理器，充当 bean 的扩展点，可以工作在 bean 的实例化、依赖注入、初始化阶段，常见的有：  AutowiredAnnotationBeanPostProcessor 功能有：解析 @Autowired，@Value 注解  CommonAnnotationBeanPostProcessor 功能有：解析 @Resource，@PostConstruct，@PreDestroy  AnnotationAwareAspectJAutoProxyCreator 功能有：为符合切点的目标 bean 自动创建代理   7. initMessageSource  这一步是为 ApplicationContext 添加 messageSource 成员，实现国际化功能 去 beanFactory 内找名为 messageSource 的 bean，如果没有，则提供空的 MessageSource 实现  8. initApplicationContextEventMulticaster  这一步为 ApplicationContext 添加事件广播器成员，即 applicationContextEventMulticaster 它的作用是发布事件给监听器 去 beanFactory 找名为 applicationEventMulticaster 的 bean 作为事件广播器，若没有，会创建默认的事件广播器 之后就可以调用 ApplicationContext.publishEvent(事件对象) 来发布事件  9. onRefresh  这一步是空实现，留给子类扩展  SpringBoot 中的子类在这里准备了 WebServer，即内嵌 web 容器   体现的是模板方法设计模式 10. registerListeners  这一步会从多种途径找到事件监听器，并添加至 applicationEventMulticaster 事件监听器顾名思义，用来接收事件广播器发布的事件，有如下来源  事先编程添加的  来自容器中的 bean  来自于 @EventListener 的解析   要实现事件监听器，只需要实现 ApplicationListener 接口，重写其中 onApplicationEvent(E e) 方法即可  11. finishBeanFactoryInitialization  这一步会将 beanFactory 的成员补充完毕，并初始化所有非延迟单例 bean conversionService 也是一套转换机制，作为对 PropertyEditor 的补充 embeddedValueResolvers 即内嵌值解析器，用来解析 @Value 中的 ${ }，借用的是 Environment 的功能 singletonObjects 即单例池，缓存所有单例对象  对象的创建都分三个阶段，每一阶段都有不同的 bean 后处理器参与进来，扩展功能   12. finishRefresh  这一步会为 ApplicationContext 添加 lifecycleProcessor 成员，用来控制容器内需要生命周期管理的 bean 如果容器中有名称为 lifecycleProcessor 的 bean 就用它，否则创建默认的生命周期管理器 准备好生命周期管理器，就可以实现  调用 context 的 start，即可触发所有实现 LifeCycle 接口 bean 的 start  调用 context 的 stop，即可触发所有实现 LifeCycle 接口 bean 的 stop   发布 ContextRefreshed 事件，整个 refresh 执行完成  2. Spring bean 生命周期 要求  掌握 Spring bean 的生命周期 bean 生命周期 概述 bean 的生命周期从调用 beanFactory 的 getBean 开始，到这个 bean 被销毁，可以总结为以下七个阶段：  处理名称，检查缓存 处理父子容器 处理 dependsOn 选择 scope 策略 创建 bean 类型转换处理 销毁 bean 注意  划分的阶段和名称并不重要，重要的是理解整个过程中做了哪些事情 1. 处理名称，检查缓存  这一步会处理别名，将别名解析为实际名称 对 FactoryBean 也会特殊处理，如果以 &amp; 开头表示要获取 FactoryBean 本身，否则表示要获取其产品 这里针对单例对象会检查一级、二级、三级缓存  singletonFactories 三级缓存，存放单例工厂对象  earlySingletonObjects 二级缓存，存放单例工厂的产品对象    如果发生循环依赖，产品是代理；无循环依赖，产品是原始对象     singletonObjects 一级缓存，存放单例成品对象   2. 处理父子容器  如果当前容器根据名字找不到这个 bean，此时若父容器存在，则执行父容器的 getBean 流程 父子容器的 bean 名称可以重复 3. 处理 dependsOn  如果当前 bean 有通过 dependsOn 指定了非显式依赖的 bean，这一步会提前创建这些 dependsOn 的 bean 所谓非显式依赖，就是指两个 bean 之间不存在直接依赖关系，但需要控制它们的创建先后顺序 4. 选择 scope 策略  对于 singleton scope，首先到单例池去获取 bean，如果有则直接返回，没有再进入创建流程 对于 prototype scope，每次都会进入创建流程 对于自定义 scope，例如 request，首先到 request 域获取 bean，如果有则直接返回，没有再进入创建流程 5.1 创建 bean - 创建 bean 实例    要点  总结     有自定义 TargetSource 的情况  由 AnnotationAwareAspectJAutoProxyCreator 创建代理返回    Supplier 方式创建 bean 实例  为 Spring 5.0 新增功能，方便编程方式创建 bean 实例    FactoryMethod 方式 创建 bean 实例  ① 分成静态工厂与实例工厂；② 工厂方法若有参数，需要对工厂方法参数进行解析，利用 resolveDependency；③ 如果有多个工厂方法候选者，还要进一步按权重筛选    AutowiredAnnotationBeanPostProcessor  ① 优先选择带 @Autowired 注解的构造；② 若有唯一的带参构造，也会入选    mbd.getPreferredConstructors  选择所有公共构造，这些构造之间按权重筛选    采用默认构造  如果上面的后处理器和 BeanDefiniation 都没找到构造，采用默认构造，即使是私有的   5.2 创建 bean - 依赖注入    要点  总结     AutowiredAnnotationBeanPostProcessor  识别 @Autowired 及 @Value 标注的成员，封装为 InjectionMetadata 进行依赖注入    CommonAnnotationBeanPostProcessor  识别 @Resource 标注的成员，封装为 InjectionMetadata 进行依赖注入    resolveDependency  用来查找要装配的值，可以识别：① Optional；② ObjectFactory 及 ObjectProvider；③ @Lazy 注解；④ @Value 注解（${ }, #{ }, 类型转换）；⑤ 集合类型（Collection，Map，数组等）；⑥ 泛型和 @Qualifier（用来区分类型歧义）；⑦ primary 及名字匹配（用来区分类型歧义）    AUTOWIRE_BY_NAME  根据成员名字找 bean 对象，修改 mbd 的 propertyValues，不会考虑简单类型的成员    AUTOWIRE_BY_TYPE  根据成员类型执行 resolveDependency 找到依赖注入的值，修改 mbd 的 propertyValues    applyPropertyValues  根据 mbd 的 propertyValues 进行依赖注入（即xml中 &lt;property name ref|value/&gt;）   5.3 创建 bean - 初始化    要点  总结     内置 Aware 接口的装配  包括 BeanNameAware，BeanFactoryAware 等    扩展 Aware 接口的装配  由 ApplicationContextAwareProcessor 解析，执行时机在 postProcessBeforeInitialization    @PostConstruct  由 CommonAnnotationBeanPostProcessor 解析，执行时机在 postProcessBeforeInitialization    InitializingBean  通过接口回调执行初始化    initMethod  根据 BeanDefinition 得到的初始化方法执行初始化，即 &lt;bean init-method&gt; 或 @Bean(initMethod)    创建 aop 代理  由 AnnotationAwareAspectJAutoProxyCreator 创建，执行时机在 postProcessAfterInitialization   5.4 创建 bean - 注册可销毁 bean 在这一步判断并登记可销毁 bean  判断依据  如果实现了 DisposableBean 或 AutoCloseable 接口，则为可销毁 bean  如果自定义了 destroyMethod，则为可销毁 bean  如果采用 @Bean 没有指定 destroyMethod，则采用自动推断方式获取销毁方法名（close，shutdown）  如果有 @PreDestroy 标注的方法   存储位置  singleton scope 的可销毁 bean 会存储于 beanFactory 的成员当中  自定义 scope 的可销毁 bean 会存储于对应的域对象当中  prototype scope 不会存储，需要自己找到此对象销毁   存储时都会封装为 DisposableBeanAdapter 类型对销毁方法的调用进行适配 6. 类型转换处理  如果 getBean 的 requiredType 参数与实际得到的对象类型不同，会尝试进行类型转换 7. 销毁 bean  销毁时机  singleton bean 的销毁在 ApplicationContext.close 时，此时会找到所有 DisposableBean 的名字，逐一销毁  自定义 scope bean 的销毁在作用域对象生命周期结束时  prototype bean 的销毁可以通过自己手动调用 AutowireCapableBeanFactory.destroyBean 方法执行销毁   同一 bean 中不同形式销毁方法的调用次序  优先后处理器销毁，即 @PreDestroy  其次 DisposableBean 接口销毁  最后 destroyMethod 销毁（包括自定义名称，推断名称，AutoCloseable 接口 多选一）   3. Spring bean 循环依赖 要求  掌握单例 set 方式循环依赖的原理 掌握其它循环依赖的解决方法 循环依赖的产生  首先要明白，bean 的创建要遵循一定的步骤，必须是创建、注入、初始化三步，这些顺序不能乱   set 方法（包括成员变量）的循环依赖如图所示   可以在【a 创建】和【a set 注入 b】之间加入 b 的整个流程来解决   【b set 注入 a】 时可以成功，因为之前 a 的实例已经创建完毕   a 的顺序，及 b 的顺序都能得到保障   构造方法的循环依赖如图所示，显然无法用前面的方法解决  构造循环依赖的解决  思路1  a 注入 b 的代理对象，这样能够保证 a 的流程走通  后续需要用到 b 的真实对象时，可以通过代理间接访问   思路2  a 注入 b 的工厂对象，让 b 的实例创建被推迟，这样能够保证 a 的流程先走通  后续需要用到 b 的真实对象时，再通过 ObjectFactory 工厂间接访问   示例1：用 @Lazy 为构造方法参数生成代理 public class App60_1 { static class A {  private static final Logger log = LoggerFactory.getLogger(A);  private B b;  public A(@Lazy B b) {   log.debug(A(B b) {}, b.getClass());   this.b = b;  }  @PostConstruct  public void init() {   log.debug(init());  }  } static class B {  private static final Logger log = LoggerFactory.getLogger(B);  private A a;  public B(A a) {   log.debug(B({}), a);   this.a = a;  }  @PostConstruct  public void init() {   log.debug(init());  }  } public static void main(String[] args) {  GenericApplicationContext context = new GenericApplicationContext();  context.registerBean(a, A.class);  context.registerBean(b, B.class);  AnnotationConfigUtils.registerAnnotationConfigProcessors(context.getDefaultListableBeanFactory());  context.refresh();  System.out.println();  } } 示例2：用 ObjectProvider 延迟依赖对象的创建 public class App60_2 { static class A {  private static final Logger log = LoggerFactory.getLogger(A);  private ObjectProvider&lt;B&gt; b;  public A(ObjectProvider&lt;B&gt; b) {   log.debug(A({}), b);   this.b = b;  }  @PostConstruct  public void init() {   log.debug(init());  }  } static class B {  private static final Logger log = LoggerFactory.getLogger(B);  private A a;  public B(A a) {   log.debug(B({}), a);   this.a = a;  }  @PostConstruct  public void init() {   log.debug(init());  }  } public static void main(String[] args) {  GenericApplicationContext context = new GenericApplicationContext();  context.registerBean(a, A.class);  context.registerBean(b, B.class);  AnnotationConfigUtils.registerAnnotationConfigProcessors(context.getDefaultListableBeanFactory());  context.refresh();  System.out.println(context.getBean(A.class).b.getObject());  System.out.println(context.getBean(B.class));  } } 示例3：用 @Scope 产生代理 public class App60_3 { public static void main(String[] args) {  GenericApplicationContext context = new GenericApplicationContext();  ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(context.getDefaultListableBeanFactory());  scanner.scan(com.itheima.app60.sub);  context.refresh();  System.out.println();  } } @Component class A {  private static final Logger log = LoggerFactory.getLogger(A);  private B b; public A(B b) {  log.debug(A(B b) {}, b.getClass());  this.b = b;  } @PostConstruct  public void init() {  log.debug(init());  } } @Scope(proxyMode = ScopedProxyMode.TARGET_CLASS) @Component class B {  private static final Logger log = LoggerFactory.getLogger(B);  private A a; public B(A a) {  log.debug(B({}), a);  this.a = a;  } @PostConstruct  public void init() {  log.debug(init());  } } 示例4：用 Provider 接口解决，原理上与 ObjectProvider 一样，Provider 接口是独立的 jar 包，需要加入依赖 &lt;dependency&gt;  &lt;groupId&gt;javax.inject&lt;/groupId&gt;  &lt;artifactId&gt;javax.inject&lt;/artifactId&gt;  &lt;version&gt;1&lt;/version&gt; &lt;/dependency&gt; public class App60_4 { static class A {  private static final Logger log = LoggerFactory.getLogger(A);  private Provider&lt;B&gt; b;  public A(Provider&lt;B&gt; b) {   log.debug(A({}}), b);   this.b = b;  }  @PostConstruct  public void init() {   log.debug(init());  }  } static class B {  private static final Logger log = LoggerFactory.getLogger(B);  private A a;  public B(A a) {   log.debug(B({}}), a);   this.a = a;  }  @PostConstruct  public void init() {   log.debug(init());  }  } public static void main(String[] args) {  GenericApplicationContext context = new GenericApplicationContext();  context.registerBean(a, A.class);  context.registerBean(b, B.class);  AnnotationConfigUtils.registerAnnotationConfigProcessors(context.getDefaultListableBeanFactory());  context.refresh();  System.out.println(context.getBean(A.class).b.get());  System.out.println(context.getBean(B.class));  } } 解决 set 循环依赖的原理 一级缓存 作用是保证单例对象仅被创建一次  第一次走 getBean(a) 流程后，最后会将成品 a 放入 singletonObjects 一级缓存 后续再走 getBean(a) 流程时，先从一级缓存中找，这时已经有成品 a，就无需再次创建 一级缓存与循环依赖 一级缓存无法解决循环依赖问题，分析如下  无论是获取 bean a 还是获取 bean b，走的方法都是同一个 getBean 方法，假设先走 getBean(a) 当 a 的实例对象创建，接下来执行 a.setB() 时，需要走 getBean(b) 流程，红色箭头 1 当 b 的实例对象创建，接下来执行 b.setA() 时，又回到了 getBean(a) 的流程，红色箭头 2 但此时 singletonObjects 一级缓存内没有成品的 a，陷入了死循环 二级缓存 解决思路如下：  再增加一个 singletonFactories 缓存 在依赖注入前，即 a.setB() 以及 b.setA() 将 a 及 b 的半成品对象（未完成依赖注入和初始化）放入此缓存 执行依赖注入时，先看看 singletonFactories 缓存中是否有半成品的对象，如果有拿来注入，顺利走完流程 对于上面的图  a = new A() 执行之后就会把这个半成品的 a 放入 singletonFactories 缓存，即 factories.put(a) 接下来执行 a.setB()，走入 getBean(b) 流程，红色箭头 3 这回再执行到 b.setA() 时，需要一个 a 对象，有没有呢？有！ factories.get() 在 singletonFactories 缓存中就可以找到，红色箭头 4 和 5 b 的流程能够顺利走完，将 b 成品放入 singletonObject 一级缓存，返回到 a 的依赖注入流程，红色箭头 6 二级缓存与创建代理 二级缓存无法正确处理循环依赖并且包含有代理创建的场景，分析如下  spring 默认要求，在 a.init 完成之后才能创建代理 pa = proxy(a) 由于 a 的代理创建时机靠后，在执行 factories.put(a) 向 singletonFactories 中放入的还是原始对象 接下来箭头 3、4、5 这几步 b 对象拿到和注入的都是原始对象 三级缓存 简单分析的话，只需要将代理的创建时机放在依赖注入之前即可，但 spring 仍然希望代理的创建时机在 init 之后，只有出现循环依赖时，才会将代理的创建时机提前。所以解决思路稍显复杂：  图中 factories.put(fa) 放入的既不是原始对象，也不是代理对象而是工厂对象 fa 当检查出发生循环依赖时，fa 的产品就是代理 pa，没有发生循环依赖，fa 的产品是原始对象 a 假设出现了循环依赖，拿到了 singletonFactories 中的工厂对象，通过在依赖注入前获得了 pa，红色箭头 5 这回 b.setA() 注入的就是代理对象，保证了正确性，红色箭头 7 还需要把 pa 存入新加的 earlySingletonObjects 缓存，红色箭头 6 a.init 完成后，无需二次创建代理，从哪儿找到 pa 呢？earlySingletonObjects 已经缓存，蓝色箭头 9 当成品对象产生，放入 singletonObject 后，singletonFactories 和 earlySingletonObjects 就中的对象就没有用处，清除即可 4. Spring 事务失效 要求  掌握事务失效的八种场景 1. 抛出检查异常导致事务不能正确回滚 @Service public class Service1 { @Autowired  private AccountMapper accountMapper; @Transactional  public void transfer(int from, int to, int amount) throws FileNotFoundException {  int fromBalance = accountMapper.findBalanceBy(from);  if (fromBalance - amount &gt;= 0) {   accountMapper.update(from, -1 * amount);   new FileInputStream(aaa);   accountMapper.update(to, amount);  }  } }   原因：Spring 默认只会回滚非检查异常   解法：配置 rollbackFor 属性  @Transactional(rollbackFor = Exception.class)   2. 业务方法内自己 try-catch 异常导致事务不能正确回滚 @Service public class Service2 { @Autowired  private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class)  public void transfer(int from, int to, int amount) {  try {   int fromBalance = accountMapper.findBalanceBy(from);   if (fromBalance - amount &gt;= 0) {    accountMapper.update(from, -1 * amount);    new FileInputStream(aaa);    accountMapper.update(to, amount);   }  } catch (FileNotFoundException e) {   e.printStackTrace();  }  } }   原因：事务通知只有捉到了目标抛出的异常，才能进行后续的回滚处理，如果目标自己处理掉异常，事务通知无法知悉 解法1：异常原样抛出  在 catch 块添加 throw new RuntimeException(e);   解法2：手动设置 TransactionStatus.setRollbackOnly()  在 catch 块添加 TransactionInterceptor.currentTransactionStatus().setRollbackOnly();   3. aop 切面顺序导致导致事务不能正确回滚 @Service public class Service3 { @Autowired  private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class)  public void transfer(int from, int to, int amount) throws FileNotFoundException {  int fromBalance = accountMapper.findBalanceBy(from);  if (fromBalance - amount &gt;= 0) {   accountMapper.update(from, -1 * amount);   new FileInputStream(aaa);   accountMapper.update(to, amount);  }  } } @Aspect public class MyAspect {  @Around(execution(* transfer(..)))  public Object around(ProceedingJoinPoint pjp) throws Throwable {  LoggerUtils.get().debug(log:{}, pjp.getTarget());  try {   return pjp.proceed();  } catch (Throwable e) {   e.printStackTrace();   return null;  }  } }   原因：事务切面优先级最低，但如果自定义的切面优先级和他一样，则还是自定义切面在内层，这时若自定义切面没有正确抛出异常… 解法1、2：同情况2 中的解法:1、2 解法3：调整切面顺序，在 MyAspect 上添加 @Order(Ordered.LOWEST_PRECEDENCE - 1) （不推荐） 4. 非 public 方法导致的事务失效 @Service public class Service4 { @Autowired  private AccountMapper accountMapper; @Transactional  void transfer(int from, int to, int amount) throws FileNotFoundException {  int fromBalance = accountMapper.findBalanceBy(from);  if (fromBalance - amount &gt;= 0) {   accountMapper.update(from, -1 * amount);   accountMapper.update(to, amount);  }  } }   原因：Spring 为方法创建代理、添加事务通知、前提条件都是该方法是 public 的 解法1：改为 public 方法 解法2：添加 bean 配置如下（不推荐） @Bean public TransactionAttributeSource transactionAttributeSource() {  return new AnnotationTransactionAttributeSource(false); } 5. 父子容器导致的事务失效 package day04.tx.app.service; // ... @Service public class Service5 { @Autowired  private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class)  public void transfer(int from, int to, int amount) throws FileNotFoundException {  int fromBalance = accountMapper.findBalanceBy(from);  if (fromBalance - amount &gt;= 0) {   accountMapper.update(from, -1 * amount);   accountMapper.update(to, amount);  }  } } 控制器类 package day04.tx.app.controller; // ... @Controller public class AccountController { @Autowired  public Service5 service; public void transfer(int from, int to, int amount) throws FileNotFoundException {  service.transfer(from, to, amount);  } } App 配置类 @Configuration @ComponentScan(day04.tx.app.service) @EnableTransactionManagement // ... public class AppConfig {  // ... 有事务相关配置 } Web 配置类 @Configuration @ComponentScan(day04.tx.app) // ... public class WebConfig {  // ... 无事务配置 } 现在配置了父子容器，WebConfig 对应子容器，AppConfig 对应父容器，发现事务依然失效  原因：子容器扫描范围过大，把未加事务配置的 service 扫描进来   解法1：各扫描各的，不要图简便   解法2：不要用父子容器，所有 bean 放在同一容器 6. 调用本类方法导致传播行为失效 @Service public class Service6 { @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class)  public void foo() throws FileNotFoundException {  LoggerUtils.get().debug(foo);  bar();  } @Transactional(propagation = Propagation.REQUIRES_NEW, rollbackFor = Exception.class)  public void bar() throws FileNotFoundException {  LoggerUtils.get().debug(bar);  } }   原因：本类方法调用不经过代理，因此无法增强   解法1：依赖注入自己（代理）来调用   解法2：通过 AopContext 拿到代理对象，来调用   解法3：通过 CTW，LTW 实现功能增强 解法1 @Service public class Service6 {  t@Autowired  tprivate Service6 proxy; // 本质上是一种循环依赖 @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class)  public void foo() throws FileNotFoundException {  LoggerUtils.get().debug(foo);  t tSystem.out.println(proxy.getClass());  t tproxy.bar();  } @Transactional(propagation = Propagation.REQUIRES_NEW, rollbackFor = Exception.class)  public void bar() throws FileNotFoundException {  LoggerUtils.get().debug(bar);  } } 解法2，还需要在 AppConfig 上添加 @EnableAspectJAutoProxy(exposeProxy = true) @Service public class Service6 {  @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class)  public void foo() throws FileNotFoundException {  LoggerUtils.get().debug(foo);  ((Service6) AopContext.currentProxy()).bar();  } @Transactional(propagation = Propagation.REQUIRES_NEW, rollbackFor = Exception.class)  public void bar() throws FileNotFoundException {  LoggerUtils.get().debug(bar);  } } 7. @Transactional 没有保证原子行为 @Service public class Service7 { private static final Logger logger = LoggerFactory.getLogger(Service7.class); @Autowired  private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class)  public void transfer(int from, int to, int amount) {  int fromBalance = accountMapper.findBalanceBy(from);  logger.debug(更新前查询余额为: {}, fromBalance);  if (fromBalance - amount &gt;= 0) {   accountMapper.update(from, -1 * amount);   accountMapper.update(to, amount);  }  } public int findBalance(int accountNo) {  return accountMapper.findBalanceBy(accountNo);  } } 上面的代码实际上是有 bug 的，假设 from 余额为 1000，两个线程都来转账 1000，可能会出现扣减为负数的情况  原因：事务的原子性仅涵盖 insert、update、delete、select … for update 语句，select 方法并不阻塞  如上图所示，红色线程和蓝色线程的查询都发生在扣减之前，都以为自己有足够的余额做扣减 8. @Transactional 方法导致的 synchronized 失效 针对上面的问题，能否在方法上加 synchronized 锁来解决呢？ @Service public class Service7 { private static final Logger logger = LoggerFactory.getLogger(Service7.class); @Autowired  private AccountMapper accountMapper; @Transactional(rollbackFor = Exception.class)  public synchronized void transfer(int from, int to, int amount) {  int fromBalance = accountMapper.findBalanceBy(from);  logger.debug(更新前查询余额为: {}, fromBalance);  if (fromBalance - amount &gt;= 0) {   accountMapper.update(from, -1 * amount);   accountMapper.update(to, amount);  }  } public int findBalance(int accountNo) {  return accountMapper.findBalanceBy(accountNo);  } } 答案是不行，原因如下：  synchronized 保证的仅是目标方法的原子性，环绕目标方法的还有 commit 等操作，它们并未处于 sync 块内 可以参考下图发现，蓝色线程的查询只要在红色线程提交之前执行，那么依然会查询到有 1000 足够余额来转账   解法1：synchronized 范围应扩大至代理方法调用   解法2：使用 select … for update 替换 select 5. Spring MVC 执行流程 要求  掌握 Spring MVC 的执行流程 了解 Spring MVC 的重要组件的作用 概要 我把整个流程分成三个阶段  准备阶段 匹配阶段 执行阶段 准备阶段  在 Web 容器第一次用到 DispatcherServlet 的时候，会创建其对象并执行 init 方法   init 方法内会创建 Spring Web 容器，并调用容器 refresh 方法   refresh 过程中会创建并初始化 SpringMVC 中的重要组件， 例如 MultipartResolver，HandlerMapping，HandlerAdapter，HandlerExceptionResolver、ViewResolver 等   容器初始化后，会将上一步初始化好的重要组件，赋值给 DispatcherServlet 的成员变量，留待后用  匹配阶段  用户发送的请求统一到达前端控制器 DispatcherServlet   DispatcherServlet 遍历所有 HandlerMapping ，找到与路径匹配的处理器 ① HandlerMapping 有多个，每个 HandlerMapping 会返回不同的处理器对象，谁先匹配，返回谁的处理器。其中能识别 @RequestMapping 的优先级最高 ② 对应 @RequestMapping 的处理器是 HandlerMethod，它包含了控制器对象和控制器方法信息 ③ 其中路径与处理器的映射关系在 HandlerMapping 初始化时就会建立好   将 HandlerMethod 连同匹配到的拦截器，生成调用链对象 HandlerExecutionChain 返回  遍历HandlerAdapter 处理器适配器，找到能处理 HandlerMethod 的适配器对象，开始调用  调用阶段  执行拦截器 preHandle   由 HandlerAdapter 调用 HandlerMethod ① 调用前处理不同类型的参数 ② 调用后处理不同类型的返回值   第 2 步没有异常 ① 返回 ModelAndView ② 执行拦截器 postHandle 方法 ③ 解析视图，得到 View 对象，进行视图渲染   第 2 步有异常，进入 HandlerExceptionResolver 异常处理流程   最后都会执行拦截器的 afterCompletion 方法   如果控制器方法标注了 @ResponseBody 注解，则在第 2 步，就会生成 json 结果，并标记 ModelAndView 已处理，这样就不会执行第 3 步的视图渲染 6. Spring 注解 要求  掌握 Spring 常见注解 提示  注解的详细列表请参考：面试题-spring-注解.xmind  下面列出了视频中重点提及的注解，考虑到大部分注解同学们已经比较熟悉了，仅对个别的作简要说明 事务注解  @EnableTransactionManagement，会额外加载 4 个 bean  BeanFactoryTransactionAttributeSourceAdvisor 事务切面类  TransactionAttributeSource 用来解析事务属性  TransactionInterceptor 事务拦截器  TransactionalEventListenerFactory 事务监听器工厂   @Transactional 核心  @Order 切面  @EnableAspectJAutoProxy  会加载 AnnotationAwareAspectJAutoProxyCreator，它是一个 bean 后处理器，用来创建代理  如果没有配置 @EnableAspectJAutoProxy，又需要用到代理（如事务）则会使用 InfrastructureAdvisorAutoProxyCreator 这个 bean 后处理器   组件扫描与配置类  @Component   @Controller   @Service   @Repository   @ComponentScan   @Conditional   @Configuration   配置类其实相当于一个工厂, 标注 @Bean 注解的方法相当于工厂方法  @Bean 不支持方法重载, 如果有多个重载方法, 仅有一个能入选为工厂方法  @Configuration 默认会为标注的类生成代理, 其目的是保证 @Bean 方法相互调用时, 仍然能保证其单例特性  @Configuration 中如果含有 BeanFactory 后处理器, 则实例工厂方法会导致 MyConfig 提前创建, 造成其依赖注入失败，解决方法是改用静态工厂方法或直接为 @Bean 的方法参数依赖注入, 针对 Mapper 扫描可以改用注解方式   @Bean   @Import    四种用法  ① 引入单个 bean  ② 引入一个配置类  ③ 通过 Selector 引入多个类  ④ 通过 beanDefinition 注册器     解析规则    同一配置类中, @Import 先解析 @Bean 后解析   同名定义, 默认后面解析的会覆盖前面解析的   不允许覆盖的情况下, 如何能够让 MyConfig(主配置类) 的配置优先? (虽然覆盖方式能解决)   采用 DeferredImportSelector，因为它最后工作, 可以简单认为先解析 @Bean, 再 Import      @Lazy   加在类上，表示此类延迟实例化、初始化  加在方法参数上，此参数会以代理方式注入   @PropertySource 依赖注入  @Autowired @Qualifier @Value mvc mapping  @RequestMapping，可以派生多个注解如 @GetMapping 等 mvc rest  @RequestBody @ResponseBody，组合 @Controller =&gt; @RestController @ResponseStatus mvc 统一处理  @ControllerAdvice，组合 @ResponseBody =&gt; @RestControllerAdvice @ExceptionHandler mvc 参数  @PathVariable mvc ajax  @CrossOrigin boot auto  @SpringBootApplication @EnableAutoConfiguration @SpringBootConfiguration boot condition  @ConditionalOnClass，classpath 下存在某个 class 时，条件才成立 @ConditionalOnMissingBean，beanFactory 内不存在某个 bean 时，条件才成立 @ConditionalOnProperty，配置文件中存在某个 property（键、值）时，条件才成立 boot properties  @ConfigurationProperties，会将当前 bean 的属性与配置文件中的键值进行绑定 @EnableConfigurationProperties，会添加两个较为重要的 bean  ConfigurationPropertiesBindingPostProcessor，bean 后处理器，在 bean 初始化前调用下面的 binder  ConfigurationPropertiesBinder，真正执行绑定操作   7. Spring 中的设计模式 要求  掌握 Spring 中常见的设计模式 1. Spring 中的 Singleton 请大家区分 singleton pattern 与 Spring 中的 singleton bean  根据单例模式的目的 Ensure a class only has one instance, and provide a global point of access to it 显然 Spring 中的 singleton bean 并非实现了单例模式，singleton bean 只能保证每个容器内，相同 id 的 bean 单实例 当然 Spring 中也用到了单例模式，例如  org.springframework.transaction.TransactionDefinition#withDefaults  org.springframework.aop.TruePointcut#INSTANCE  org.springframework.aop.interceptor.ExposeInvocationInterceptor#ADVISOR  org.springframework.core.annotation.AnnotationAwareOrderComparator#INSTANCE  org.springframework.core.OrderComparator#INSTANCE   2. Spring 中的 Builder 定义 Separate the construction of a complex object from its representation so that the same construction process can create different representations 它的主要亮点有三处：  较为灵活的构建产品对象   在不执行最后 build 方法前，产品对象都不可用   构建过程采用链式调用，看起来比较爽 Spring 中体现 Builder 模式的地方：  org.springframework.beans.factory.support.BeanDefinitionBuilder   org.springframework.web.util.UriComponentsBuilder   org.springframework.http.ResponseEntity.HeadersBuilder   org.springframework.http.ResponseEntity.BodyBuilder 3. Spring 中的 Factory Method 定义 Define an interface for creating an object, but let subclasses decide which class to instantiate. Factory Method lets a class defer instantiation to subclasses 根据上面的定义，Spring 中的 ApplicationContext 与 BeanFactory 中的 getBean 都可以视为工厂方法，它隐藏了 bean （产品）的创建过程和具体实现 Spring 中其它工厂：  org.springframework.beans.factory.FactoryBean   @Bean 标注的静态方法及实例方法   ObjectFactory 及 ObjectProvider 前两种工厂主要封装第三方的 bean 的创建过程，后两种工厂可以推迟 bean 创建，解决循环依赖及单例注入多例等问题 4. Spring 中的 Adapter 定义 Convert the interface of a class into another interface clients expect. Adapter lets classes work together that couldn’t otherwise because of incompatible interfaces 典型的实现有两处：  org.springframework.web.servlet.HandlerAdapter – 因为控制器实现有各种各样，比如有  大家熟悉的 @RequestMapping 标注的控制器实现  传统的基于 Controller 接口（不是 @Controller注解啊）的实现  较新的基于 RouterFunction 接口的实现  它们的处理方法都不一样，为了统一调用，必须适配为 HandlerAdapter 接口   org.springframework.beans.factory.support.DisposableBeanAdapter – 因为销毁方法多种多样，因此都要适配为 DisposableBean 来统一调用销毁方法 5. Spring 中的 Composite 定义 Compose objects into tree structures to represent part-whole hierarchies. Composite lets clients treat individual objects and compositions of objects uniformly 典型实现有：  org.springframework.web.method.support.HandlerMethodArgumentResolverComposite org.springframework.web.method.support.HandlerMethodReturnValueHandlerComposite org.springframework.web.servlet.handler.HandlerExceptionResolverComposite org.springframework.web.servlet.view.ViewResolverComposite composite 对象的作用是，将分散的调用集中起来，统一调用入口，它的特征是，与具体干活的实现实现同一个接口，当调用 composite 对象的接口方法时，其实是委托具体干活的实现来完成 6. Spring 中的 Decorator 定义 Attach additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality 典型实现：  org.springframework.web.util.ContentCachingRequestWrapper 7. Spring 中的 Proxy 定义 Provide a surrogate or placeholder for another object to control access to it 装饰器模式注重的是功能增强，避免子类继承方式进行功能扩展，而代理模式更注重控制目标的访问 典型实现：  org.springframework.aop.framework.JdkDynamicAopProxy org.springframework.aop.framework.ObjenesisCglibAopProxy 8. Spring 中的 Chain of Responsibility 定义 Avoid coupling the sender of a request to its receiver by giving more than one object a chance to handle the request. Chain the receiving objects and pass the request along the chain until an object handles it 典型实现：  org.springframework.web.servlet.HandlerInterceptor 9. Spring 中的 Observer 定义 Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically 典型实现：  org.springframework.context.ApplicationListener org.springframework.context.event.ApplicationEventMulticaster org.springframework.context.ApplicationEvent 10. Spring 中的 Strategy 定义 Define a family of algorithms, encapsulate each one, and make them interchangeable. Strategy lets the algorithm vary independently from clients that use it 典型实现：  org.springframework.beans.factory.support.InstantiationStrategy org.springframework.core.annotation.MergedAnnotations.SearchStrategy org.springframework.boot.autoconfigure.condition.SearchStrategy 11. Spring 中的 Template Method 定义 Define the skeleton of an algorithm in an operation, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm’s structure 典型实现：  大部分以 Template 命名的类，如 JdbcTemplate，TransactionTemplate 很多以 Abstract 命名的类，如 AbstractApplicationContext ",
      "url"      : "http://zhangjinmiao.github.io/interview/2022/08/21/spring.html",
      "keywords" : "Interview,面试,spring"
    } ,
  
    {
      "title"    : "Spring Boot 面试问题集锦",
      "category" : "Interview",
      "content": "1. SpringBoot 自动配置原理 要求  掌握 SpringBoot 自动配置原理 自动配置原理 @SpringBootConfiguration 是一个组合注解，由 @ComponentScan、@EnableAutoConfiguration 和 @SpringBootConfiguration 组成  @SpringBootConfiguration 与普通 @Configuration 相比，唯一区别是前者要求整个 app 中只出现一次 @ComponentScan  excludeFilters - 用来在组件扫描时进行排除，也会排除自动配置类   @EnableAutoConfiguration 也是一个组合注解，由下面注解组成  @AutoConfigurationPackage – 用来记住扫描的起始包  @Import(AutoConfigurationImportSelector.class) 用来加载 META-INF/spring.factories 中的自动配置类   为什么不使用 @Import 直接引入自动配置类 有两个原因：  让主配置类和自动配置类变成了强耦合，主配置类不应该知道有哪些从属配置 直接用 @Import(自动配置类.class)，引入的配置解析优先级较高，自动配置类的解析应该在主配置没提供时作为默认配置 因此，采用了 @Import(AutoConfigurationImportSelector.class)  由 AutoConfigurationImportSelector.class 去读取 META-INF/spring.factories 中的自动配置类，实现了弱耦合。 另外 AutoConfigurationImportSelector.class 实现了 DeferredImportSelector 接口，让自动配置的解析晚于主配置的解析 ",
      "url"      : "http://zhangjinmiao.github.io/interview/2022/08/22/spring-boot.html",
      "keywords" : "Interview,面试,spring boot"
    } ,
  
    {
      "title"    : "JVM 面试问题集锦",
      "category" : "Interview",
      "content": "1. JVM 内存结构 要求  掌握 JVM 内存结构划分 尤其要知道方法区、永久代、元空间的关系 结合一段 java 代码的执行理解内存划分  执行 javac 命令编译源代码为字节码 执行 java 命令  创建 JVM，调用类加载子系统加载 class，将类的信息存入方法区  创建 main 线程，使用的内存区域是 JVM 虚拟机栈，开始执行 main 方法代码  如果遇到了未见过的类，会继续触发类加载过程，同样会存入方法区  需要创建对象，会使用堆内存来存储对象  不再使用的对象，会由垃圾回收器在内存不足时回收其内存  调用方法时，方法内的局部变量、方法参数所使用的是 JVM 虚拟机栈中的栈帧内存  调用方法时，先要到方法区获得到该方法的字节码指令，由解释器将字节码指令解释为机器码执行  调用方法时，会将要执行的指令行号读到程序计数器，这样当发生了线程切换，恢复时就可以从中断的位置继续  对于非 java 实现的方法调用，使用内存称为本地方法栈（见说明）  对于热点方法调用，或者频繁的循环代码，由 JIT 即时编译器将这些代码编译成机器码缓存，提高执行性能   说明  加粗字体代表了 JVM 虚拟机组件 对于 Oracle 的 Hotspot 虚拟机实现，不区分虚拟机栈和本地方法栈 会发生内存溢出的区域  不会出现内存溢出的区域 – 程序计数器 出现 OutOfMemoryError 的情况  堆内存耗尽 – 对象越来越多，又一直在使用，不能被垃圾回收  方法区内存耗尽 – 加载的类越来越多，很多框架都会在运行期间动态产生新的类  虚拟机栈累积 – 每个线程最多会占用 1 M 内存，线程个数越来越多，而又长时间运行不销毁时   出现 StackOverflowError 的区域  JVM 虚拟机栈，原因有方法递归调用未正确结束、反序列化 json 时循环引用   方法区、永久代、元空间  方法区是 JVM 规范中定义的一块内存区域，用来存储类元数据、方法字节码、即时编译器需要的信息等 永久代是 Hotspot 虚拟机对 JVM 规范的实现（1.8 之前） 元空间是 Hotspot 虚拟机对 JVM 规范的另一种实现（1.8 以后），使用本地内存作为这些信息的存储空间  从这张图学到三点  当第一次用到某个类是，由类加载器将 class 文件的类元信息读入，并存储于元空间 X，Y 的类元信息是存储于元空间中，无法直接访问 可以用 X.class，Y.class 间接访问类元信息，它们俩属于 java 对象，我们的代码中可以使用  从这张图可以学到  堆内存中：当一个类加载器对象，这个类加载器对象加载的所有类对象，这些类对象对应的所有实例对象都没人引用时，GC 时就会对它们占用的对内存进行释放 元空间中：内存释放以类加载器为单位，当堆中类加载器内存释放时，对应的元空间中的类元信息也会释放 2. JVM 内存参数 要求  熟悉常见的 JVM 参数，尤其和大小相关的 堆内存，按大小设置 解释：  -Xms 最小堆内存（包括新生代和老年代） -Xmx 最大对内存（包括新生代和老年代） 通常建议将 -Xms 与 -Xmx 设置为大小相等，即不需要保留内存，不需要从小到大增长，这样性能较好 -XX:NewSize 与 -XX:MaxNewSize 设置新生代的最小与最大值，但一般不建议设置，由 JVM 自己控制 -Xmn 设置新生代大小，相当于同时设置了 -XX:NewSize 与 -XX:MaxNewSize 并且取值相等 保留是指，一开始不会占用那么多内存，随着使用内存越来越多，会逐步使用这部分保留内存。下同 堆内存，按比例设置 解释：  -XX:NewRatio=2:1 表示老年代占两份，新生代占一份 -XX:SurvivorRatio=4:1 表示新生代分成六份，伊甸园占四份，from 和 to 各占一份 元空间内存设置 解释：  class space 存储类的基本信息，最大值受 -XX:CompressedClassSpaceSize 控制 non-class space 存储除类的基本信息以外的其它信息（如方法字节码、注解等） class space 和 non-class space 总大小受 -XX:MaxMetaspaceSize 控制 注意：  这里 -XX:CompressedClassSpaceSize 这段空间还与是否开启了指针压缩有关，这里暂不深入展开，可以简单认为指针压缩默认开启 代码缓存内存设置 解释：  如果 -XX:ReservedCodeCacheSize &lt; 240m，所有优化机器代码不加区分存在一起 否则，分成三个区域（图中笔误 mthod 拼写错误，少一个 e）  non-nmethods - JVM 自己用的代码  profiled nmethods - 部分优化的机器码  non-profiled nmethods - 完全优化的机器码   线程内存设置  官方参考文档  https://docs.oracle.com/en/java/javase/11/tools/java.html#GUID-3B1CE181-CD30-4178-9602-230B800D4FAE 3. JVM 垃圾回收 要求  掌握垃圾回收算法 掌握分代回收思想 理解三色标记及漏标处理 了解常见垃圾回收器 三种垃圾回收算法 标记清除法 解释：  找到 GC Root 对象，即那些一定不会被回收的对象，如正执行方法内局部变量引用的对象、静态变量引用的对象 标记阶段：沿着 GC Root 对象的引用链找，直接或间接引用到的对象加上标记 清除阶段：释放未加标记的对象占用的内存 要点：  标记速度与存活对象线性关系 清除速度与内存大小线性关系 缺点是会产生内存碎片 标记整理法 解释：  前面的标记阶段、清理阶段与标记清除法类似 多了一步整理的动作，将存活对象向一端移动，可以避免内存碎片产生 特点：  标记速度与存活对象线性关系 清除与整理速度与内存大小成线性关系 缺点是性能上较慢 标记复制法 解释：  将整个内存分成两个大小相等的区域，from 和 to，其中 to 总是处于空闲，from 存储新创建的对象 标记阶段与前面的算法类似 在找出存活对象后，会将它们从 from 复制到 to 区域，复制的过程中自然完成了碎片整理 复制完成后，交换 from 和 to 的位置即可 特点：  标记与复制速度与存活对象成线性关系 缺点是会占用成倍的空间 GC 与分代回收算法 GC 的目的在于实现无用对象内存自动释放，减少内存碎片、加快分配速度 GC 要点：  回收区域是堆内存，不包括虚拟机栈 判断无用对象，使用可达性分析算法，三色标记法标记存活对象，回收未标记对象 GC 具体的实现称为垃圾回收器 GC 大都采用了分代回收思想  理论依据是大部分对象朝生夕灭，用完立刻就可以回收，另有少部分对象会长时间存活，每次很难回收  根据这两类对象的特性将回收区域分为新生代和老年代，新生代采用标记复制法、老年代一般采用标记整理法   根据 GC 的规模可以分成 Minor GC，Mixed GC，Full GC 分代回收  伊甸园 eden，最初对象都分配到这里，与幸存区 survivor（分成 from 和 to）合称新生代，  当伊甸园内存不足，标记伊甸园与 from（现阶段没有）的存活对象  将存活对象采用复制算法复制到 to 中，复制完毕后，伊甸园和 from 内存都得到释放  将 from 和 to 交换位置  经过一段时间后伊甸园的内存又出现不足  标记伊甸园与 from（现阶段没有）的存活对象  将存活对象采用复制算法复制到 to 中  复制完毕后，伊甸园和 from 内存都得到释放  将 from 和 to 交换位置  老年代 old，当幸存区对象熬过几次回收（最多15次），晋升到老年代（幸存区内存不足或大对象会导致提前晋升） GC 规模  Minor GC 发生在新生代的垃圾回收，暂停时间短   Mixed GC 新生代 + 老年代部分区域的垃圾回收，G1 收集器特有   Full GC 新生代 + 老年代完整垃圾回收，暂停时间长，应尽力避免 三色标记 即用三种颜色记录对象的标记状态  黑色 – 已标记 灰色 – 标记中 白色 – 还未标记 起始的三个对象还未处理完成，用灰色表示  该对象的引用已经处理完成，用黑色表示，黑色引用的对象变为灰色  依次类推  沿着引用链都标记了一遍  最后为标记的白色对象，即为垃圾  并发漏标问题 比较先进的垃圾回收器都支持并发标记，即在标记过程中，用户线程仍然能工作。但这样带来一个新的问题，如果用户线程修改了对象引用，那么就存在漏标问题。例如：  如图所示标记工作尚未完成  用户线程同时在工作，断开了第一层 3、4 两个对象之间的引用，这时对于正在处理 3 号对象的垃圾回收线程来讲，它会将 4 号对象当做是白色垃圾  但如果其他用户线程又建立了 2、4 两个对象的引用，这时因为 2 号对象是黑色已处理对象了，因此垃圾回收线程不会察觉到这个引用关系的变化，从而产生了漏标  如果用户线程让黑色对象引用了一个新增对象，一样会存在漏标问题  因此对于并发标记而言，必须解决漏标问题，也就是要记录标记过程中的变化。有两种解决方法：  Incremental Update 增量更新法，CMS 垃圾回收器采用  思路是拦截每次赋值动作，只要赋值发生，被赋值的对象就会被记录下来，在重新标记阶段再确认一遍   Snapshot At The Beginning，SATB 原始快照法，G1 垃圾回收器采用  思路也是拦截每次赋值动作，不过记录的对象不同，也需要在重新标记阶段对这些对象二次处理  新加对象会被记录  被删除引用关系的对象也被记录   垃圾回收器 - Parallel GC  eden 内存不足发生 Minor GC，采用标记复制算法，需要暂停用户线程  old 内存不足发生 Full GC，采用标记整理算法，需要暂停用户线程 注重吞吐量 垃圾回收器 - ConcurrentMarkSweep GC  它是工作在 old 老年代，支持并发标记的一款回收器，采用并发清除算法  并发标记时不需暂停用户线程  重新标记时仍需暂停用户线程   如果并发失败（即回收速度赶不上创建新对象速度），会触发 Full GC 注重响应时间 垃圾回收器 - G1 GC  响应时间与吞吐量兼顾 划分成多个区域，每个区域都可以充当 eden，survivor，old， humongous，其中 humongous 专为大对象准备 分成三个阶段：新生代回收、并发标记、混合收集 如果并发失败（即回收速度赶不上创建新对象速度），会触发 Full GC G1 回收阶段 - 新生代回收  初始时，所有区域都处于空闲状态  创建了一些对象，挑出一些空闲区域作为伊甸园区存储这些对象  当伊甸园需要垃圾回收时，挑出一个空闲区域作为幸存区，用复制算法复制存活对象，需要暂停用户线程  复制完成，将之前的伊甸园内存释放  随着时间流逝，伊甸园的内存又有不足  将伊甸园以及之前幸存区中的存活对象，采用复制算法，复制到新的幸存区，其中较老对象晋升至老年代  释放伊甸园以及之前幸存区的内存  G1 回收阶段 - 并发标记与混合收集  当老年代占用内存超过阈值后，触发并发标记，这时无需暂停用户线程  并发标记之后，会有重新标记阶段解决漏标问题，此时需要暂停用户线程。这些都完成后就知道了老年代有哪些存活对象，随后进入混合收集阶段。此时不会对所有老年代区域进行回收，而是根据暂停时间目标优先回收价值高（存活对象少）的区域（这也是 Gabage First 名称的由来）。  混合收集阶段中，参与复制的有 eden、survivor、old，下图显示了伊甸园和幸存区的存活对象复制  下图显示了老年代和幸存区晋升的存活对象的复制  复制完成，内存得到释放。进入下一轮的新生代回收、并发标记、混合收集  4. 内存溢出 要求  能够说出几种典型的导致内存溢出的情况 典型情况  误用线程池导致的内存溢出  参考 day03.TestOomThreadPool   查询数据量太大导致的内存溢出  参考 day03.TestOomTooManyObject   动态生成类导致的内存溢出  参考 day03.TestOomTooManyClass   5. 类加载 要求  掌握类加载阶段 掌握类加载器 理解双亲委派机制 类加载过程的三个阶段  加载    将类的字节码载入方法区，并创建类.class 对象   如果此类的父类没有加载，先加载父类  加载是懒惰执行   链接  验证 – 验证类是否符合 Class 规范，合法性、安全性检查  准备 – 为 static 变量分配空间，设置默认值  解析 – 将常量池的符号引用解析为直接引用   初始化  静态代码块、static 修饰的变量赋值、static final 修饰的引用类型变量赋值，会被合并成一个 &lt;cinit&gt; 方法，在初始化时被调用  static final 修饰的基本类型变量赋值，在链接阶段就已完成  初始化是懒惰执行   验证手段  使用 jps 查看进程号  使用 jhsdb 调试，执行命令 jhsdb.exe hsdb 打开它的图形界面   Class Browser 可以查看当前 jvm 中加载了哪些类  控制台的 universe 命令查看堆内存范围  控制台的 g1regiondetails 命令查看 region 详情  scanoops 起始地址 结束地址 对象类型 可以根据类型查找某个区间内的对象地址  控制台的 inspect 地址 指令能够查看这个地址对应的对象详情    使用 javap 命令可以查看 class 字节码  代码说明  day03.loader.TestLazy - 验证类的加载是懒惰的，用到时才触发类加载  day03.loader.TestFinal - 验证使用 final 修饰的变量不会触发类加载 jdk 8 的类加载器    名称  加载哪的类  说明     Bootstrap ClassLoader  JAVA_HOME/jre/lib  无法直接访问    Extension ClassLoader  JAVA_HOME/jre/lib/ext  上级为 Bootstrap，显示为 null    Application ClassLoader  classpath  上级为 Extension    自定义类加载器  自定义  上级为 Application   双亲委派机制 所谓的双亲委派，就是指优先委派上级类加载器进行加载，如果上级类加载器  能找到这个类，由上级加载，加载后该类也对下级加载器可见 找不到这个类，则下级类加载器才有资格执行加载 双亲委派的目的有两点  让上级类加载器中的类对下级共享（反之不行），即能让你的类能依赖到 jdk 提供的核心类   让类的加载有优先次序，保证核心类优先加载 对双亲委派的误解 下面面试题的回答是错误的 错在哪了？  自己编写类加载器就能加载一个假冒的 java.lang.System 吗? 答案是不行。   假设你自己的类加载器用双亲委派，那么优先由启动类加载器加载真正的 java.lang.System，自然不会加载假冒的   假设你自己的类加载器不用双亲委派，那么你的类加载器加载假冒的 java.lang.System 时，它需要先加载父类 java.lang.Object，而你没有用委派，找不到 java.lang.Object 所以加载会失败   以上也仅仅是假设。事实上操作你就会发现，自定义类加载器加载以 java. 打头的类时，会抛安全异常，在 jdk9 以上版本这些特殊包名都与模块进行了绑定，更连编译都过不了  代码说明  day03.loader.TestJdk9ClassLoader - 演示类加载器与模块的绑定关系 6. 四种引用 要求  掌握四种引用 强引用  普通变量赋值即为强引用，如 A a = new A();   通过 GC Root 的引用链，如果强引用不到该对象，该对象才能被回收  软引用（SoftReference）  例如：SoftReference a = new SoftReference(new A());   如果仅有软引用该对象时，首次垃圾回收不会回收该对象，如果内存仍不足，再次回收时才会释放对象   软引用自身需要配合引用队列来释放   典型例子是反射数据  弱引用（WeakReference）  例如：WeakReference a = new WeakReference(new A());   如果仅有弱引用引用该对象时，只要发生垃圾回收，就会释放该对象   弱引用自身需要配合引用队列来释放   典型例子是 ThreadLocalMap 中的 Entry 对象  虚引用（PhantomReference）  例如： PhantomReference a = new PhantomReference(new A(), referenceQueue);   必须配合引用队列一起使用，当虚引用所引用的对象被回收时，由 Reference Handler 线程将虚引用对象入队，这样就可以知道哪些对象被回收，从而对它们关联的资源做进一步处理   典型例子是 Cleaner 释放 DirectByteBuffer 关联的直接内存   代码说明  day03.reference.TestPhantomReference - 演示虚引用的基本用法  day03.reference.TestWeakReference - 模拟 ThreadLocalMap, 采用引用队列释放 entry 内存 7. finalize 要求  掌握 finalize 的工作原理与缺点 finalize  它是 Object 中的一个方法，如果子类重写它，垃圾回收时此方法会被调用，可以在其中进行资源释放和清理工作 将资源释放和清理放在 finalize 方法中非常不好，非常影响性能，严重时甚至会引起 OOM，从 Java9 开始就被标注为 @Deprecated，不建议被使用了 finalize 原理  对 finalize 方法进行处理的核心逻辑位于 java.lang.ref.Finalizer 类中，它包含了名为 unfinalized 的静态变量（双向链表结构），Finalizer 也可被视为另一种引用对象（地位与软、弱、虚相当，只是不对外，无法直接使用） 当重写了 finalize 方法的对象，在构造方法调用之时，JVM 都会将其包装成一个 Finalizer 对象，并加入 unfinalized 链表中  Finalizer 类中还有另一个重要的静态变量，即 ReferenceQueue 引用队列，刚开始它是空的。当狗对象可以被当作垃圾回收时，就会把这些狗对象对应的 Finalizer 对象加入此引用队列 但此时 Dog 对象还没法被立刻回收，因为 unfinalized -&gt; Finalizer 这一引用链还在引用它嘛，为的是【先别着急回收啊，等我调完 finalize 方法，再回收】 FinalizerThread 线程会从 ReferenceQueue 中逐一取出每个 Finalizer 对象，把它们从链表断开并真正调用 finallize 方法  由于整个 Finalizer 对象已经从 unfinalized 链表中断开，这样没谁能引用到它和狗对象，所以下次 gc 时就被回收了 finalize 缺点  无法保证资源释放：FinalizerThread 是守护线程，代码很有可能没来得及执行完，线程就结束了 无法判断是否发生错误：执行 finalize 方法时，会吞掉任意异常（Throwable） 内存释放不及时：重写了 finalize 方法的对象在第一次被 gc 时，并不能及时释放它占用的内存，因为要等着 FinalizerThread 调用完 finalize，把它从 unfinalized 队列移除后，第二次 gc 时才能真正释放内存 有的文章提到【Finalizer 线程会和我们的主线程进行竞争，不过由于它的优先级较低，获取到的CPU时间较少，因此它永远也赶不上主线程的步伐】这个显然是错误的，FinalizerThread 的优先级较普通线程更高，原因应该是 finalize 串行执行慢等原因综合导致 代码说明  day03.reference.TestFinalize - finalize 的测试代码  ",
      "url"      : "http://zhangjinmiao.github.io/interview/2022/08/23/jvm.html",
      "keywords" : "Interview,面试,jvm,gc"
    } ,
  
    {
      "title"    : "Axure",
      "category" : "tools",
      "content": "快捷键 C –&gt; Ctrl S –&gt; Shift M –&gt; Alt Cmd –&gt; Command 常用操作    功能  Windows  Mac OS X     当前页面下新建页面  C-Return       预览  F5       生成 HTML 文件  F8       置于顶层  C-S-]       置于底层  C-S-[       上移一层  C-]       下移一层  C-[       开关左侧功能栏  C-M-[       开关右侧功能栏  C-M-]      操作多个元件    功能  Windows  Mac OS X     左对齐  C-M-l       左右居中  C-M-c       右对齐  C-M-r       顶部对齐  C-M-t       上下居中  C-M-m       底部对齐  C-M-b       组合  C-g     ",
      "url"      : "http://zhangjinmiao.github.io/wiki/axure/",
      "keywords" : "Axure RP"
    } ,
  
    {
      "title"    : "Badminton",
      "category" : "Hobbies",
      "content": "视频  李在福《Play To Win》完整版   李在福《追球》全集 图文  超全羽毛球技术图解 公众号  和蔡赟聊羽毛球 微信号：caiyunliaoyumaoqiu   羽毛球 微信号：yu-mao-qiu  ",
      "url"      : "http://zhangjinmiao.github.io/wiki/badminton/",
      "keywords" : "羽毛球"
    } ,
  
    {
      "title"    : "中文文案排版指北（简体中文版）",
      "category" : "Copywriting",
      "content": "GitHub repository: mzlogin/chinese-copywriting-guidelines 统一中文文案、排版的相关用法，降低团队成员之间的沟通成本，增强网站气质。 Other languages:  Chinese Traditional English  目录  空格   中英文之间需要增加空格  中文与数字之间需要增加空格  数字与单位之间需要增加空格  全角标点与其他字符之间不加空格  -ms-text-autospace to the rescue?   标点符号   不重复使用标点符号   全角和半角   使用全角中文标点  数字使用半角字符  遇到完整的英文整句、特殊名词，其內容使用半角标点   名词   专有名词使用正确的大小写  不要使用不地道的缩写   争议   链接之间增加空格  简体中文使用直角引号   工具 谁在这样做？ 参考文献 空格 「有研究显示，打字的时候不喜欢在中文和英文之间加空格的人，感情路都走得很辛苦，有七成的比例会在 34 岁的时候跟自己不爱的人结婚，而其余三成的人最后只能把遗产留给自己的猫。毕竟爱情跟书写都需要适时地留白。 与大家共勉之。」——vinta/paranoid-auto-spacing 中英文之间需要增加空格 正确：  在 LeanCloud 上，数据存储是围绕 AVObject 进行的。 错误：  在LeanCloud上，数据存储是围绕AVObject进行的。 在 LeanCloud上，数据存储是围绕AVObject 进行的。 完整的正确用法：  在 LeanCloud 上，数据存储是围绕 AVObject 进行的。每个 AVObject 都包含了与 JSON 兼容的 key-value 对应的数据。数据是 schema-free 的，你不需要在每个 AVObject 上提前指定存在哪些键，只要直接设定对应的 key-value 即可。 例外：「豆瓣FM」等产品名词，按照官方所定义的格式书写。 中文与数字之间需要增加空格 正确：  今天出去买菜花了 5000 元。 错误：  今天出去买菜花了 5000元。 今天出去买菜花了5000元。 数字与单位之间需要增加空格 正确：  我家的光纤入户宽带有 10 Gbps，SSD 一共有 20 TB。 错误：  我家的光纤入户宽带有 10Gbps，SSD 一共有 10TB。 例外：度／百分比与数字之间不需要增加空格： 正确：  今天是 233° 的高温。 新 MacBook Pro 有 15% 的 CPU 性能提升。 错误：  今天是 233 ° 的高温。 新 MacBook Pro 有 15 % 的 CPU 性能提升。 全角标点与其他字符之间不加空格 正确：  刚刚买了一部 iPhone，好开心！ 错误：  刚刚买了一部 iPhone ，好开心！ -ms-text-autospace to the rescue? Microsoft 有个 -ms-text-autospace 的 CSS 属性可以实现自动为中英文之间增加空白。不过目前并未普及，另外在其他应用场景，例如 OS X、iOS 的用户界面目前并不存在这个特性，所以请继续保持随手加空格的习惯。 标点符号 不重复使用标点符号 正确：  德国队竟然战胜了巴西队！ 她竟然对你说「喵」？！ 错误：  德国队竟然战胜了巴西队！！ 德国队竟然战胜了巴西队！！！！！！！！ 她竟然对你说「喵」？？！！ 她竟然对你说「喵」？！？！？？！！ 全角和半角 不明白什么是全角（全形）与半角（半形）符号？请查看维基百科词条『全角和半角』。 使用全角中文标点 正确：  嗨！你知道嘛？今天前台的小妹跟我说「喵」了哎！ 核磁共振成像（NMRI）是什么原理都不知道？JFGI！ 错误：  嗨! 你知道嘛? 今天前台的小妹跟我说 “喵” 了哎! 嗨!你知道嘛?今天前台的小妹跟我说”喵”了哎! 核磁共振成像 (NMRI) 是什么原理都不知道? JFGI! 核磁共振成像(NMRI)是什么原理都不知道?JFGI! 数字使用半角字符 正确：  这件蛋糕只卖 1000 元。 错误：  这件蛋糕只卖 １０００ 元。 例外：在设计稿、宣传海报中如出现极少量数字的情形时，为方便文字对齐，是可以使用全角数字的。 遇到完整的英文整句、特殊名词，其內容使用半角标点 正确：  乔布斯那句话是怎么说的？「Stay hungry, stay foolish.」 推荐你阅读《Hackers &amp; Painters: Big Ideas from the Computer Age》，非常的有趣。 错误：  乔布斯那句话是怎么说的？「Stay hungry，stay foolish。」 推荐你阅读《Hackers＆Painters：Big Ideas from the Computer Age》，非常的有趣。 名词 专有名词使用正确的大小写 大小写相关用法原属于英文书写范畴，不属于本 wiki 讨论內容，在这里只对部分易错用法进行简述。 正确：  使用 GitHub 登录 我们的客户有 GitHub、Foursquare、Microsoft Corporation、Google、Facebook, Inc.。 错误：  使用 github 登录 使用 GITHUB 登录 使用 Github 登录 使用 gitHub 登录 使用 gｲんĤЦ8 登录 我们的客户有 github、foursquare、microsoft corporation、google、facebook, inc.。 我们的客户有 GITHUB、FOURSQUARE、MICROSOFT CORPORATION、GOOGLE、FACEBOOK, INC.。 我们的客户有 Github、FourSquare、MicroSoft Corporation、Google、FaceBook, Inc.。 我们的客户有 gitHub、fourSquare、microSoft Corporation、google、faceBook, Inc.。 我们的客户有 gｲんĤЦ8、ｷouЯƧquﾑгє、๓เςг๏ร๏Ŧt ς๏гק๏гคtเ๏ภn、900913、ƒ4ᄃëв๏๏к, IПᄃ.。 注意：当网页中需要配合整体视觉风格而出现全部大写／小写的情形，HTML 中请使用标准的大小写规范进行书写；并通过 text-transform: uppercase;／text-transform: lowercase; 对表现形式进行定义。 不要使用不地道的缩写 正确：  我们需要一位熟悉 JavaScript、HTML5，至少理解一种框架（如 Backbone.js、AngularJS、React 等）的前端开发者。 错误：  我们需要一位熟悉 Js、h5，至少理解一种框架（如 backbone、angular、RJS 等）的 FED。 争议 以下用法略带有个人色彩，既：无论是否遵循下述规则，从语法的角度来讲都是正确的。 链接之间增加空格 用法：  请 提交一个 issue 并分配给相关同事。 访问我们网站的最新动态，请 点击这里 进行订阅！ 对比用法：  请提交一个 issue 并分配给相关同事。 访问我们网站的最新动态，请点击这里进行订阅！ 简体中文使用直角引号 用法：  「老师，『有条不紊』的『紊』是什么意思？」 对比用法：  “老师，‘有条不紊’的‘紊’是什么意思？” 工具    仓库  语言     vinta/paranoid-auto-spacing  JavaScript    huei90/pangu.node  Node.js    huacnlee/auto-correct  Ruby    sparanoid/space-lover  PHP (WordPress)    nauxliu/auto-correct  PHP    hotoo/pangu.vim  Vim    sparanoid/grunt-auto-spacing  Node.js (Grunt)    hjiang/scripts/add-space-between-latin-and-cjk  Python   谁在这样做？    网站  文案  UGC     Apple 中国  Yes  N/A    Apple 香港  Yes  N/A    Apple 台湾  Yes  N/A    Microsoft 中国  Yes  N/A    Microsoft 香港  Yes  N/A    Microsoft 台湾  Yes  N/A    LeanCloud  Yes  N/A    知乎  Yes  部分用户达成    V2EX  Yes  Yes    SegmentFault  Yes  部分用户达成    Apple4us  Yes  N/A    豌豆荚  Yes  N/A    Ruby China  Yes  标题达成    PHPHub  Yes  标题达成   参考文献  Guidelines for Using Capital Letters Letter case - Wikipedia Punctuation - Oxford Dictionaries Punctuation - The Purdue OWL How to Use English Punctuation Corrently - wikiHow 格式 - openSUSE 全角和半角 - 维基百科 引号 - 维基百科 疑问惊叹号 - 维基百科 ",
      "url"      : "http://zhangjinmiao.github.io/wiki/chinese-copywriting-guidelines/",
      "keywords" : "中文文案排版指北"
    } ,
  
    {
      "title"    : "Eclipse",
      "category" : "Eclipse",
      "content": "快捷键 C –&gt; Ctrl S –&gt; Shift A –&gt; Alt    功能  快捷键     显示所有快捷键  C-S-l    开/关注释  C-/    显示 outline  C-o    当前打开的文件列表  C-e    快速查找打开文件  C-S-r    查找  C-h    查找后跳到下一处  C-.    Undo  C-z    Redo  C-y    跳到指定行  C-l    自动补全  A-/    自动解决导入包问题  C-S-o    返回  A-Left    反返回  A-Right    步进  F5    单步  F6    执行到返回  F7    继续执行  F8    删除当前行  C-d    删除前一个词  C-Backspace    删除后一个词  C-Delete    缩进  Tab    减少缩进  S-Tab    在下面新起一行  S-Enter    在上面新起一行  C-S-Enter   Q&amp;A  如何解决 Mac OS X 下安装的是 Java 1.8，运行 Eclipse 时提示「您需要安装旧 Java SE 6 运行环境才能打开「Eclipse.app」。」的问题？  更改 /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Info.plist 文件里的 JVMCapabilities 段如下（默认只有 CommandLine）：  &lt;key&gt;JVMCapabilities&lt;/key&gt;   &lt;array&gt;    &lt;string&gt;JNI&lt;/string&gt;    &lt;string&gt;BundledApp&lt;/string&gt;    &lt;string&gt;WebStart&lt;/string&gt;    &lt;string&gt;Applets&lt;/string&gt;    &lt;string&gt;CommandLine&lt;/string&gt;   &lt;/array&gt;   然后重启电脑。 没有找到为何这样改的解释，按名称猜想应该是说在这几种环境下启动 Java 具有跨版本的兼容性吧。  ",
      "url"      : "http://zhangjinmiao.github.io/wiki/eclipse/",
      "keywords" : "Eclipse"
    } ,
  
    {
      "title"    : "Emacs",
      "category" : "Emacs",
      "content": "约定：C- 前缀表示 Ctrl，M- 前缀表示 Alt，S- 前缀表示 Shift，上档字符比如 @ 的实际按键应为 Shift+2。 目录  移动 编辑 缓冲区 窗口 文件 代码 命令 重复 外部命令 模式 显示 搜索 帮助 右键菜单 插件   evil-nerd-commenter  function-args  hexl-mode  ido  jedi  projectile  python  package   其它需求   统计字数   移动 上 C-p 下 C-n 左 C-b 右 C-f 前一个词首 M-b 后一个词尾 M-f 跳到某一行 M-gg 行首 C-a 行尾 C-e 句首/前一个句首 M-a 名尾/前一个句尾 M-e 向前一个段落 M-{ 向后一个段落 M-} 下翻页 C-v 上翻页 M-v 跳到文首 M-&lt; 跳到文尾 M-&gt; 当前光标行移动到屏显上/中/下部 C-l 编辑 选取块 C-@ 复制 M-w 剪切 C-w 粘贴 C-y 全选 C-x h 切换只读/编辑模式 C-x C-q 交换当前字符与前一字符 C-t 交换当前单词与后一单词 M-t 交换当前行与上一行 C-x C-t 撤消 C-/ 或 C-x u 撤消撤消 C-g C-/ 当前单全大写 M-u 当前单词全小写 M-l 缓冲区 查看所有打开的缓冲区 C-x C-b 切换缓冲区 C-x b 关闭缓冲区 C-x k 关闭 emacsclientw.exe 打开的缓冲区 C-x # 窗口 关闭其它窗口 C-x 1 关闭当前窗口 C-x 0 在下面分割出一个窗口 C-x 2 在右边分割出一个窗口 C-x 3 依次切换到其它窗口 C-x o 文件 打开文件 C-x C-f 保存文件 C-x C-s 保存所有打开的文件 C-x s 在当前位置插入某文件内容 C-x i 代码 注释选中块 C-x r t 反注释选中块 C-x r k 注释/反注释 M-; 格式化光标之前的代码 C-M-   与上一行合并 M-^ 带注释前缀换行 M-j 解释运行当前 elisp 语句/函数 C-M-x 命令 输入命令 M-x 运行 SHELL shell 运行 ESHELL eshell 列出 elpa 上可用包 list-packages 安装插件 package-install 格式化输出 JSON json-pretty-print-buffer elisp 交互解释器 ielm 解释运行当前 Buffer eval-buffer 中止一个操作 C-g 对选中区域执行命令 M-| 重复 重复操作 50 次 M-50 命令 外部命令 输入外部命令 M-! 模式 打开/关闭某个模式 M-x 模式名 显示 放大字体 C-x C-= 缩小字体 C-x C– 重置字体 C-x C-0 自定义颜色 M-x customize-face default 搜索 渐进的搜索 C-s 往回搜索 C-r 按了回车之后继续搜索 C-s C-s 按了回车之后往回搜索 C-r C-r 帮助 查看变量的文档 C-h v 查看函数的文档 C-h f 查看某快捷键说明 C-h k 打开 Tutorial C-h t 打开帮助文档 C-h i 右键菜单 将如下代码命令为 .reg 文件，运行后可为鼠标右键添加菜单项「Edit with Emacs」（运行之前将 exe 路径先替换为自己的）。 前提是在配置文件里添加了 (server-start)。 Windows Registry Editor Version 5.00 [HKEY_CLASSES_ROOT  *  shell  Edit with Emacs] [HKEY_CLASSES_ROOT  *  shell  Edit with Emacs  command] @=  D:    emacs    bin    emacsclientw.exe   -a   D:    emacs    bin    runemacs.exe     %1   插件 evil-nerd-commenter 注释/反注释 M-; 注释多行 M-9 M-; function-args 提示函数参数 M-i 显示本文件大纲选择某项后跳转 C-M-j 跳转到函数定义（显示函数参数的时候有效） M-j hexl-mode 进入十六进制模式 M-x hexl-mode 退出十六进制模式 M-x hexl-mode-exit 输入十六进制数 M-x hexl-insert-hex-char ido 切换到上一个选项 C-r 切换到下一个选项 C-s jedi 显示光标处 Python 模块或函数的文档 C-c ? projectile 显示/刷新当前项目文件列表 C-c p f python 打开 Python 交互式Shell C-c C-p package 升级已安装的包 U 标记要安装的包 i 标记要删除的包 d 取消标记 u 执行操作 x 上一行 p 下一行 n 刷新包列表 r 重置缓冲区 g 退出窗口 q 其它需求 统计字数 （前提是 shell 下能运行 wc 程序）  选中要统计区域 M-| wc -c ",
      "url"      : "http://zhangjinmiao.github.io/wiki/emacs/",
      "keywords" : "Emacs, 快捷键"
    } ,
  
    {
      "title"    : "Git",
      "category" : "Git",
      "content": "常用命令    功能  命令     添加文件/更改到暂存区  git add filename    添加所有文件/更改到暂存区  git add .    提交  git commit -m msg    从远程仓库拉取最新代码  git pull origin master    推送到远程仓库  git push origin master    查看配置信息  git config –list    查看文件列表  git ls-files    比较工作区和暂存区  git diff    比较暂存区和版本库  git diff –cached    比较工作区和版本库  git diff HEAD    从暂存区移除文件  git reset HEAD filename    查看本地远程仓库配置  git remote -v    回滚  git reset –hard 提交SHA    强制推送到远程仓库  git push -f origin master    修改上次 commit  git commit –amend    推送 tags 到远程仓库  git push –tags    推送单个 tag 到远程仓库  git push origin [tagname]    删除远程分支  git push origin –delete [branchName]    远程空分支（等同于删除）  git push origin :[branchName]    查看所有分支历史  gitk –all    按日期排序显示历史  gitk –date-order   Q&amp;A 如何解决gitk中文乱码，git ls-files 中文文件名乱码问题？ 在~/.gitconfig中添加如下内容 [core] quotepath = false [gui] encoding = utf-8 [i18n] commitencoding = utf-8 [svn] pathnameencoding = utf-8 参考 http://zengrong.net/post/1249.htm 如何处理本地有更改需要从服务器合入新代码的情况？ git stash git pull git stash pop stash 查看 stash 列表： git stash list 查看某一次 stash 的改动文件列表（不传最后一个参数默认显示最近一次）： git stash show stash@{0} 以 patch 方式显示改动内容 git stash show -p stash@{0} 如何合并 fork 的仓库的上游更新？ git remote add upstream https://upstream-repo-url git fetch upstream git merge upstream/master 如何通过 TortoiseSVN 带的 TortoiseMerge.exe 处理 git 产生的 conflict？ 将 TortoiseMerge.exe 所在路径添加到 path 环境变量。 运行命令 git config --global merge.tool tortoisemerge 将 TortoiseMerge.exe 设置为默认的 merge tool。  在产生 conflict 的目录运行 git mergetool，TortoiseMerge.exe 会跳出来供你 resolve conflict。   也可以运行 git mergetool -t vimdiff 使用 -t 参数临时指定一个想要使用的 merge tool。   不想跟踪的文件已经被提交了，如何不再跟踪而保留本地文件？ git rm --cached /path/to/file，然后正常 add 和 commit 即可。 如何不建立一个没有 parent 的 branch？ git checkout --orphan newbranch 此时 git branch 是不会显示该 branch 的，直到你做完更改首次 commit。比如你可能会想建立一个空的 gh-pages branch，那么： git checkout --orphan gh-pages git rm -rf . // add your gh-pages branch files git add . git commit -m init commit submodule 的常用命令 添加 submodule git submodule add git@github.com:philsquared/Catch.git Catch 这会在仓库根目录下生成如下 .gitmodules 文件并 clone 该 submodule 到本地。 [submodule Catch] path = Catch url = git@github.com:philsquared/Catch.git 更新 submodule git submodule update 当 submodule 的 remote 有更新的时候，需要 git submodule update --remote 删除 submodule 在 .gitmodules 中删除对应 submodule 的信息，然后使用如下命令删除子模块所有文件： git rm --cached Catch clone 仓库时拉取 submodule git submodule update --init --recursive 删除远程 tag git tag -d v0.0.9 git push origin :refs/tags/v0.0.9 或 git push origin --delete tag [tagname] 清除未跟踪文件 git clean 可选项：    选项  含义     -q, –quiet  不显示删除文件名称    -n, –dry-run  试运行    -f, –force  强制删除    -i, –interactive  交互式删除    -d  删除文件夹    -e, –exclude  忽略符合 的文件    -x  清除包括 .gitignore 里忽略的文件    -X  只清除 .gitignore 里忽略的文件   忽略文件属性更改 因为临时需求对某个文件 chmod 了一下，结果这个就被记为了更改，有时候这是想要的，有时候这会造成困扰。 git config --global core.filemode false 参考：How do I make Git ignore file mode (chmod) changes? patch 将未添加到暂存区的更改生成 patch 文件： git diff &gt; demo.patch 将已添加到暂存区的更改生成 patch 文件： git diff --cached &gt; demo.patch 合并上面两条命令生成的 patch 文件包含的更改： git apply demo.patch 将从 HEAD 之前的 3 次 commit 生成 3 个 patch 文件： （HEAD 可以换成 sha1 码） git format-patch -3 HEAD 生成 af8e2 与 eaf8e 之间的 commits 的 patch 文件： （注意 af8e2 比 eaf8e 早） git format-patch af8e2..eaf8e 合并 format-patch 命令生成的 patch 文件： git am 0001-Update.patch 与 git apply 不同，这会直接 add 和 commit。 只下载最新代码 git clone --depth 1 git://xxxxxx 这样 clone 出来的仓库会是一个 shallow 的状态，要让它变成一个完整的版本： git fetch --unshallow 或 git pull --unshallow 基于某次 commit 创建分支 git checkout -b test 5234ab 表示以 commit hash 为 5234ab 的代码为基础创建分支 test。 恢复单个文件到指定版本 git reset 5234ab MainActivity.java 恢复 MainActivity.java 文件到 commit hash 为 5234ab 时的状态。 设置全局 hooks git config --global core.hooksPath C:/Users/mazhuang/git-hooks 然后把对应的 hooks 文件放在最后一个参数指定的目录即可。 比如想要设置在 commit 之前如果检测到没有从服务器同步则不允许 commit，那在以上目录下建立文件 pre-commit，内容如下： #!/bin/sh CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD) git fetch origin $CURRENT_BRANCH HEAD=$(git rev-parse HEAD) FETCH_HEAD=$(git rev-parse FETCH_HEAD) if [ $FETCH_HEAD = $HEAD ]; then  echo Pre-commit check passed  exit 0 fi echo Error: you need to update from remote first exit 1 查看某次 commit 的修改内容 git show &lt;commit-hash-id&gt; 查看某个文件的修改历史 git log -p &lt;filename&gt; 查看最近两次的修改内容 git log -p -2 应用已存在的某次更改 / merge 某一个 commit git cherry-pick &lt;commit-hash-id&gt; cherry-pick 有更多详细的用法，可以参见帮助文档。 命令行自动补全 在 shell 里加载 git-completion 系列脚本，详见 https://github.com/git/git/tree/master/contrib/completion 文件每一行变更明细 git blame &lt;filename&gt; 找回曾经的历史 git reflog 列出 HEAD 曾指向过的一系列 commit，它们只存在于本机，不是版本仓库的一部分。 还有： git fsck 记住 http(s) 方式的用户名密码 在有些情况下无法使用 git 协议，比如公司的 git 服务器设置了 IP 白名单，只能在公司内网使用 ssh，那么在外面就只能使用 http(s) 上传下载源码了，但每次都手动输入用户名/密码特别惨，于是乎就记住吧。 设置记住密码（默认 15 分钟）： git config --global credential.helper cache 自定义记住的时间（如下面是一小时）： git config credential.helper 'cache --timeout=3600' 长期存储密码： git config --global credential.helper store git commit 使用 vim 编辑 commit message 中文乱码 这个问题在 Windows 下出现了，没找到能完美解决的办法，一种方法是在 vim 打开后输入： :set termencoding=GBK 这就有点太麻烦了，折衷的方法是改为使用 gVim 或其它你喜欢的编辑器来编辑 commit message： git config --global core.editor gvim 参考： How do I make Git use the editor of my choice for commits? 转：git windows中文 乱码问题解决汇总 git log 中文乱码 只在 Windows 下遇到。 git config --global i18n.logoutputencoding gbk 编辑 git 安装目录下 etc/profile 文件，在最后添加如下内容： export LESSCHARSET=utf-8 参考：Git for windows 中文乱码解决方案 git diff 中文乱码 只在 Windows 下遇到，目前尚未找到有效办法。 统计代码行数 CMD 下直接执行可能失败，可以在右键，Git Bash here 里执行。 统计某人的代码提交量 git log --author=$(git config --get user.name) --pretty=tformat: --numstat | gawk '{ add += $1 ; subs += $2 ; loc += $1 - $2 } END { printf added lines: %s removed lines : %s total lines: %s  ,add,subs,loc }' 仓库提交都排名前 5 如果看全部，去掉 head 管道即可。 git log --pretty='%aN' | sort | uniq -c | sort -k1 -n -r | head -n 5 仓库提交者（邮箱）排名前 5 这个统计可能不太准，可能有同名。 git log --pretty=format:%ae | gawk -- '{ ++c[$0]; } END { for(cc in c) printf %5d %s  ,c[cc],cc; }' | sort -u -n -r | head -n 5 贡献者排名 git log --pretty='%aN' | sort -u | wc -l 提交数统计 git log --oneline | wc -l 参考：Git代码行统计命令集 修改文件名时的大小写问题 修改文件名大小写时，默认会被忽略（在 Windows 下是这样），让 git 对大小写敏感的方法： git config --global core.ignorecase false 或者使用 git mv oldname newname 也是可以的。 ",
      "url"      : "http://zhangjinmiao.github.io/wiki/git/",
      "keywords" : "Git, 版本控制"
    } ,
  
    {
      "title"    : "善用佳软",
      "category" : "Recommends",
      "content": "好的软件总是给人一种相见恨晚的感觉。 软件列表    功能  Windows  Mac OS X     文本编辑  gVim  MacVim    离线 API 文档  Zeal  Dash    UML  Visio  StarUML    流程图  Visio  ProcessOn.com    文件查找  Everything  Alfred    文件内容查找  FileLocator  Alfred    Android 开发  Android Studio  Android Studio    Android 虚拟机  Genymotion  Genymotion    Android 当无线鼠标、键盘、远程桌面等  WiFi Mouse  WiFi Mouse    源码阅读  Source Insight  IDE/Vim    笔记  OneNote  OneNote    终端  Cmder  zsh    视频播放器  QQ 影音  mpv    下载  迅雷精简版  迅雷    录制屏幕生成 GIF  LICEcap  LICEcap   亮点 Cmder  alias 在 Cmder 下可以很方便地像在类 Unix 系统下使用 alias 功能，比如： alias blog=cd /d d:  github  mzlogin.github.io   然后就能愉快地使用 blog 命令在任意目录进入 blog 仓库的目录了。 Genymotion 配置好后，那启动速度，嗖嗖地，秒 Android SDK 自带的八条街。 mpv 不像 MPlayerX 和 Perian 那样无故卡死转码半天就是最大的亮点了。 ",
      "url"      : "http://zhangjinmiao.github.io/wiki/good-soft/",
      "keywords" : "软件, 推荐"
    } ,
  
    {
      "title"    : "IDA Pro",
      "category" : "Debug",
      "content": "快捷键 C –&gt; Ctrl S –&gt; Shift M –&gt; Alt Cmd –&gt; Command    功能  快捷键     搜索文本  M-t    返回上一个位置  Esc    前进到下一位置  C-Enter    显示伪 C 代码  F5    跳转到地址  g  ",
      "url"      : "http://zhangjinmiao.github.io/wiki/ida-pro/",
      "keywords" : "debug, IDA Pro"
    } ,
  
    {
      "title"    : "IntelliJ IDEA",
      "category" : "Tools",
      "content": "快捷键基本与 Android Studio 一致，这里重点记录解决遇到过的问题。 Q&amp;A 解决导入 Eclipse Maven 工程后无法读取 .xml 文件的问题 IDEA 与 Eclipse 配置文件目录的方式不同，可以将文件夹标记为 Sources、Resources 和 tests 等，而 src/main/java 默认被标记为 Sources，src/main/resources 才默认被标记为 Resources，编译时自动复制。 这样放在 src/main/java 目录下的文件与子文件夹均为 Sources，只将编译生成的 .class 文件复制到编译目录，在 Eclipse Maven 工程里放在 src/main/java 文件夹里的 xml、props 和 properties 文件就不会被拷贝到编译文件夹，导致执行时找不到这些文件，报类似下面这样的错误： org.springframework.beans.factory.BeanDefinitionStoreException: IOException parsing XML document from class path resource [spring-demo.xml]; nested exception is java.io.FileNotFoundException: class path resource [spring-demo.xml] cannot be opened because it does not exist  tat org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:343)  tat org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)  tat org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)  tat org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)  tat org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:187)  tat org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:251)  tat org.springframework.context.support.AbstractXmlApplicationContext.loadBeanDefinitions(AbstractXmlApplicationContext.java:127)  tat org.springframework.context.support.AbstractXmlApplicationContext.loadBeanDefinitions(AbstractXmlApplicationContext.java:93)  tat org.springframework.context.support.AbstractRefreshableApplicationContext.refreshBeanFactory(AbstractRefreshableApplicationContext.java:129)  tat org.springframework.context.support.AbstractApplicationContext.obtainFreshBeanFactory(AbstractApplicationContext.java:540)  tat org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:454)  tat org.springframework.context.support.ClassPathXmlApplicationContext.&lt;init&gt;(ClassPathXmlApplicationContext.java:139)  tat org.springframework.context.support.ClassPathXmlApplicationContext.&lt;init&gt;(ClassPathXmlApplicationContext.java:83)  tat org.mazhuang.demo.protocol.db.DemoContext.init(DemoContext.java:22)  tat org.mazhuang.demo.protocol.DemoServer.start(DemoServer.java:40)  tat org.mazhuang.demo.DemoSrv.main(DemoSrv.java:17) Caused by: java.io.FileNotFoundException: class path resource [spring-demo.xml] cannot be opened because it does not exist  tat org.springframework.core.io.ClassPathResource.getInputStream(ClassPathResource.java:158)  tat org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:329)  t... 15 more 解决方案： 可以通过在 pom.xml 文件里添加 resources 配置来指定将哪些文件作为 resources 包含： &lt;build&gt;  &lt;resources&gt;  &lt;resource&gt;   &lt;directory&gt;${basedir}/src/main/java&lt;/directory&gt;   &lt;includes&gt;    &lt;include&gt;**/*.props&lt;/include&gt;    &lt;include&gt;**/*.xml&lt;/include&gt;   &lt;/includes&gt;  &lt;/resource&gt;  &lt;/resources&gt; &lt;/build&gt; 如何导出 jar 包 File -&gt; Project Structure -&gt; Artifacts -&gt; Click green plus sign -&gt; Jav -&gt; From modules with dependencies Build -&gt; Build Artifacts 参考  解决IntelliJ IDEA无法读取配置*.properties文件的问题 How to build jars from IntelliJ properly? ",
      "url"      : "http://zhangjinmiao.github.io/wiki/intellij-idea/",
      "keywords" : "IDEA, Java"
    } ,
  
    {
      "title"    : "Linux/Unix",
      "category" : "Linux",
      "content": "类 Unix 系统下的一些常用命令和用法。 实用命令 fuser 查看文件被谁占用。 fuser -u .linux.md.swp id 查看当前用户、组 id。 lsof 查看打开的文件列表。  An open file may be a regular file, a directory, a block special file, a character special file, an executing text reference, a library, a stream or a network file (Internet socket, NFS file or UNIX domain socket.) A specific file or all the files in a file system may be selected by path. 查看网络相关的文件占用 lsof -i 查看端口占用 lsof -i tcp:5037 查看某个文件被谁占用 lsof .linux.md.swp 查看某个用户占用的文件信息 lsof -u mazhuang -u 后面可以跟 uid 或 login name。 查看某个程序占用的文件信息 lsof -c Vim 注意程序名区分大小写。 ",
      "url"      : "http://zhangjinmiao.github.io/wiki/linux/",
      "keywords" : "Linux"
    } ,
  
    {
      "title"    : "Mac OS X",
      "category" : "Mac",
      "content": "快捷键约定： C –&gt; Ctrl S –&gt; Shift M –&gt; Alt/Option Cmd –&gt; Command Mac 键盘快捷键官方参考 窗口    功能  快捷键     显示桌面  F11    切换窗口全屏状态  C-Cmd-F    隐藏当前程序的所有窗口  Cmd-H    最小化窗口  Cmd-M    关闭窗口  Cmd-W    关闭当前程序  Cmd-Q    新建标签  Cmd-T    新建窗口  Cmd-N   程序    功能  快捷键     打开 emoji 表情窗口  C-Cmd- 空格    打开 Spotlight  C- 空格    切换输入法  Cmd- 空格    打开 Alfred  M- 空格    打开 Finder 并查找  C-M- 空格    打开 Launchpad  四指合拢   命令行 快捷键    功能  按键     移动光标至行首  C-a    移动光标至行尾  C-e    清屏  C-l    清屏  C-k    删除光标前的所有文字。如果光标位于行尾则删除整行。  C-u    与退格键相同  C-h    检索使用过的命令  C-r    终止当前执行  C-c    退出当前 shell  C-d    将执行中的任何东西放入后台进程。fg 可以将其恢复。  C-z    删除光标之前的单词  C-w    删除光标后的所有文字  C-k    将光标前的两个文字进行互换  C-t    光标向前移动一个单词  C-f    光标向后移动一个单词  C-b    将光标前的两个单词进行互换  Esc + t    自动补全文件或文件夹的名称  Tab   命令    按键 / 命令  描述     cd  Home 目录    cd [folder]  切换目录    cd ~  Home 目录，例如 ‘cd ~/folder/’    cd /  根目录    ls  文件列表    ls -l  文件详细列表    ls -a  列出隐藏文件    ls -lh  文件详细列表中的文件大小以更友好的形式列出    ls -R  递归显示文件夹中的内容    sudo [command]  以超级用户身份执行命令    open [file]  打开文件 ( 相当于双击一个文件 )    top  显示运行中的进程，按 q 终止    nano [file]  打开编辑    pico [file]  打开编辑    q  退出    clear  清屏   命令历史    按键/命令  描述     history n  列出最近执行过的 n 条命令    ctrl-r  检索之前执行过的命令    ![value]  执行最近以 ‘value’ 开始的命令    !!  执行最近执行过的命令   文件管理    按键/命令  描述     touch [file]  创建一个新文件    pwd  显示当前工作目录    ..  上级目录, 例如.    ‘ls -l ..’  上级目录的文件详细列表    ‘cd ../../’  向上移动两个层级    .  当前目录    cat  连接    rm [file]  移除文件, 例如 rm [file] [file]    rm -i [file]  移除时出现确认提示    rm -r [dir]  移除文件及内容    rm -f [file]  强制移除    cp [file] [newfile]  复制文件    cp [file] [dir]  复制文件到指定目录    mv [file] [new filename]  移动 / 重命名, 例如 mv -v [file] [dir]   目录管理    按键/命令  描述     mkdir [dir]  创建新目录    mkdir -p [dir]/[dir]  创建子目录    rmdir [dir]  移除目录 ( 仅限目录下没有内容时 )    rm -R [dir]  移除目录及内容   管道 - 连接多个带有输出的命令    按键/命令  描述     more  按当前窗口大小输出内容    &gt; [file]  输出至指定文件, 注意文件将会覆盖    » [file]  在制定文件的末尾附加内容    &lt;  从文件中读取内容   帮助    按键/命令  描述     [command] -h  显示帮助信息    [command] –help  显示帮助信息    [command] help  显示帮助信息    reset  重置当前终端    man [command]  显示指定命令的帮助信息    whatis [command]  显示指定命令的简述   搜索  使用 find 命令，例如： find ~ -iname aapt   使用 mdfind 命令，例如： 全局搜索 mdfind -name aapt   或搜索指定文件夹 mdfind -onlyin ~/Library aapt   使用 locate 命令，例如： locate aapt   复制文件路径  在 Finder 下 先按键 Cmd-i，然后从弹出的窗口里复制。   在 Terminal 下 pwd|pbcopy   Safari    功能  快捷键     定位到地址栏  Cmd-L    切换标签  Cmd-S-Left/Right    收藏页面  Cmd-D   保存 Safari 里正在播放的视频 $ su # cd /private/var/folders # ls nk zz # cd nk # ls zy3770994vqg83xvmbc9pd0m0000gn # cd zy3770994vqg83xvmbc9pd0m0000gn/T # open . 然后复制里面叫 FlashTmp.xxx 的文件，改名为 FlashTmp.flv。（操作过程中保持视频在播放状态） Terminal    功能  快捷键     新建标签  Cmd-T    上 / 下个标签  Cmd-{/}    删除光标前的输入  C-U   WireShark 使用 WireShark 1.99 开发版，可以不依赖于 X11，界面基于 Qt，更加美观，符合 Mac 界面风格。 截图    功能  快捷键     全屏截图保存到桌面  Cmd-S-3    全屏截图并复制  Cmd-C-S-3    选区截图保存到桌面  Cmd-S-4    选区截图并复制  Cmd-C-S-4    窗口截图保存到桌面  Cmd-S-4 空格    窗口截图并复制  Cmd-C-S-4 空格    QQ 截图  Cmd-S-A   去除窗口截图时的阴影 defaults write com.apple.screencapture disable-shadow -bool TRUE Killall SystemUIServer 如果要保留窗口截图时的阴影，则将 TRUE 改为 FALSE。 调整选区大小 使用选区模式选中一个区域并松开鼠标前，  按住空格并移动鼠标，可以保持区域大小不变，并移动区域； 按住Shift并移动鼠标，就可以保持区域的其它三个边不变，移动一个边的位置； 按住Alt并移动鼠标，就可以对称的调整区域大小。 截图标注 使用预览工具可以完成截图标注。 延时截图 使用系统自带的 Grab 工具，运行后选择菜单的 Capture - Timed Screen。 iBooks 里的电子书保存路径 /Users/&lt;username&gt;/Library/Containers/com.apple.BKAgentService/Data/Documents/iBooks/Books 安装 mpv 没有图形界面 使用 brew options mpv 可以看到有个 --with-bundle 是安装时创建 .app 文件。 brew install mpv --with-bundle brew linkapps mpv 屏幕取色 使用 Mac 自带的“数码测色计”。 参考  你可能不知道的 Mac 技巧 - 截图，Gif 制作及 App 推荐 terminal-mac-cheatsheet ",
      "url"      : "http://zhangjinmiao.github.io/wiki/mac/",
      "keywords" : "Mac"
    } ,
  
    {
      "title"    : "Markdown",
      "category" : "",
      "content": "目录  超链接 列表 强调 标题 表格 代码块 图片 锚点 Inline Attribute Emoji Footnotes mermaid sequence flowchart mathjax mindmap 超链接 [靠谱-ing](https://mazhuang.org) &lt;https://mazhuang.org&gt; 靠谱-ing https://mazhuang.org 列表 1. 有序列表项 1 2. 有序列表项 2 3. 有序列表项 3   有序列表项 1   有序列表项 2   有序列表项 3 * 无序列表项 1 * 无序列表项 2 * 无序列表项 3   无序列表项 1   无序列表项 2   无序列表项 3 - [x] 任务列表 1 - [ ] 任务列表 2 任务列表 1 任务列表 2 强调 ~~删除线~~ **加黑** *斜体* 删除线 加黑 斜体 标题 # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 Tips: # 与标题中间要加空格。 表格 | HEADER1 | HEADER2 | HEADER3 | HEADER4 | | ------- | :------ | :-----: | ------: | | content | content | content | content |   HEADER1  HEADER2  HEADER3  HEADER4     content  content  content  content   :—– 表示左对齐 :—-: 表示中对齐 —–: 表示右对齐 代码块 print 'Hello, World!'   list item1   list item2 print 'hello'   图片 ![本站favicon](/favicon.ico)  锚点 * [目录](#目录) 目录 Inline Attribute Span Inline Attribute 详情参考 https://kramdown.gettalong.org/syntax.html#span-ials Block Inline Attribute 详情参考 https://kramdown.gettalong.org/syntax.html#block-ials 给块/元素添加 class、id、内嵌样式等： ![本站favicon](/favicon.ico){:.center} Hello, *world*{:#world} Hello, *world*{: style=color:red} Hello, world Hello, world 结合自定义的样式，有些场景比较有用。 Emoji :camel: :blush: :smile: Footnotes This is a text with footnote1. mermaid sequenceDiagram  Alice--&gt;&gt;John: Hello John, how are you?  John--&gt;&gt;Alice: Great! sequence Andrew-&gt;China: Says Hello Note right of China: China thinks  about it China--&gt;Andrew: How are you? Andrew-&gt;&gt;China: I am good thanks! flowchart st=&gt;start: Start e=&gt;end op1=&gt;operation: My Operation sub1=&gt;subroutine: My Subroutine cond=&gt;condition: Yes or No? io=&gt;inputoutput: catch something... st-&gt;op1-&gt;cond cond(yes)-&gt;io-&gt;e cond(no)-&gt;sub1(right)-&gt;op1 mathjax When   ((a   e 0)  ), there are two solutions to   ((ax^2 + bx + c = 0)  ) and they are [x = {-b   pm   sqrt{b^2-4ac}   over 2a}.] mindmap # topic ## topic2 ### topic2.1 ### topic2.2 ## topic3 &lt;!--Note--&gt; 这是一个备注 &lt;!--/Note--&gt; ### topic3.1 ### topic3.2 #### topic3.2.1 #### topic3.2.2 #### topic3.2.3 #### topic3.2.4 #### topic3.2.5 ### topic3.4 ### topic3.5 ### topic3.6   Here is the footnote 1 definition. &#8617;  ",
      "url"      : "http://zhangjinmiao.github.io/wiki/markdown/",
      "keywords" : "Markdown"
    } ,
  
    {
      "title"    : "Skill Tree",
      "category" : "",
      "content": "# 技能树 ## Java 后端开发 ## Android 开发 ## Windows 开发 ## 通用技能 ### 操作系统 #### Windows #### Linux #### macOS ### 编辑器 #### Vim #### Visual Studio Code ### 标记语言 #### Markdown ### 版本控制 #### Git #### SVN ",
      "url"      : "http://zhangjinmiao.github.io/wiki/skill-tree/",
      "keywords" : "技能树, 思维导图, mindmap, 脑图"
    } 
  
]

